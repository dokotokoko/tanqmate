"""
main.py çµ±åˆãƒ‘ãƒƒãƒ
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€main.pyã®ç‰¹å®šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç½®ãæ›ãˆã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
"""

# ===================================
# 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
# ===================================
IMPORT_ADDITIONS = """
# æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from backend.async_helpers import (
    AsyncDatabaseHelper,
    AsyncProjectContextBuilder,
    parallel_fetch_context_and_history,
    parallel_save_chat_logs
)
from module.llm_api import get_async_llm_client
"""

# ===================================
# 2. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®è¿½åŠ 
# ===================================
GLOBAL_ADDITIONS = """
# éåŒæœŸå‡¦ç†ç”¨ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
async_llm_client = None  # éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
USE_OPTIMIZED_ENDPOINTS = os.environ.get("USE_OPTIMIZED_ENDPOINTS", "true").lower() == "true"
"""

# ===================================
# 3. startup_event ã®ä¿®æ­£ç‰ˆ
# ===================================
STARTUP_EVENT_MODIFICATION = """
    # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã®å¾Œã«è¿½åŠ 
    
    # éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    global async_llm_client
    if USE_OPTIMIZED_ENDPOINTS:
        try:
            async_llm_client = get_async_llm_client(pool_size=10)
            logger.info("âœ… éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 10ï¼‰")
        except Exception as e:
            logger.error(f"âŒ éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            async_llm_client = None
"""

# ===================================
# 4. æœ€é©åŒ–ç‰ˆ /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ===================================
OPTIMIZED_CHAT_ENDPOINT = '''
@app.post("/chat", response_model=ChatResponse, dependencies=[Depends(chat_rate_limiter)])
async def chat_with_ai(
    chat_data: ChatMessage,
    current_user: int = Depends(get_current_user_cached)
):
    """AIã¨ã®ãƒãƒ£ãƒƒãƒˆï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ - æœ€é©åŒ–å¯¾å¿œï¼‰"""
    try:
        validate_supabase()
        
        # æœ€é©åŒ–ç‰ˆã®ä½¿ç”¨åˆ¤å®š
        if USE_OPTIMIZED_ENDPOINTS and async_llm_client:
            # æœ€é©åŒ–ç‰ˆå®Ÿè£…
            from backend.async_helpers import (
                AsyncDatabaseHelper,
                AsyncProjectContextBuilder,
                parallel_fetch_context_and_history
            )
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
            import time
            start_time = time.time()
            
            # ãƒ˜ãƒ«ãƒ‘ãƒ¼åˆæœŸåŒ–
            db_helper = AsyncDatabaseHelper(supabase)
            context_builder = AsyncProjectContextBuilder(db_helper)
            
            # ãƒšãƒ¼ã‚¸IDã®æ±ºå®š
            page_id = chat_data.page_id or chat_data.page or "general"
            
            # conversationã®å–å¾—/ä½œæˆ
            conversation_id = await get_or_create_conversation(current_user, page_id)
            
            # å±¥æ­´å–å¾—æ•°ã®å‹•çš„èª¿æ•´
            history_limit = 20  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’æ¸›ã‚‰ã™
            if chat_data.message and len(chat_data.message) > 500:
                history_limit = 50
            elif ENABLE_CONVERSATION_AGENT and conversation_orchestrator:
                history_limit = 100
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å±¥æ­´ã‚’ä¸¦åˆ—å–å¾—
            project_id, project_context, project, conversation_history = await parallel_fetch_context_and_history(
                db_helper=db_helper,
                context_builder=context_builder,
                page_id=page_id,
                conversation_id=conversation_id,
                user_id=current_user,
                history_limit=history_limit
            )
            
            logger.info(f"ğŸ“Š ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: å±¥æ­´{len(conversation_history)}ä»¶, å‡¦ç†æ™‚é–“{time.time() - start_time:.2f}ç§’")
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            from prompt.prompt import system_prompt
            system_prompt_with_context = system_prompt
            if project_context:
                system_prompt_with_context += project_context
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
            messages = [{"role": "system", "content": system_prompt_with_context}]
            if conversation_history:
                for history_msg in conversation_history:
                    role = "user" if history_msg["sender"] == "user" else "assistant"
                    messages.append({"role": role, "content": history_msg["message"]})
            messages.append({"role": "user", "content": chat_data.message})
            
            # LLMå¿œç­”ç”Ÿæˆï¼ˆéåŒæœŸï¼‰
            response = await async_llm_client.generate_response_async(messages)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            context_data_dict = {"timestamp": datetime.now(timezone.utc).isoformat()}
            if chat_data.memo_content:
                context_data_dict["memo_content"] = chat_data.memo_content[:500]
            if project_id:
                context_data_dict["project_id"] = project_id
            if project:
                context_data_dict["project_info"] = {
                    "theme": project.get("theme"),
                    "question": project.get("question"),
                    "hypothesis": project.get("hypothesis")
                }
            
            # ãƒ­ã‚°ã®ä¸¦åˆ—ä¿å­˜
            from backend.async_helpers import parallel_save_chat_logs
            
            user_msg_data = {
                "user_id": current_user,
                "page_id": page_id,
                "sender": "user",
                "message": chat_data.message,
                "conversation_id": conversation_id,
                "context_data": context_data_dict
            }
            
            ai_msg_data = {
                "user_id": current_user,
                "page_id": page_id,
                "sender": "assistant",
                "message": response,
                "conversation_id": conversation_id,
                "context_data": {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "has_project_context": bool(project_context),
                    "optimized": True
                }
            }
            
            await parallel_save_chat_logs(db_helper, user_msg_data, ai_msg_data)
            
            # conversation timestampæ›´æ–°ï¼ˆéãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            asyncio.create_task(update_conversation_timestamp(conversation_id))
            
            total_time = time.time() - start_time
            logger.info(f"âœ… æœ€é©åŒ–ç‰ˆãƒãƒ£ãƒƒãƒˆå‡¦ç†å®Œäº†: ç·å‡¦ç†æ™‚é–“{total_time:.2f}ç§’")
            
            return ChatResponse(
                response=response,
                timestamp=datetime.now(timezone.utc).isoformat(),
                token_usage=None,
                context_metadata={"has_project_context": bool(project_context), "optimized": True}
            )
            
        else:
            # ========== æ—¢å­˜ã®å®Ÿè£…ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰==========
            # ä»¥ä¸‹ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
            # ... [æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰]
            pass  # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã¯å…ƒã®main.pyã‹ã‚‰å–å¾—
            
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"Chat API Error: {str(e)}\\nTraceback: {traceback.format_exc()}")
        handle_database_error(e, "AIå¿œç­”ã®ç”Ÿæˆ")
'''

# ===================================
# 5. .env ãƒ•ã‚¡ã‚¤ãƒ«ã®è¿½åŠ è¨­å®š
# ===================================
ENV_ADDITIONS = """
# æœ€é©åŒ–è¨­å®š
USE_OPTIMIZED_ENDPOINTS=true    # æœ€é©åŒ–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨ãƒ•ãƒ©ã‚°
OPENAI_API_POOL_SIZE=10         # OpenAI APIæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º
ASYNC_DB_POOL_SIZE=20            # éåŒæœŸDBæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º
PARALLEL_FETCH_ENABLED=true      # ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—æœ‰åŠ¹åŒ–
CACHE_ENABLED=false              # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
"""

# ===================================
# 6. ç°¡æ˜“çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ===================================
def generate_patch_instructions():
    """
    ãƒ‘ãƒƒãƒé©ç”¨ã®æ‰‹é †ã‚’ç”Ÿæˆ
    """
    instructions = """
    ========================================
    main.py ãƒ‘ãƒƒãƒé©ç”¨æ‰‹é †
    ========================================
    
    1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ:
       cp backend/main.py backend/main.py.backup
    
    2. ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®è¿½åŠ :
       - main.pyã®å†’é ­ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã« IMPORT_ADDITIONS ã‚’è¿½åŠ 
    
    3. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®è¿½åŠ :
       - ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆllm_client = None ã®å¾Œï¼‰ã« GLOBAL_ADDITIONS ã‚’è¿½åŠ 
    
    4. startup_event ã®æ›´æ–°:
       - startup_event é–¢æ•°ã®æœ€å¾Œã« STARTUP_EVENT_MODIFICATION ã‚’è¿½åŠ 
    
    5. /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç½®ãæ›ãˆ:
       - æ—¢å­˜ã® chat_with_ai é–¢æ•°ã‚’ OPTIMIZED_CHAT_ENDPOINT ã§ç½®ãæ›ãˆ
       - ãŸã ã—ã€æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯ else ãƒ–ãƒ­ãƒƒã‚¯å†…ã«ä¿æŒ
    
    6. .env ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°:
       - ENV_ADDITIONS ã®å†…å®¹ã‚’ .env ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
    
    7. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
       pip install aiofiles
    
    8. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:
       python -m pytest tests/test_optimized_endpoints.py
    
    9. ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•:
       uvicorn backend.main:app --reload --workers 4
    
    ========================================
    ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †
    ========================================
    
    å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆ:
    1. cp backend/main.py.backup backend/main.py
    2. .env ã§ USE_OPTIMIZED_ENDPOINTS=false ã«è¨­å®š
    3. ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
    
    ========================================
    """
    return instructions

if __name__ == "__main__":
    print(generate_patch_instructions())