# services/chat_service.py - ãƒãƒ£ãƒƒãƒˆãƒ»å¯¾è©±ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹

from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone
import time
import asyncio
import os
import json
import uuid
import logging
from .base import BaseService
from async_helpers import (
    parallel_fetch_context_and_history,
    parallel_save_chat_logs,
    rate_limited_openai_call
)

from prompt.prompt import RESPONSE_STYLE_PROMPTS

# turn_indexãƒã‚°ä¿®æ­£ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from async_helpers_turn_index import (
#     ChatLogStore,
#     parallel_save_chat_logs_with_turn_index
# )

class ChatService(BaseService):
    """ãƒãƒ£ãƒƒãƒˆãƒ»å¯¾è©±ç®¡ç†ã‚’æ‹…å½“ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, supabase_client, user_id: Optional[int] = None):
        super().__init__(supabase_client, user_id)
        self.history_limit_default = int(os.environ.get("CHAT_HISTORY_LIMIT_DEFAULT", "50"))
        self.history_limit_max = int(os.environ.get("CHAT_HISTORY_LIMIT_MAX", "100"))
    
    def get_service_name(self) -> str:
        return "ChatService"

    def dump_response_events(self, resp):
        logger = logging.getLogger(__name__)

        try:
            logger.info(f"dump_response_events: resp type={type(resp)} repr={repr(resp)[:200]}")

            events = getattr(resp, "output", []) or []
            logger.info("=== Response.output events dump ===")
            for i, ev in enumerate(events):
                # SDKã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’dictåŒ–ã—ã¦å‡ºã™ï¼ˆå¯èƒ½ãªã‚‰ï¼‰
                if hasattr(ev, "model_dump"):
                    logger.info(f"[{i}] {json.dumps(ev.model_dump(), ensure_ascii=False)}")
                else:
                    logger.info(f"[{i}] {repr(ev)}")
        except Exception as e:
            logger.exception(f"Failed to dump response events: {e}")
    
    async def process_chat_message(
        self,
        message: str,
        user_id: int,
        project_id: Optional[str] = None,
        session_type: str = "general",
        response_style: Optional[str] = "auto",
        custom_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆå‡¦ç†ï¼ˆçµ±åˆæœ€é©åŒ–ç‰ˆï¼‰"""
        start_time = time.time()
        metrics = {
            "db_fetch_time": 0,
            "llm_response_time": 0,
            "db_save_time": 0,
            "total_time": 0
        }
        
        try:
            # Phase 1: ä¸¦åˆ—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ»å±¥æ­´å–å¾—
            fetch_start = time.time()
            
            # AsyncDatabaseHelperã¨AsyncProjectContextBuilderã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            from async_helpers import AsyncDatabaseHelper, AsyncProjectContextBuilder
            db_helper = AsyncDatabaseHelper(self.supabase)
            # AsyncProjectContextBuilder ã¯ AsyncDatabaseHelper ã‚’å—ã‘å–ã‚‹
            context_builder = AsyncProjectContextBuilder(db_helper)
            
            # ä¼šè©±IDã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
            conversation_id = self.get_or_create_conversation_sync(session_type)
            
            project_id, project_context, project, conversation_history = \
                await parallel_fetch_context_and_history(
                    db_helper, 
                    context_builder,
                    project_id or "",  # page_id
                    conversation_id,   # conversation_id
                    user_id,          # user_id
                    self.history_limit_default
                )
            metrics["db_fetch_time"] = time.time() - fetch_start
            
            # Phase 2: AIå¿œç­”ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ä»˜ãï¼‰
            llm_start = time.time()
            ai_response = await self._generate_ai_response(
                message, user_id, project_context, conversation_history, session_type, response_style, custom_instruction
            )

            metrics["llm_response_time"] = time.time() - llm_start
            
            # Phase 3: ä¸¦åˆ—ãƒ­ã‚°ä¿å­˜
            save_start = time.time()
            
            # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            context_data = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "project_id": project_id
            }
            
            user_message_data = {
                "user_id": user_id,
                "page_id": project_id or "",
                "sender": "user",
                "message": message,
                "conversation_id": conversation_id,
                "context_data": context_data
            }
            ai_message_data = {
                "user_id": user_id,
                "page_id": project_id or "",
                "sender": "ai",
                "message": ai_response["response"],
                "conversation_id": conversation_id,
                "context_data": context_data
            }
            
            # å¾“æ¥ã®ä¿å­˜å‡¦ç†ã‚’ä½¿ç”¨ï¼ˆturn_indexãƒã‚°ä¿®æ­£ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
            user_saved, ai_saved = await parallel_save_chat_logs(
                db_helper,
                user_message_data,
                ai_message_data
            )
            metrics["db_save_time"] = time.time() - save_start
            
            # Phase 4: éåŒæœŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°ï¼ˆãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            if conversation_id:
                asyncio.create_task(self._update_conversation_timestamp_async(conversation_id))
            
            metrics["total_time"] = time.time() - start_time
            
            return {
                "response": ai_response["response"],
                "project_id": project_id,
                "metrics": metrics,
                "agent_used": ai_response.get("agent_used", False),
                "fallback_used": ai_response.get("fallback_used", False),
                # è³ªå•æ˜ç¢ºåŒ–æ©Ÿèƒ½ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                "is_clarification": ai_response.get("is_clarification", False),
                "clarification_questions": ai_response.get("clarification_questions"),
                "suggestion_options": ai_response.get("suggestion_options"),
                # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                "response_style_used": ai_response.get("response_style_used")
            }
            
        except Exception as e:
            error_result = self.handle_error(e, "Chat processing")
            self.logger.error(f"Chat processing failed for user {user_id}: {e}")
            raise Exception(error_result["error"])
    
    async def _generate_ai_response(
        self,
        message: str,
        user_id: int,
        project_context: str,
        conversation_history: List[Dict],
        session_type: str,
        response_style: Optional[str] = "auto",
        custom_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """AIå¿œç­”ç”Ÿæˆï¼ˆè³ªå•æ˜ç¢ºåŒ–æ©Ÿèƒ½ä»˜ãï¼‰"""

        # ç’°å¢ƒå¤‰æ•°ã§æ©Ÿèƒ½ã®ON/OFFã‚’åˆ¶å¾¡
        enable_clarification = os.environ.get("ENABLE_CLARIFICATION", "true").lower() == "true"

        # é•·è€ƒãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã®ã¿è³ªå•ã®æŠ½è±¡åº¦ã‚’åˆ¤å®š
        is_deep_thinking = response_style in ["research", "deepen"]

        if enable_clarification and not is_deep_thinking:
            intent = await self._classify_question_intent(message)

            # æŠ½è±¡çš„ãªè³ªå•ã®å ´åˆã¯æ˜ç¢ºåŒ–è³ªå•ã‚’ç”Ÿæˆ
            if intent == "abstract":
                try:
                    return await self._generate_clarification_questions(message)
                except Exception as e:
                    self.logger.warning(f"Clarification failed, falling back to normal response: {e}")
                    # æ˜ç¢ºåŒ–å¤±æ•—æ™‚ã¯é€šå¸¸ã®å¿œç­”ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        # é€šå¸¸ã®å¿œç­”ç”Ÿæˆ
        # éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
        try:
            return await self._process_with_async_llm(
                message, project_context, conversation_history, response_style, custom_instruction
            )
        except Exception as e:
            self.logger.warning(f"Async LLM failed, falling back to sync: {e}")
            # åŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            return await self._process_with_sync_llm(
                message, project_context, conversation_history, response_style, custom_instruction
            )
    
    
    async def _process_with_async_llm(
        self,
        message: str,
        project_context: str,
        conversation_history: List[Dict],
        response_style: Optional[str] = "auto",
        custom_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚ˆã‚‹å‡¦ç†"""
        try:
            from module.llm_api import get_async_llm_client
            from .response_styles import ResponseStyleManager

            # NOTE: get_async_llm_client ã¯åˆå›å‘¼ã³å‡ºã—ã® pool_sizeï¼ˆ=Semaphoreä¸Šé™ï¼‰ã®ã¿æœ‰åŠ¹
            pool_size = int(os.environ.get("LLM_POOL_SIZE", "5"))
            llm_client = get_async_llm_client(pool_size=pool_size)  # awaitã¯ä¸è¦
            if not llm_client:
                raise Exception("Async LLM client not available")

            context_data = self._build_context_data(project_context, conversation_history)

            # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            if response_style == "custom" and custom_instruction:
                # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«ã®å ´åˆã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æŒ‡ç¤ºã‚’åŸ‹ã‚è¾¼ã‚€
                system_prompt = RESPONSE_STYLE_PROMPTS["custom"].replace("{custom_instruction}", custom_instruction)
            else:
                system_prompt = ResponseStyleManager.get_system_prompt(response_style)

            # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã‚’è¨­å®š
            # é•·è€ƒãƒ¢ãƒ¼ãƒ‰: research, deepen â†’ åˆ¶é™ãªã—ï¼ˆå¾“æ¥é€šã‚Šï¼‰
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: organize, expand, ideas â†’ 300ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç´„400æ–‡å­—ï¼‰
            is_deep_thinking = response_style in ["research", "deepen"]
            max_tokens = None if is_deep_thinking else int(os.environ.get("DEFAULT_MAX_TOKENS", "300"))

            # llm_clientã®generate_response_asyncãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
            input_items = [
                llm_client.text("system", system_prompt),
                llm_client.text("user", f"{context_data}\n\n{message}")
            ]
            response_obj = await llm_client.generate_response_async(input_items, max_tokens=max_tokens)

            # Webæ¤œç´¢å®Ÿè¡Œç¢ºèªã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆResponseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾ã—ã¦è¡Œã†ï¼‰
            self.dump_response_events(response_obj)
            
            # Response APIã®output_textã‚’å–å¾—
            response = llm_client.extract_output_text(response_obj)

            # selectã‚¹ã‚¿ã‚¤ãƒ«ã®å ´åˆã¯JSONå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            if response_style == "select":
                try:
                    # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    if json_start != -1 and json_end > json_start:
                        json_text = response[json_start:json_end]
                        parsed = json.loads(json_text)

                        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è¡Œå‹•ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                        message_text = parsed.get('message', '')
                        action_options = parsed.get('action_options', [])[:3]  # 3ã¤ã¾ã§

                        return {
                            "response": message_text,
                            "agent_used": False,
                            "fallback_used": False,
                            "response_style_used": response_style,
                            "suggestion_options": action_options  # è¡Œå‹•é¸æŠè‚¢ã¨ã—ã¦è¿”ã™
                        }
                except Exception as parse_error:
                    self.logger.warning(f"Select style JSON parse failed: {parse_error}")
                    # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯é€šå¸¸å¿œç­”ã¨ã—ã¦è¿”ã™

            return {
                "response": response,
                "agent_used": False,
                "fallback_used": False,
                "response_style_used": response_style  # ä½¿ç”¨ã—ãŸå¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨˜éŒ²
            }
            
        except Exception as e:
            self.logger.error(f"Async LLM error: {e}")
            raise
    
    async def _process_with_sync_llm(
        self,
        message: str,
        project_context: str,
        conversation_history: List[Dict],
        response_style: Optional[str] = "auto",
        custom_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """åŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚ˆã‚‹å‡¦ç†ï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            import sys
            #sys.path.append('C:\\Users\\kouta\\learning-assistant')
            from module.llm_api import learning_plannner
            from .response_styles import ResponseStyleManager

            context_data = self._build_context_data(project_context, conversation_history)

            # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            if response_style == "custom" and custom_instruction:
                # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«ã®å ´åˆã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æŒ‡ç¤ºã‚’åŸ‹ã‚è¾¼ã‚€
                system_prompt = RESPONSE_STYLE_PROMPTS["custom"].replace("{custom_instruction}", custom_instruction)
            else:
                system_prompt = ResponseStyleManager.get_system_prompt(response_style)

            # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ã‚’è¨­å®š
            # é•·è€ƒãƒ¢ãƒ¼ãƒ‰: research, deepen â†’ åˆ¶é™ãªã—ï¼ˆå¾“æ¥é€šã‚Šï¼‰
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: organize, expand, ideas â†’ 300ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç´„400æ–‡å­—ï¼‰
            is_deep_thinking = response_style in ["research", "deepen"]
            max_tokens = None if is_deep_thinking else int(os.environ.get("DEFAULT_MAX_TOKENS", "300"))

            # learning_plannnerã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            llm_instance = learning_plannner()

            # åŒæœŸå‡¦ç†ã‚’éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿè¡Œ
            input_items = [
                llm_instance.text("system", system_prompt),
                llm_instance.text("user", f"{context_data}\n\n{message}")
            ]

            # max_tokensã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
            if max_tokens:
                response_obj = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: llm_instance.generate_response(input_items, max_tokens)
                )
            else:
                response_obj = await asyncio.get_event_loop().run_in_executor(
                    None,
                    llm_instance.generate_response,
                    input_items
                )

            # Webæ¤œç´¢å®Ÿè¡Œç¢ºèªã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆResponseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾ã—ã¦è¡Œã†ï¼‰
            self.dump_response_events(response_obj)
            
            # Response APIã®output_textã‚’å–å¾—
            response = llm_instance.extract_output_text(response_obj)

            # selectã‚¹ã‚¿ã‚¤ãƒ«ã®å ´åˆã¯JSONå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            if response_style == "select":
                try:
                    # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
                    json_start = response.find('{')
                    json_end = response.rfind('}') + 1
                    if json_start != -1 and json_end > json_start:
                        json_text = response[json_start:json_end]
                        parsed = json.loads(json_text)

                        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è¡Œå‹•ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                        message_text = parsed.get('message', '')
                        action_options = parsed.get('action_options', [])[:3]  # 3ã¤ã¾ã§

                        return {
                            "response": message_text,
                            "agent_used": False,
                            "fallback_used": True,
                            "response_style_used": response_style,
                            "suggestion_options": action_options  # è¡Œå‹•é¸æŠè‚¢ã¨ã—ã¦è¿”ã™
                        }
                except Exception as parse_error:
                    self.logger.warning(f"Select style JSON parse failed: {parse_error}")
                    # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯é€šå¸¸å¿œç­”ã¨ã—ã¦è¿”ã™

            return {
                "response": response,
                "agent_used": False,
                "fallback_used": True,
                "response_style_used": response_style  # ä½¿ç”¨ã—ãŸå¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨˜éŒ²
            }
            
        except Exception as e:
            self.logger.error(f"Sync LLM error: {e}")
            raise Exception("All LLM processing methods failed")
    
    def get_chat_history(self, user_id: int, limit: int = 20) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒƒãƒˆå±¥æ­´å–å¾—"""
        try:
            limit = min(limit, 100)  # æœ€å¤§100ä»¶
            
            result = self.supabase.table("chat_logs")\
                .select("id, message, sender, created_at")\
                .eq("user_id", user_id)\
                .order("created_at", desc=True)\
                .limit(limit)\
                .execute()
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨AIå¿œç­”ã‚’ãƒšã‚¢ã«ã—ã¦è¿”ã™
            history = []
            for i, log in enumerate(result.data):
                if log["sender"] == "user":
                    # æ¬¡ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒAIå¿œç­”ã‹ãƒã‚§ãƒƒã‚¯
                    ai_response = ""
                    if i + 1 < len(result.data) and result.data[i + 1]["sender"] == "ai":
                        ai_response = result.data[i + 1]["message"]
                    
                    history.append({
                        "id": log["id"],
                        "message": log["message"],
                        "response": ai_response,
                        "timestamp": log["created_at"],
                        "sender": log["sender"],
                        "created_at": log["created_at"]
                    })
            
            return history
            
        except Exception as e:
            error_result = self.handle_error(e, "Get chat history")
            self.logger.error(f"Failed to get chat history: {e}")
            return []
    
    def get_or_create_conversation_sync(self, session_type: str = "general") -> str:
        """ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†"""
        try:
            if not self.user_id:
                raise ValueError("User ID is required for conversation management")
            
            # æ—¢å­˜ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªä¼šè©±ã‚’æ¤œç´¢
            result = self.supabase.table("chat_conversations")\
                .select("id")\
                .eq("user_id", self.user_id)\
                .eq("is_active", True)\
                .order("updated_at", desc=True)\
                .limit(1)\
                .execute()
            
            if result.data:
                return result.data[0]["id"]
            
            # æ–°ã—ã„ä¼šè©±ã‚’ä½œæˆ
            new_conversation = self.supabase.table("chat_conversations").insert({
                "user_id": self.user_id,
                "title": f"AIãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ - {session_type}",
                "is_active": True,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "updated_at": datetime.now(timezone.utc).isoformat()
            }).execute()
            
            return new_conversation.data[0]["id"]
            
        except Exception as e:
            self.logger.error(f"Conversation management error: {e}")
            # UUIDã‚’ç”Ÿæˆã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆUUIDå‹ã®ã‚«ãƒ©ãƒ ã«å¯¾å¿œï¼‰
            return str(uuid.uuid4())
    
    def _build_context_data(
        self,
        project_context: str,
        conversation_history: List[Dict]
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        context_parts = []
        
        if project_context:
            context_parts.append(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±:\n{project_context}")
        
        if conversation_history:
            history_text = "\n".join([
                f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {item.get('message', '')}\nAI: {item.get('response', '')}"
                for item in conversation_history[-10:]  # æœ€æ–°10ä»¶
            ])
            context_parts.append(f"ä¼šè©±å±¥æ­´:\n{history_text}")
        
        return "\n\n".join(context_parts)
    
    
    async def _update_conversation_timestamp_async(self, conversation_id: str) -> None:
        """éåŒæœŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°ï¼ˆãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰"""
        try:
            await asyncio.sleep(0)  # éåŒæœŸå®Ÿè¡Œ
            self.supabase.table("chat_conversations")\
                .update({"updated_at": datetime.now(timezone.utc).isoformat()})\
                .eq("id", conversation_id)\
                .execute()
        except Exception as e:
            self.logger.warning(f"Conversation timestamp update failed: {e}")

    async def _classify_question_intent(self, message: str) -> str:
        """
        è³ªå•ã®æŠ½è±¡åº¦ã‚’åˆ¤å®š

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            "abstract" | "specific"
        """
        # ç°¡æ˜“åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        abstract_keywords = [
            "ã«ã¤ã„ã¦", "ã¨ã¯", "ã©ã†", "ãªãœ", "ãªã«", "ä½•",
            "æ­´å²", "å…¨ä½“", "åŸºæœ¬", "æ¦‚è¦", "æ•™ãˆã¦", "çŸ¥ã‚ŠãŸã„"
        ]
        specific_keywords = [
            "ã©ã®ã‚ˆã†ã«", "æ‰‹é †", "æ–¹æ³•", "ã‚„ã‚Šæ–¹", "å…·ä½“çš„",
            "ã„ã¤", "ã©ã“ã§", "ã‚¹ãƒ†ãƒƒãƒ—", "å®Ÿè£…", "ã‚³ãƒ¼ãƒ‰"
        ]

        message_lower = message.lower()
        abstract_count = sum(1 for kw in abstract_keywords if kw in message_lower)
        specific_count = sum(1 for kw in specific_keywords if kw in message_lower)

        message_length = len(message)

        # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        # 1. æ–‡å­—æ•°ãŒçŸ­ãï¼ˆ50æ–‡å­—æœªæº€ï¼‰ã€æŠ½è±¡çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå¤šã„
        if message_length < 50 and abstract_count > specific_count:
            return "abstract"

        # 2. éå¸¸ã«çŸ­ã„è³ªå•ï¼ˆ30æ–‡å­—æœªæº€ï¼‰ã§æŠ½è±¡çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ1ã¤ä»¥ä¸Š
        if message_length < 30 and abstract_count > 0:
            return "abstract"

        return "specific"

    async def _generate_clarification_questions(self, message: str) -> Dict[str, Any]:
        """
        æŠ½è±¡çš„ãªè³ªå•ã«å¯¾ã™ã‚‹3ã¤ã®æ˜ç¢ºåŒ–è³ªå•ã¨é¸æŠè‚¢ã‚’ç”Ÿæˆ

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ½è±¡çš„ãªè³ªå•

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã®æ˜ç¢ºåŒ–è³ªå•ã‚’å«ã‚€å¿œç­”
        """
        from module.llm_api import get_async_llm_client
        from prompt.prompt import CLARIFICATION_PROMPT

        llm_client = get_async_llm_client()

        # æ˜ç¢ºåŒ–è³ªå•ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œ
        prompt_text = CLARIFICATION_PROMPT.replace("{user_message}", message)
        input_items = [
            llm_client.text("system", prompt_text),
            llm_client.text("user", message)
        ]

        # è³ªå•ç”Ÿæˆã¯é«˜é€ŸåŒ–ã®ãŸã‚max_tokensåˆ¶é™
        response_obj = await llm_client.generate_response_async(input_items, max_tokens=500)
        response_text = llm_client.extract_output_text(response_obj)

        # JSONãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ä»˜ãï¼‰
        try:
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                parsed = json.loads(json_text)
            else:
                raise ValueError("JSON not found in response")

            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            formatted_response = f"{parsed['summary']}\n\n"
            formatted_response += "ä»¥ä¸‹ã®ç‚¹ã«ã¤ã„ã¦ã€è©³ã—ãæ•™ãˆã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ\n\n"

            # è³ªå•ãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            questions_list = []
            all_options = []

            # è³ªå•æ•°ã‚’3ã¤ã«åˆ¶é™
            clarification_questions = parsed['clarification_questions'][:3]

            for i, q_data in enumerate(clarification_questions, 1):
                question_text = q_data['question']
                options = q_data.get('options', [])[:3]  # å„è³ªå•ã®é¸æŠè‚¢ã‚‚3ã¤ã¾ã§

                formatted_response += f"{i}. {question_text}\n"
                if options:
                    formatted_response += f"   ï¼ˆä¾‹: {', '.join(options)}ï¼‰\n"

                questions_list.append(question_text)
                all_options.extend(options)

            # quick_optionsã‚‚è¿½åŠ ï¼ˆ3ã¤ã¾ã§ï¼‰
            quick_opts = parsed.get('quick_options', [])[:3]
            all_options.extend(quick_opts)

            formatted_response += "\nğŸ’¡ ç´°ã‹ãå¸Œæœ›ãŒãªã‘ã‚Œã°ã€ä¸Šè¨˜ã®é¸æŠè‚¢ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ã€‚"

            return {
                "response": formatted_response,
                "agent_used": False,
                "fallback_used": False,
                "is_clarification": True,
                "clarification_questions": questions_list,
                "suggestion_options": all_options,  # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªå…¨é¸æŠè‚¢
                "response_style_used": "clarification"  # æ˜ç¢ºåŒ–è³ªå•ãƒ¢ãƒ¼ãƒ‰
            }

        except Exception as parse_error:
            self.logger.warning(f"JSON parse failed for clarification: {parse_error}")

            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯é€šå¸¸å¿œç­”ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "response": f"ã€Œ{message}ã€ã«ã¤ã„ã¦ã€ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ\n\nä¾‹ãˆã°ã€çŸ¥ã‚ŠãŸã„ç¯„å›²ã‚„ç›®çš„ã€å…·ä½“çš„ãªé–¢å¿ƒäº‹é …ãªã©ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
                "agent_used": False,
                "fallback_used": True,
                "is_clarification": False
            }

