# services/quest_map_ai.py - æ¢Qãƒãƒƒãƒ—æ©Ÿèƒ½ã®AIé€£æºã‚µãƒ¼ãƒ“ã‚¹

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timezone
import json
import re
import hashlib
from functools import wraps
import time

from services.base import BaseService
from module.xai_llm_adapter import XAILLMAdapter
from schemas.quest_map import (
    NodeType, NodeStatus, EdgeType,
    GeneratedNodeOption, NodeGenerationResponse,
    BreakdownNodeOption, NodeBreakdownResponse,
    AlternativeNodeOption, NodeExpansionResponse,
    NodeRecommendation, RecommendationResponse
)
from prompts.quest_map_prompts import (
    QuestMapPrompts,
    PromptCategory,
    PersonaType
)

logger = logging.getLogger(__name__)


# ===== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ =====

def cache_result(cache_duration_minutes: int = 30):
    """AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        cache = {}
        
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆ
            cache_key = _generate_cache_key(args, kwargs)
            current_time = time.time()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆãƒã‚§ãƒƒã‚¯
            if cache_key in cache:
                cached_data, timestamp = cache[cache_key]
                if current_time - timestamp < cache_duration_minutes * 60:
                    logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {func.__name__}")
                    return cached_data
            
            # å®Ÿéš›ã®é–¢æ•°ã‚’å®Ÿè¡Œ
            result = await func(*args, **kwargs)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            cache[cache_key] = (result, current_time)
            
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            _cleanup_cache(cache, cache_duration_minutes)
            
            return result
        
        return wrapper
    return decorator


def retry_on_failure(max_retries: int = 3, delay_seconds: int = 1):
    """å¤±æ•—æ™‚ã«ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        logger.warning(f"âš ï¸ {func.__name__} å¤±æ•— (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                        await asyncio.sleep(delay_seconds * (attempt + 1))  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    else:
                        logger.error(f"âŒ {func.__name__} æœ€çµ‚çš„ã«å¤±æ•—: {e}")
            
            raise last_exception
        
        return wrapper
    return decorator


def _generate_cache_key(args: tuple, kwargs: dict) -> str:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    content = str(args) + str(sorted(kwargs.items()))
    return hashlib.md5(content.encode()).hexdigest()


def _cleanup_cache(cache: dict, cache_duration_minutes: int):
    """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    current_time = time.time()
    expired_keys = [
        key for key, (_, timestamp) in cache.items()
        if current_time - timestamp >= cache_duration_minutes * 60
    ]
    
    for key in expired_keys:
        del cache[key]
    
    if expired_keys:
        logger.info(f"ğŸ§¹ {len(expired_keys)}å€‹ã®æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")


# ===== ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒªãƒ¼ã‚¯ãƒ©ã‚¹ =====

class AIErrorRecovery:
    """AIç”Ÿæˆå¤±æ•—æ™‚ã®ãƒªã‚«ãƒãƒªãƒ¼ãƒ¡ã‚«ãƒ‹ã‚ºãƒ """
    
    @staticmethod
    def get_fallback_generation_response(quest_id: int, goal: str) -> NodeGenerationResponse:
        """é¸æŠè‚¢ç”Ÿæˆå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        fallback_nodes = [
            GeneratedNodeOption(
                title="ç›®æ¨™ã®è©³ç´°åˆ†æ",
                description=f"ã€Œ{goal}ã€ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªè¦ç´ ã¨é”æˆæ¡ä»¶ã‚’è©³ã—ãåˆ†æã—ã¾ã—ã‚‡ã†",
                type=NodeType.ACTION,
                category="analysis",
                priority=5,
                difficulty=2,
                estimated_duration="30åˆ†",
                prerequisites=[],
                expected_outcome="ç›®æ¨™ã®æ˜ç¢ºåŒ–ã¨å…·ä½“åŒ–"
            ),
            GeneratedNodeOption(
                title="ç¾çŠ¶ã®æŠŠæ¡ã¨æ•´ç†",
                description="ç¾åœ¨ã®çŠ¶æ³ã€æŒã£ã¦ã„ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã€åˆ¶ç´„æ¡ä»¶ã‚’æ•´ç†ã—ã¦æŠŠæ¡ã—ã¾ã—ã‚‡ã†",
                type=NodeType.ACTION,
                category="preparation",
                priority=4,
                difficulty=1,
                estimated_duration="20åˆ†",
                prerequisites=[],
                expected_outcome="ç¾çŠ¶ã®æ˜ç¢ºãªç†è§£"
            ),
            GeneratedNodeOption(
                title="å°ã•ãªç¬¬ä¸€æ­©ã®è¨ˆç”»",
                description="ç›®æ¨™ã«å‘ã‹ã£ã¦ä»Šã™ãå§‹ã‚ã‚‰ã‚Œã‚‹å°ã•ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç”»ã—ã¾ã—ã‚‡ã†",
                type=NodeType.ACTION,
                category="action",
                priority=3,
                difficulty=1,
                estimated_duration="15åˆ†",
                prerequisites=["ç¾çŠ¶ã®æŠŠæ¡"],
                expected_outcome="å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»"
            )
        ]
        
        return NodeGenerationResponse(
            quest_id=quest_id,
            suggested_nodes=fallback_nodes,
            reasoning="AIç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€åŸºæœ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å§‹ã‚ã¦ã€ã‚ˆã‚Šå…·ä½“çš„ãªè¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",
            next_steps_advice="ã¾ãšç›®æ¨™ã®è©³ç´°åˆ†æã‹ã‚‰å§‹ã‚ã¦ã€æ®µéšçš„ã«å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç”»ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
        )
    
    @staticmethod
    def get_fallback_breakdown_response(node_id: int, title: str, description: str) -> NodeBreakdownResponse:
        """åˆ†è§£å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        fallback_subtasks = [
            BreakdownNodeOption(
                title=f"{title} - è¨ˆç”»ç«‹æ¡ˆ",
                description=f"ã€Œ{description}ã€ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®è©³ç´°ãªè¨ˆç”»ã‚’ç«‹ã¦ã‚‹",
                order=1,
                type=NodeType.ACTION,
                estimated_duration="è¨ˆç”»å†…å®¹ã«ã‚ˆã‚Šå¤‰å‹•",
                dependencies=[]
            ),
            BreakdownNodeOption(
                title=f"{title} - æº–å‚™ãƒ»ãƒªã‚½ãƒ¼ã‚¹ç¢ºä¿",
                description="å¿…è¦ãªè³‡æ–™ã€ãƒ„ãƒ¼ãƒ«ã€æƒ…å ±ã‚’é›†ã‚ã¦æº–å‚™ã™ã‚‹",
                order=2,
                type=NodeType.ACTION,
                estimated_duration="æº–å‚™å†…å®¹ã«ã‚ˆã‚Šå¤‰å‹•",
                dependencies=[1]
            ),
            BreakdownNodeOption(
                title=f"{title} - å®Ÿè¡Œ",
                description="æº–å‚™ã—ãŸè¨ˆç”»ã«åŸºã¥ã„ã¦å®Ÿéš›ã®ä½œæ¥­ã‚’å®Ÿè¡Œã™ã‚‹",
                order=3,
                type=NodeType.ACTION,
                estimated_duration="ä½œæ¥­å†…å®¹ã«ã‚ˆã‚Šå¤‰å‹•",
                dependencies=[2]
            ),
            BreakdownNodeOption(
                title=f"{title} - ç¢ºèªãƒ»èª¿æ•´",
                description="å®Ÿè¡Œçµæœã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ãƒ»æ”¹å–„ã™ã‚‹",
                order=4,
                type=NodeType.ACTION,
                estimated_duration="ç¢ºèªå†…å®¹ã«ã‚ˆã‚Šå¤‰å‹•",
                dependencies=[3]
            )
        ]
        
        return NodeBreakdownResponse(
            original_node_id=node_id,
            subtasks=fallback_subtasks,
            reasoning="AIåˆ†è§£ã«å¤±æ•—ã—ãŸãŸã‚ã€ä¸€èˆ¬çš„ãª4æ®µéšï¼ˆè¨ˆç”»â†’æº–å‚™â†’å®Ÿè¡Œâ†’ç¢ºèªï¼‰ã«åˆ†å‰²ã—ã¦ã„ã¾ã™ã€‚",
            completion_criteria="å…¨ã¦ã®æ®µéšãŒå®Œäº†ã—ã€æœŸå¾…ã•ã‚ŒãŸæˆæœãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨"
        )
    
    @staticmethod
    def get_fallback_expansion_response(node_id: int, title: str, description: str) -> NodeExpansionResponse:
        """æ‹¡æ•£å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        fallback_alternatives = [
            AlternativeNodeOption(
                title=f"{title}ï¼ˆæ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰",
                description=f"ã€Œ{description}ã€ã‚’å°ã•ãªã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã¦æ®µéšçš„ã«é€²ã‚ã‚‹æ–¹æ³•",
                approach="ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã—ã€ç¢ºå®Ÿæ€§ã‚’é‡è¦–ã—ãŸæ®µéšçš„å®Ÿè¡Œ",
                pros=["ãƒªã‚¹ã‚¯ãŒä½ã„", "é€²æ—ãŒè¦‹ãˆã‚„ã™ã„", "ä¿®æ­£ã—ã‚„ã™ã„"],
                cons=["æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚‹", "ã‚¹ãƒ”ãƒ¼ãƒ‰ãŒé…ã„"],
                difficulty=2,
                risk_level=1
            ),
            AlternativeNodeOption(
                title=f"{title}ï¼ˆé›†ä¸­ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰",
                description=f"ã€Œ{description}ã€ã«é›†ä¸­çš„ã«å–ã‚Šçµ„ã¿ã€çŸ­æœŸé–“ã§å®Œæˆã•ã›ã‚‹æ–¹æ³•",
                approach="åŠ¹ç‡æ€§ã¨ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚’é‡è¦–ã—ãŸé›†ä¸­çš„å®Ÿè¡Œ",
                pros=["çŸ­æœŸé–“ã§å®Œæˆ", "é›†ä¸­åŠ¹æœãŒé«˜ã„", "ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç¶­æŒã—ã‚„ã™ã„"],
                cons=["è² æ‹…ãŒå¤§ãã„", "ä»–ã®ä½œæ¥­ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§"],
                difficulty=4,
                risk_level=3
            ),
            AlternativeNodeOption(
                title=f"{title}ï¼ˆå”åƒã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰",
                description=f"ã€Œ{description}ã€ã‚’ä»–è€…ã¨å”åŠ›ã—ã¦é€²ã‚ã‚‹æ–¹æ³•",
                approach="ä»–è€…ã¨ã®å”åŠ›ãƒ»å”åƒã«ã‚ˆã‚‹å®Ÿè¡Œ",
                pros=["è² æ‹…åˆ†æ•£", "å¤šæ§˜ãªè¦–ç‚¹", "å­¦ç¿’åŠ¹æœãŒé«˜ã„"],
                cons=["èª¿æ•´ãŒå¿…è¦", "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«èª¿æ•´ãŒè¤‡é›‘"],
                difficulty=3,
                risk_level=2
            )
        ]
        
        return NodeExpansionResponse(
            original_node_id=node_id,
            alternatives=fallback_alternatives,
            reasoning="AIæ‹¡æ•£ã«å¤±æ•—ã—ãŸãŸã‚ã€ä¸€èˆ¬çš„ãª3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚",
            recommendation="çŠ¶æ³ã¨å€‹äººã®ç‰¹æ€§ã«å¿œã˜ã¦ã€æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚"
        )


class QuestMapAIService(BaseService):
    """æ¢Qãƒãƒƒãƒ—æ©Ÿèƒ½ã®AIé€£æºã‚µãƒ¼ãƒ“ã‚¹ï¼ˆxAI Grokä½¿ç”¨ï¼‰"""
    
    def __init__(self, supabase_client, user_id: Optional[int] = None):
        super().__init__(supabase_client, user_id)
        try:
            self.llm_client = XAILLMAdapter(model="grok-4-1-fast-reasoning", pool_size=5)
            self.prompt_builder = QuestMapPrompts()
            logger.info("âœ… QuestMapAIService: xAI Grokã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ QuestMapAIService: xAI Grokã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
            raise
    
    def get_service_name(self) -> str:
        return "QuestMapAIService"
    
    def _detect_user_persona(self, user_context: Optional[Dict[str, Any]] = None) -> PersonaType:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒšãƒ«ã‚½ãƒŠã‚’æ¨å®š"""
        if not user_context:
            return PersonaType.INTERMEDIATE
        
        # ç°¡å˜ãªãƒšãƒ«ã‚½ãƒŠæ¨å®šãƒ­ã‚¸ãƒƒã‚¯
        experience_level = user_context.get("experience_level", "medium")
        learning_style = user_context.get("learning_style", "balanced")
        
        if experience_level == "beginner":
            return PersonaType.BEGINNER
        elif experience_level == "expert":
            return PersonaType.ADVANCED
        elif learning_style == "creative":
            return PersonaType.CREATIVE
        elif learning_style == "analytical":
            return PersonaType.ANALYTICAL
        elif learning_style == "practical":
            return PersonaType.PRACTICAL
        else:
            return PersonaType.INTERMEDIATE
    
    @retry_on_failure(max_retries=3, delay_seconds=2)
    @cache_result(cache_duration_minutes=15)
    async def generate_action_nodes(
        self,
        quest_id: int,
        goal: str,
        current_context: Optional[str] = None,
        node_count: int = 5,
        focus_category: Optional[str] = None,
        user_context: Optional[Dict[str, Any]] = None,
        user_preferences: Optional[Dict[str, Any]] = None
    ) -> NodeGenerationResponse:
        """
        ã‚´ãƒ¼ãƒ«ã¨ç¾çŠ¶ã‹ã‚‰é¸æŠè‚¢ã‚’ç”Ÿæˆ
        
        Args:
            quest_id: ã‚¯ã‚¨ã‚¹ãƒˆID
            goal: é”æˆã—ãŸã„ç›®æ¨™
            current_context: ç¾åœ¨ã®çŠ¶æ³ãƒ»èƒŒæ™¯æƒ…å ±
            node_count: ç”Ÿæˆã™ã‚‹é¸æŠè‚¢ã®æ•°
            focus_category: ç‰¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã‚«ãƒ†ã‚´ãƒª
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
            user_preferences: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨­å®šãƒ»å¥½ã¿
            
        Returns:
            NodeGenerationResponse: ç”Ÿæˆã•ã‚ŒãŸé¸æŠè‚¢
        """
        try:
            # ãƒšãƒ«ã‚½ãƒŠæ¨å®š
            persona = self._detect_user_persona(user_context)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            system_prompt = self.prompt_builder.build_system_prompt(
                PromptCategory.GENERATION,
                persona,
                user_context
            )
            user_prompt = self.prompt_builder.build_generation_prompt(
                goal, current_context, node_count, focus_category,
                persona, user_preferences
            )
            
            input_items = [
                self.llm_client.text("system", system_prompt),
                self.llm_client.text("user", user_prompt)
            ]
            
            logger.info(f"ğŸ” AIãƒãƒ¼ãƒ‰ç”Ÿæˆé–‹å§‹: quest_id={quest_id}, goal='{goal[:50]}...'")
            
            # AIå¿œç­”ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†è¿½åŠ ï¼‰
            try:
                response_text = await asyncio.wait_for(
                    self.llm_client.generate_text(input_items, max_tokens=2000),
                    timeout=55.0  # 55ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆHTTPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚ˆã‚Šå°‘ã—çŸ­ãï¼‰
                )
                # xAI Grokç”¨: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’é©ç”¨
                if hasattr(self.llm_client, 'optimize_prompt_for_grok'):
                    logger.debug("ğŸ“ xAI Grokç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’å®Ÿè¡Œ")
            except asyncio.TimeoutError:
                logger.error(f"â° AIãƒãƒ¼ãƒ‰ç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: quest_id={quest_id}")
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ç°¡æ˜“çš„ãªå¿œç­”ã‚’è¿”ã™
                return AIErrorRecovery.get_fallback_generation_response(quest_id, goal)
            
            # å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            parsed_response = self._parse_generation_response(response_text, quest_id)
            
            logger.info(f"âœ… AIãƒãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†: {len(parsed_response.suggested_nodes)}å€‹ã®é¸æŠè‚¢ã‚’ç”Ÿæˆ")
            
            return parsed_response
            
        except Exception as e:
            error_msg = f"é¸æŠè‚¢ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒªãƒ¼
            return AIErrorRecovery.get_fallback_generation_response(quest_id, goal)
    
    async def breakdown_node(
        self,
        node_id: int,
        node_title: str,
        node_description: str,
        detail_level: int = 3,
        context: Optional[str] = None
    ) -> NodeBreakdownResponse:
        """
        ãƒãƒ¼ãƒ‰ã‚’ç´°åˆ†åŒ–
        
        Args:
            node_id: ãƒãƒ¼ãƒ‰ID
            node_title: ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«
            node_description: ãƒãƒ¼ãƒ‰èª¬æ˜
            detail_level: è©³ç´°ãƒ¬ãƒ™ãƒ«ï¼ˆ2-5ï¼‰
            context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            NodeBreakdownResponse: åˆ†è§£ã•ã‚ŒãŸå­ã‚¿ã‚¹ã‚¯
        """
        try:
            system_prompt = self._build_breakdown_system_prompt(detail_level)
            user_prompt = self._build_breakdown_user_prompt(
                node_title, node_description, context, detail_level
            )
            
            input_items = [
                self.llm_client.text("system", system_prompt),
                self.llm_client.text("user", user_prompt)
            ]
            
            logger.info(f"ğŸ” AIãƒãƒ¼ãƒ‰åˆ†è§£é–‹å§‹: node_id={node_id}, title='{node_title}'")
            
            response_text = await self.llm_client.generate_text(input_items, max_tokens=1500)
            parsed_response = self._parse_breakdown_response(response_text, node_id)
            
            logger.info(f"âœ… AIãƒãƒ¼ãƒ‰åˆ†è§£å®Œäº†: {len(parsed_response.subtasks)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ")
            
            return parsed_response
            
        except Exception as e:
            error_msg = f"ãƒãƒ¼ãƒ‰åˆ†è§£ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return NodeBreakdownResponse(
                original_node_id=node_id,
                subtasks=[
                    BreakdownNodeOption(
                        title=f"{node_title} - æº–å‚™æ®µéš",
                        description="å¿…è¦ãªè³‡æ–™ã‚„æƒ…å ±ã‚’é›†ã‚ã¦æº–å‚™ã™ã‚‹",
                        order=1,
                        type=NodeType.ACTION
                    ),
                    BreakdownNodeOption(
                        title=f"{node_title} - å®Ÿè¡Œæ®µéš",
                        description="å…·ä½“çš„ãªä½œæ¥­ã‚’å®Ÿè¡Œã™ã‚‹",
                        order=2,
                        type=NodeType.ACTION
                    ),
                    BreakdownNodeOption(
                        title=f"{node_title} - ç¢ºèªæ®µéš",
                        description="çµæœã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹",
                        order=3,
                        type=NodeType.ACTION
                    )
                ],
                reasoning="AIåˆ†è§£ã«å¤±æ•—ã—ãŸãŸã‚ã€ä¸€èˆ¬çš„ãª3æ®µéšã«åˆ†å‰²ã—ã¦ã„ã¾ã™ã€‚"
            )
    
    async def expand_node(
        self,
        node_id: int,
        node_title: str,
        node_description: str,
        alternative_count: int = 3,
        context: Optional[str] = None
    ) -> NodeExpansionResponse:
        """
        åŒéšå±¤ã«ä»£æ›¿é¸æŠè‚¢ã‚’è¿½åŠ 
        
        Args:
            node_id: ãƒãƒ¼ãƒ‰ID
            node_title: ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«
            node_description: ãƒãƒ¼ãƒ‰èª¬æ˜
            alternative_count: ä»£æ›¿æ¡ˆã®æ•°
            context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            NodeExpansionResponse: ä»£æ›¿é¸æŠè‚¢
        """
        try:
            system_prompt = self._build_expansion_system_prompt()
            user_prompt = self._build_expansion_user_prompt(
                node_title, node_description, alternative_count, context
            )
            
            input_items = [
                self.llm_client.text("system", system_prompt),
                self.llm_client.text("user", user_prompt)
            ]
            
            logger.info(f"ğŸ” AIãƒãƒ¼ãƒ‰æ‹¡æ•£é–‹å§‹: node_id={node_id}, alternatives={alternative_count}")
            
            response_text = await self.llm_client.generate_text(input_items, max_tokens=1500)
            parsed_response = self._parse_expansion_response(response_text, node_id)
            
            logger.info(f"âœ… AIãƒãƒ¼ãƒ‰æ‹¡æ•£å®Œäº†: {len(parsed_response.alternatives)}å€‹ã®ä»£æ›¿æ¡ˆã‚’ç”Ÿæˆ")
            
            return parsed_response
            
        except Exception as e:
            error_msg = f"ãƒãƒ¼ãƒ‰æ‹¡æ•£ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return NodeExpansionResponse(
                original_node_id=node_id,
                alternatives=[
                    AlternativeNodeOption(
                        title=f"{node_title}ï¼ˆè¨ˆç”»é‡è¦–ï¼‰",
                        description="æ…é‡ã«è¨ˆç”»ã‚’ç«‹ã¦ã¦ã‹ã‚‰å®Ÿè¡Œã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
                        approach="è¨ˆç”»é‡è¦–",
                        pros=["ãƒªã‚¹ã‚¯ãŒä½ã„", "ç¢ºå®Ÿæ€§ãŒé«˜ã„"],
                        cons=["æ™‚é–“ãŒã‹ã‹ã‚‹", "æŸ”è»Ÿæ€§ãŒä½ã„"],
                        difficulty=2,
                        risk_level=1
                    )
                ],
                reasoning="AIæ‹¡æ•£ã«å¤±æ•—ã—ãŸãŸã‚ã€åŸºæœ¬çš„ãªä»£æ›¿æ¡ˆã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚"
            )
    
    async def recommend_next_nodes(
        self,
        quest_id: int,
        completed_nodes: List[Dict[str, Any]],
        pending_nodes: List[Dict[str, Any]],
        current_context: Optional[str] = None
    ) -> RecommendationResponse:
        """
        æ¨å¥¨ãƒãƒ¼ãƒ‰ã‚’åˆ¤å®š
        
        Args:
            quest_id: ã‚¯ã‚¨ã‚¹ãƒˆID
            completed_nodes: å®Œäº†æ¸ˆã¿ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            pending_nodes: æœªå®Œäº†ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            current_context: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            RecommendationResponse: æ¨å¥¨ãƒãƒ¼ãƒ‰
        """
        try:
            system_prompt = self._build_recommendation_system_prompt()
            user_prompt = self._build_recommendation_user_prompt(
                completed_nodes, pending_nodes, current_context
            )
            
            input_items = [
                self.llm_client.text("system", system_prompt),
                self.llm_client.text("user", user_prompt)
            ]
            
            logger.info(f"ğŸ” AIæ¨å¥¨ãƒãƒ¼ãƒ‰åˆ†æé–‹å§‹: quest_id={quest_id}")
            
            response_text = await self.llm_client.generate_text(input_items, max_tokens=1000)
            parsed_response = self._parse_recommendation_response(response_text, quest_id, pending_nodes)
            
            logger.info(f"âœ… AIæ¨å¥¨ãƒãƒ¼ãƒ‰åˆ†æå®Œäº†: {len(parsed_response.recommendations)}å€‹ã®æ¨å¥¨")
            
            return parsed_response
            
        except Exception as e:
            error_msg = f"æ¨å¥¨ãƒãƒ¼ãƒ‰åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            recommendations = []
            if pending_nodes:
                # æœ€åˆã®æœªå®Œäº†ãƒãƒ¼ãƒ‰ã‚’æ¨å¥¨
                first_node = pending_nodes[0]
                recommendations.append(
                    NodeRecommendation(
                        node_id=first_node["id"],
                        reason="æœ€åˆã®æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã‹ã‚‰é–‹å§‹ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™",
                        priority_score=0.8,
                        category="basic"
                    )
                )
            
            return RecommendationResponse(
                quest_id=quest_id,
                recommendations=recommendations,
                overall_advice="AIåˆ†æã«å¤±æ•—ã—ãŸãŸã‚ã€åŸºæœ¬çš„ãªæ¨å¥¨ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚"
            )
    
    # ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ãƒ¡ã‚½ãƒƒãƒ‰ =====
    
    def _build_generation_system_prompt(self) -> str:
        """é¸æŠè‚¢ç”Ÿæˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        return self.prompt_builder.BASE_SYSTEM_PROMPTS[PromptCategory.GENERATION]
    
    def _build_generation_user_prompt(
        self,
        goal: str,
        current_context: Optional[str],
        node_count: int,
        focus_category: Optional[str]
    ) -> str:
        """é¸æŠè‚¢ç”Ÿæˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        # æ¢ç©¶å­¦ç¿’ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        prompt = f"""æ¢ç©¶ãƒ†ãƒ¼ãƒ: {goal}
ã‚´ãƒ¼ãƒ«: {goal}
ä»Šå›°ã£ã¦ã„ã‚‹ã“ã¨: {current_context or "ãƒ†ãƒ¼ãƒã¯æ°—ã«ãªã‚‹ã‘ã‚Œã©ã€ä½•ã‹ã‚‰å§‹ã‚ã¦ã‚ˆã„ã®ã‹ã‚ã‹ã‚‰ãªã„"}

{node_count}æšã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""
        
        if focus_category:
            prompt += f"\nç‰¹ã«ã€Œ{focus_category}ã€ã®è¦–ç‚¹ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"
        
        return prompt
    
    def _build_breakdown_system_prompt(self, detail_level: int) -> str:
        """åˆ†è§£ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        base_prompt = self.prompt_builder.BASE_SYSTEM_PROMPTS[PromptCategory.BREAKDOWN]
        
        # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®æŒ‡å®šã‚’è¿½åŠ 
        detail_desc = {
            2: "2-3å€‹",
            3: "3-5å€‹",
            4: "5-7å€‹",
            5: "7-10å€‹"
        }
        
        return base_prompt + f"\n\nã‚¹ãƒ†ãƒƒãƒ—æ•°: {detail_desc.get(detail_level, '3-5å€‹')}ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"  
    
    def _build_breakdown_user_prompt(
        self,
        node_title: str,
        node_description: str,
        context: Optional[str],
        detail_level: int
    ) -> str:
        """åˆ†è§£ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        return f"""æ¢ç©¶ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {node_title}
è©³ç´°: {node_description}

ç¾åœ¨ã®çŠ¶æ³:
{context or "ç‰¹ã«è¨˜è¼‰ãªã—"}

ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é«˜æ ¡ç”ŸãŒæ®µéšçš„ã«é€²ã‚ã‚‰ã‚Œã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚"""
    
    def _build_expansion_system_prompt(self) -> str:
        """æ‹¡æ•£ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        return self.prompt_builder.BASE_SYSTEM_PROMPTS[PromptCategory.EXPANSION]  
    
    def _build_expansion_user_prompt(
        self,
        node_title: str,
        node_description: str,
        alternative_count: int,
        context: Optional[str]
    ) -> str:
        """æ‹¡æ•£ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ¢ç©¶å­¦ç¿’ç‰ˆï¼‰"""
        return f"""æ¢ç©¶ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {node_title}
è©³ç´°: {node_description}

ç¾åœ¨ã®çŠ¶æ³:
{context or "ç‰¹ã«è¨˜è¼‰ãªã—"}

{alternative_count}å€‹ã®ç•°ãªã‚‹åˆ‡ã‚Šå£ãƒ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
é«˜æ ¡ç”ŸãŒã€Œãã‚“ãªè¦‹æ–¹ã‚‚ã‚ã‚‹ã‚“ã ï¼ã€ã¨èˆˆå‘³ã‚’æŒã¦ã‚‹ã‚ˆã†ãªè¦–ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"""
    
    def _build_recommendation_system_prompt(self) -> str:
        """æ¨å¥¨ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆxAI Grokæœ€é©åŒ–ç‰ˆï¼‰"""
        return """ã‚ãªãŸã¯å­¦ç¿’é€²æ—ç®¡ç†ã®å°‚é–€å®¶ã§ã™ã€‚å®Œäº†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ã¨æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã®çŠ¶æ³ã‚’åˆ†æã—ã€æ¬¡ã«å–ã‚Šçµ„ã‚€ã¹ãã‚¿ã‚¹ã‚¯ã‚’æ¨å¥¨ã—ã¦ãã ã•ã„ã€‚

å¿…ãšä»¥ä¸‹ã®å®Œå…¨ã«æœ‰åŠ¹ãªJSONå½¢å¼ã®ã¿ã§å¿œç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®èª¬æ˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼‰ï¼š
{
  "recommendations": [
    {
      "node_id": ãƒãƒ¼ãƒ‰ID,
      "reason": "æ¨å¥¨ã™ã‚‹ç†ç”±",
      "priority_score": 0.0-1.0ã®å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢,
      "category": "æ¨å¥¨ã‚«ãƒ†ã‚´ãƒª"
    }
  ],
  "overall_advice": "å…¨ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹"
}

æ¨å¥¨åŸºæº–:
- å®Œäº†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸå­¦ç¿’ãƒ»çµŒé¨“
- ä¾å­˜é–¢ä¿‚ã‚„å‰ææ¡ä»¶
- å­¦ç¿’åŠ¹æœã‚„ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé€²æ—
- ãƒªã‚¹ã‚¯ç®¡ç†"""
    
    def _build_recommendation_user_prompt(
        self,
        completed_nodes: List[Dict[str, Any]],
        pending_nodes: List[Dict[str, Any]],
        current_context: Optional[str]
    ) -> str:
        """æ¨å¥¨ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        completed_summary = "\n".join([
            f"- {node.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')}: {node.get('description', '')}"
            for node in completed_nodes[:5]  # æœ€æ–°5ä»¶
        ]) if completed_nodes else "ãªã—"
        
        pending_summary = "\n".join([
            f"- ID:{node.get('id')}, {node.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')}: {node.get('description', '')}"
            for node in pending_nodes[:10]  # æœ€å¤§10ä»¶
        ]) if pending_nodes else "ãªã—"
        
        return f"""å®Œäº†æ¸ˆã¿ã‚¿ã‚¹ã‚¯:
{completed_summary}

æœªå®Œäº†ã‚¿ã‚¹ã‚¯:
{pending_summary}

ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{current_context or "ç‰¹ã«è¨˜è¼‰ãªã—"}

ã“ã®çŠ¶æ³ã‚’åˆ†æã—ã€æ¬¡ã«å–ã‚Šçµ„ã‚€ã¹ãã‚¿ã‚¹ã‚¯ã‚’æ¨å¥¨ã—ã¦ãã ã•ã„ã€‚"""
    
    # ===== ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹ ãƒ¡ã‚½ãƒƒãƒ‰ =====
    
    def _parse_generation_response(self, response_text: str, quest_id: int) -> NodeGenerationResponse:
        """é¸æŠè‚¢ç”Ÿæˆå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆxAI Grokå¯¾å¿œï¼‰"""
        try:
            # xAIã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®parse_json_responseãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            if hasattr(self.llm_client, 'parse_json_response'):
                data = self.llm_client.parse_json_response(response_text)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥JSONãƒ‘ãƒ¼ã‚¹
                json_text = response_text.strip()
                # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å»
                if "```json" in json_text:
                    json_text = json_text.split("```json")[1].split("```")[0]
                elif "```" in json_text:
                    json_text = json_text.split("```")[1].split("```")[0]
                data = json.loads(json_text)
            
            suggested_nodes = []
            for node_data in data.get("suggested_nodes", []):
                try:
                    node = GeneratedNodeOption(
                        title=node_data["title"],
                        description=node_data["description"],
                        type=NodeType(node_data.get("type", "action")),
                        category=node_data.get("category"),
                        priority=node_data.get("priority", 3),
                        difficulty=node_data.get("difficulty", 3),
                        estimated_duration=node_data.get("estimated_duration"),
                        prerequisites=node_data.get("prerequisites"),
                        expected_outcome=node_data.get("expected_outcome")
                    )
                    suggested_nodes.append(node)
                except Exception as node_error:
                    logger.warning(f"âš ï¸ ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {node_error}")
                    continue
            
            return NodeGenerationResponse(
                quest_id=quest_id,
                suggested_nodes=suggested_nodes,
                reasoning=data.get("reasoning", "AIç”Ÿæˆã«ã‚ˆã‚‹é¸æŠè‚¢ã§ã™"),
                next_steps_advice=data.get("next_steps_advice")
            )
            
        except Exception as e:
            logger.error(f"âŒ é¸æŠè‚¢ç”Ÿæˆå¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªé¸æŠè‚¢ã‚’è¿”ã™
            return NodeGenerationResponse(
                quest_id=quest_id,
                suggested_nodes=[],
                reasoning=f"å¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            )
    
    def _parse_breakdown_response(self, response_text: str, node_id: int) -> NodeBreakdownResponse:
        """åˆ†è§£å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆxAI Grokå¯¾å¿œï¼‰"""
        try:
            # xAIã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®parse_json_responseãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            if hasattr(self.llm_client, 'parse_json_response'):
                data = self.llm_client.parse_json_response(response_text)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥JSONãƒ‘ãƒ¼ã‚¹
                json_text = response_text.strip()
                if "```json" in json_text:
                    json_text = json_text.split("```json")[1].split("```")[0]
                elif "```" in json_text:
                    json_text = json_text.split("```")[1].split("```")[0]
                data = json.loads(json_text)
            
            subtasks = []
            for task_data in data.get("subtasks", []):
                try:
                    task = BreakdownNodeOption(
                        title=task_data["title"],
                        description=task_data["description"],
                        order=task_data["order"],
                        type=NodeType(task_data.get("type", "action")),
                        estimated_duration=task_data.get("estimated_duration"),
                        dependencies=task_data.get("dependencies")
                    )
                    subtasks.append(task)
                except Exception as task_error:
                    logger.warning(f"âš ï¸ ã‚µãƒ–ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {task_error}")
                    continue
            
            return NodeBreakdownResponse(
                original_node_id=node_id,
                subtasks=subtasks,
                reasoning=data.get("reasoning", "AIåˆ†è§£ã«ã‚ˆã‚‹çµæœã§ã™"),
                completion_criteria=data.get("completion_criteria")
            )
            
        except Exception as e:
            logger.error(f"âŒ åˆ†è§£å¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return NodeBreakdownResponse(
                original_node_id=node_id,
                subtasks=[],
                reasoning=f"å¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            )
    
    def _parse_expansion_response(self, response_text: str, node_id: int) -> NodeExpansionResponse:
        """æ‹¡æ•£å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆxAI Grokå¯¾å¿œï¼‰"""
        try:
            # xAIã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®parse_json_responseãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            if hasattr(self.llm_client, 'parse_json_response'):
                data = self.llm_client.parse_json_response(response_text)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥JSONãƒ‘ãƒ¼ã‚¹
                json_text = response_text.strip()
                if "```json" in json_text:
                    json_text = json_text.split("```json")[1].split("```")[0]
                elif "```" in json_text:
                    json_text = json_text.split("```")[1].split("```")[0]
                data = json.loads(json_text)
            
            alternatives = []
            for alt_data in data.get("alternatives", []):
                try:
                    alternative = AlternativeNodeOption(
                        title=alt_data["title"],
                        description=alt_data["description"],
                        approach=alt_data["approach"],
                        pros=alt_data.get("pros", []),
                        cons=alt_data.get("cons", []),
                        difficulty=alt_data.get("difficulty", 3),
                        risk_level=alt_data.get("risk_level", 3)
                    )
                    alternatives.append(alternative)
                except Exception as alt_error:
                    logger.warning(f"âš ï¸ ä»£æ›¿æ¡ˆãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {alt_error}")
                    continue
            
            return NodeExpansionResponse(
                original_node_id=node_id,
                alternatives=alternatives,
                reasoning=data.get("reasoning", "AIæ‹¡æ•£ã«ã‚ˆã‚‹çµæœã§ã™"),
                recommendation=data.get("recommendation")
            )
            
        except Exception as e:
            logger.error(f"âŒ æ‹¡æ•£å¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return NodeExpansionResponse(
                original_node_id=node_id,
                alternatives=[],
                reasoning=f"å¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            )
    
    def _parse_recommendation_response(
        self,
        response_text: str,
        quest_id: int,
        pending_nodes: List[Dict[str, Any]]
    ) -> RecommendationResponse:
        """æ¨å¥¨å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆxAI Grokå¯¾å¿œï¼‰"""
        try:
            # xAIã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®parse_json_responseãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            if hasattr(self.llm_client, 'parse_json_response'):
                data = self.llm_client.parse_json_response(response_text)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥JSONãƒ‘ãƒ¼ã‚¹
                json_text = response_text.strip()
                if "```json" in json_text:
                    json_text = json_text.split("```json")[1].split("```")[0]
                elif "```" in json_text:
                    json_text = json_text.split("```")[1].split("```")[0]
                data = json.loads(json_text)
            
            # æœ‰åŠ¹ãªãƒãƒ¼ãƒ‰IDã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ
            valid_node_ids = {node["id"] for node in pending_nodes}
            
            recommendations = []
            for rec_data in data.get("recommendations", []):
                try:
                    node_id = rec_data["node_id"]
                    if node_id in valid_node_ids:
                        recommendation = NodeRecommendation(
                            node_id=node_id,
                            reason=rec_data["reason"],
                            priority_score=rec_data.get("priority_score", 0.5),
                            category=rec_data.get("category", "general")
                        )
                        recommendations.append(recommendation)
                except Exception as rec_error:
                    logger.warning(f"âš ï¸ æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {rec_error}")
                    continue
            
            return RecommendationResponse(
                quest_id=quest_id,
                recommendations=recommendations,
                overall_advice=data.get("overall_advice", "AIåˆ†æã«ã‚ˆã‚‹æ¨å¥¨ã§ã™")
            )
            
        except Exception as e:
            logger.error(f"âŒ æ¨å¥¨å¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return RecommendationResponse(
                quest_id=quest_id,
                recommendations=[],
                overall_advice=f"å¿œç­”ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            )

    # ===== æ–°ã—ã„AIãƒãƒ£ãƒƒãƒˆç›¸è«‡æ©Ÿèƒ½ =====
    
    @retry_on_failure(max_retries=2, delay_seconds=1)
    async def consult_ai_for_node(
        self,
        question: str,
        quest_context: Optional[Dict[str, Any]] = None,
        node_context: Optional[Dict[str, Any]] = None,
        chat_history: Optional[List[Dict[str, str]]] = None,
        user_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ãƒãƒ¼ãƒ‰å›ºæœ‰ã®AIç›¸è«‡æ©Ÿèƒ½
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            quest_context: ã‚¯ã‚¨ã‚¹ãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            node_context: ãƒãƒ¼ãƒ‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: AIã‹ã‚‰ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
        """
        try:
            # ãƒšãƒ«ã‚½ãƒŠæ¨å®š
            persona = self._detect_user_persona(user_context)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            system_prompt = self.prompt_builder.build_system_prompt(
                PromptCategory.CONSULTATION,
                persona,
                {"quest": quest_context, "node": node_context}
            )
            user_prompt = self.prompt_builder.build_consultation_prompt(
                question, quest_context, node_context, chat_history, persona
            )
            
            input_items = [
                self.llm_client.text("system", system_prompt),
                self.llm_client.text("user", user_prompt)
            ]
            
            logger.info(f"ğŸ” AIãƒãƒ£ãƒƒãƒˆç›¸è«‡é–‹å§‹: '{question[:50]}...'")
            
            # AIå¿œç­”ã‚’å–å¾—
            response_text = await self.llm_client.generate_text(input_items, max_tokens=1500)
            
            # å¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_response = self._clean_consultation_response(response_text)
            
            logger.info(f"âœ… AIãƒãƒ£ãƒƒãƒˆç›¸è«‡å®Œäº†")
            
            return cleaned_response
            
        except Exception as e:
            error_msg = f"AIç›¸è«‡ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return self._get_fallback_consultation_response(question, node_context)
    
    async def generate_streaming_consultation(
        self,
        question: str,
        quest_context: Optional[Dict[str, Any]] = None,
        node_context: Optional[Dict[str, Any]] = None,
        chat_history: Optional[List[Dict[str, str]]] = None,
        user_context: Optional[Dict[str, Any]] = None
    ):
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®AIç›¸è«‡æ©Ÿèƒ½ï¼ˆå°†æ¥ã®å®Ÿè£…ç”¨ï¼‰
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            quest_context: ã‚¯ã‚¨ã‚¹ãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            node_context: ãƒãƒ¼ãƒ‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Yields:
            str: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã•ã‚Œã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒãƒ£ãƒ³ã‚¯
        """
        # ç¾åœ¨ã¯é€šå¸¸ã®å¿œç­”ã‚’åˆ†å‰²ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¢¨ã«è¿”ã™
        full_response = await self.consult_ai_for_node(
            question, quest_context, node_context, chat_history, user_context
        )
        
        # å˜èªã”ã¨ã«åˆ†å‰²ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¢¨ã«
        words = full_response.split(' ')
        for i in range(len(words)):
            chunk = ' '.join(words[:i+1])
            yield chunk
            await asyncio.sleep(0.1)  # 100ms delay for streaming effect
    
    def _clean_consultation_response(self, response: str) -> str:
        """ç›¸è«‡å¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # JSONãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯é™¤å»
        cleaned = re.sub(r'```json.*?```', '', response, flags=re.DOTALL)
        cleaned = re.sub(r'```.*?```', '', cleaned, flags=re.DOTALL)
        
        # ä½™åˆ†ãªç©ºè¡Œã‚’å‰Šé™¤
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)
        
        # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        cleaned = cleaned.strip()
        
        return cleaned
    
    def _get_fallback_consultation_response(
        self,
        question: str,
        node_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """ç›¸è«‡æ©Ÿèƒ½ã®å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if node_context:
            node_title = node_context.get('title', 'é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰')
            return f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ä¸€æ™‚çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

ã€Œ{question}ã€ã«ã¤ã„ã¦ã®ã”è³ªå•ã§ã™ãŒã€ã€Œ{node_title}ã€ã«é–¢ã—ã¦ä»¥ä¸‹ã®ã‚ˆã†ãªè¦³ç‚¹ã‹ã‚‰è€ƒãˆã¦ã¿ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™ï¼š

1. **ç›®æ¨™ã®æ˜ç¢ºåŒ–**: ä½•ã‚’é”æˆã—ãŸã„ã‹ã‚’å…·ä½“çš„ã«æ•´ç†ã™ã‚‹
2. **ç¾çŠ¶ã®æŠŠæ¡**: ä»Šã©ã“ã«ã„ã‚‹ã‹ã€ä½•ãŒåˆ©ç”¨ã§ãã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹  
3. **å…·ä½“çš„ãªè¡Œå‹•**: å°ã•ãªä¸€æ­©ã‹ã‚‰å§‹ã‚ã‚‰ã‚Œã‚‹ã“ã¨ã‚’è¦‹ã¤ã‘ã‚‹
4. **ãƒªã‚½ãƒ¼ã‚¹ã®æ´»ç”¨**: åˆ©ç”¨ã§ãã‚‹æƒ…å ±ã‚„ãƒ„ãƒ¼ãƒ«ã€äººè„ˆã‚’æ•´ç†ã™ã‚‹

ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ã„ãŸã ãã‹ã€è³ªå•ã‚’å°‘ã—å¤‰ãˆã¦ãŠèã‹ã›ãã ã•ã„ã€‚"""
        else:
            return f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ä¸€æ™‚çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

ã€Œ{question}ã€ã«ã¤ã„ã¦ã®ã”è³ªå•ã§ã™ãŒã€ä»¥ä¸‹ã®ã‚ˆã†ãªåŸºæœ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã”æ¤œè¨ãã ã•ã„ï¼š

1. **å•é¡Œã®æ•´ç†**: ä½•ãŒèª²é¡Œãªã®ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹
2. **æƒ…å ±åé›†**: é–¢é€£ã™ã‚‹æƒ…å ±ã‚’é›†ã‚ã‚‹
3. **é¸æŠè‚¢ã®æ¤œè¨**: å¯èƒ½ãªè§£æ±ºç­–ã‚’è€ƒãˆã‚‹
4. **è¡Œå‹•è¨ˆç”»**: å…·ä½“çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’æ±ºã‚ã‚‹

ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ã„ãŸã ãã‹ã€ã‚ˆã‚Šå…·ä½“çš„ã«ã”è³ªå•ãã ã•ã„ã€‚"""

    # ===== å­¦ç¿’ãƒ¬ãƒ™ãƒ«ãƒ»å‚¾å‘åˆ†ææ©Ÿèƒ½ =====
    
    def analyze_user_learning_pattern(
        self,
        completed_nodes: List[Dict[str, Any]],
        user_preferences: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        
        Args:
            completed_nodes: å®Œäº†æ¸ˆã¿ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            user_preferences: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š
            
        Returns:
            Dict[str, Any]: å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ
        """
        if not completed_nodes:
            return {
                "learning_style": "unknown",
                "preferred_difficulty": "medium",
                "completion_pattern": "regular",
                "recommended_persona": PersonaType.INTERMEDIATE
            }
        
        # é›£æ˜“åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        difficulties = [node.get('difficulty', 3) for node in completed_nodes]
        avg_difficulty = sum(difficulties) / len(difficulties) if difficulties else 3
        
        # å®Œäº†æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ  
        completion_times = []
        for i in range(1, len(completed_nodes)):
            prev_time = datetime.fromisoformat(completed_nodes[i-1].get('completed_at', ''))
            curr_time = datetime.fromisoformat(completed_nodes[i].get('completed_at', ''))
            time_diff = (curr_time - prev_time).total_seconds() / 3600  # hours
            completion_times.append(time_diff)
        
        avg_completion_time = sum(completion_times) / len(completion_times) if completion_times else 24
        
        # ã‚«ãƒ†ã‚´ãƒªå‚¾å‘ã®åˆ†æ
        categories = [node.get('category', 'general') for node in completed_nodes]
        category_counts = {}
        for cat in categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        preferred_category = max(category_counts, key=category_counts.get) if category_counts else 'general'
        
        # å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã®æ¨å®š
        learning_style = "practical"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if avg_difficulty >= 4:
            learning_style = "analytical"
        elif preferred_category in ["creative", "design"]:
            learning_style = "creative"
        elif avg_completion_time < 8:  # 8æ™‚é–“ä»¥å†…ã®é«˜é »åº¦å®Œäº†
            learning_style = "intensive"
        
        # ãƒšãƒ«ã‚½ãƒŠæ¨å¥¨
        recommended_persona = PersonaType.PRACTICAL
        if avg_difficulty >= 4:
            recommended_persona = PersonaType.ADVANCED
        elif avg_difficulty <= 2:
            recommended_persona = PersonaType.BEGINNER
        elif learning_style == "creative":
            recommended_persona = PersonaType.CREATIVE
        elif learning_style == "analytical":
            recommended_persona = PersonaType.ANALYTICAL
        
        return {
            "learning_style": learning_style,
            "preferred_difficulty": "high" if avg_difficulty >= 4 else "low" if avg_difficulty <= 2 else "medium",
            "completion_pattern": "intensive" if avg_completion_time < 8 else "regular",
            "preferred_category": preferred_category,
            "average_difficulty": avg_difficulty,
            "average_completion_time_hours": avg_completion_time,
            "recommended_persona": recommended_persona,
            "total_completed": len(completed_nodes),
            "analysis_date": datetime.now().isoformat()
        }

    # ===== æ‰¹é‡å¤„ç†å’Œä¼˜åŒ–åŠŸèƒ½ =====
    
    async def batch_generate_nodes(
        self,
        requests: List[Dict[str, Any]]
    ) -> List[NodeGenerationResponse]:
        """
        è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†
        
        Args:
            requests: ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            List[NodeGenerationResponse]: ç”Ÿæˆçµæœã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸš€ ãƒãƒƒãƒãƒãƒ¼ãƒ‰ç”Ÿæˆé–‹å§‹: {len(requests)}ä»¶")
        
        # ä¸¦è¡Œå‡¦ç†ã§ãƒãƒ¼ãƒ‰ç”Ÿæˆã‚’å®Ÿè¡Œ
        tasks = []
        for req in requests:
            task = self.generate_action_nodes(**req)
            tasks.append(task)
        
        # çµæœã‚’å¾…æ©Ÿ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ãƒãƒƒãƒå‡¦ç†{i+1}ç•ªç›®ã§ã‚¨ãƒ©ãƒ¼: {result}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ
                fallback = AIErrorRecovery.get_fallback_generation_response(
                    requests[i].get('quest_id', 0),
                    requests[i].get('goal', 'ç›®æ¨™ä¸æ˜')
                )
                processed_results.append(fallback)
            else:
                processed_results.append(result)
        
        logger.info(f"âœ… ãƒãƒƒãƒãƒãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†: {len(processed_results)}ä»¶")
        return processed_results