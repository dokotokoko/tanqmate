"""
LLM APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆçµ±åˆç‰ˆï¼‰
åŒæœŸãƒ»éåŒæœŸã®ä¸¡æ–¹ã«å¯¾å¿œ
"""

import os
import asyncio
import time
import logging
from typing import List, Dict, Any, Optional, Union, AsyncIterator
from openai import OpenAI, AsyncOpenAI
from dotenv import load_dotenv
from collections import deque

logger = logging.getLogger(__name__)


class learning_plannner():
    """
    çµ±åˆç‰ˆLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    åŒæœŸãƒ»éåŒæœŸã®ä¸¡æ–¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒã¤
    """
    
    def __init__(self, pool_size: int = 5):
        """
        åˆæœŸåŒ–
        
        Args:
            pool_size: éåŒæœŸå‡¦ç†ç”¨ã®ã‚»ãƒãƒ•ã‚©ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º
        """
        load_dotenv()
        self.model = "gpt-5.2"
        self.api_key = os.getenv("OPENAI_API_KEY")
        
        if not self.api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        # åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.client = OpenAI(api_key=self.api_key)
        
        # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            timeout=30.0,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’30ç§’ã«è¨­å®š
            max_retries=2   # ãƒªãƒˆãƒ©ã‚¤ã‚’2å›ã«åˆ¶é™
        )
        
        # éåŒæœŸå‡¦ç†ç”¨ã®ã‚»ãƒãƒ•ã‚©ï¼ˆåŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼‰
        self.semaphore = asyncio.Semaphore(pool_size)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ç”¨
        self.request_count = 0
        self.total_response_time = 0.0
        self.sync_requests = 0
        self.async_requests = 0
    
    # =====================================
    # å…±é€šãƒ¡ã‚½ãƒƒãƒ‰
    # =====================================

    @staticmethod
    def text_type_role(role: str) -> str:
        """
        Responses API ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«åˆã‚ã›ã‚‹ã€‚
        - user/system/developer: input_text
        - assistant: output_textï¼ˆrefusal ã¯ãƒ¢ãƒ‡ãƒ«å´ãŒè¿”ã™ï¼‰
        """
        return "output_text" if role == "assistant" else "input_text"
    
    def text(self, role: str, content: str) -> Dict[str, Any]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ç”¨ã®Response API input itemã‚’ä½œæˆ
        
        Args:
            role: "system", "user", "assistant"ã®ã„ãšã‚Œã‹
            content: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
            
        Returns:
            Response APIç”¨ã®input item
        """
        return {
            "role": role,
            "content": [{"type": self.text_type_role(role),  "text": content}]
        }
    
    def image(self, role: str, image_data: Any, text: Optional[str] = None) -> Dict[str, Any]:
        """
        ç”»åƒå…¥åŠ›ç”¨ã®Response API input itemã‚’ä½œæˆ
        
        Args:
            role: "system", "user", "assistant"ã®ã„ãšã‚Œã‹
            image_data: ç”»åƒãƒ‡ãƒ¼ã‚¿
            text: ç”»åƒã«ä»˜éšã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Response APIç”¨ã®input item
        """
        parts = []
        if text:
            parts.append({"type": "input_text", "text": text})
        parts.append({"type": "input_image", "data": image_data})
        return {
            "role": role,
            "content": parts
        }
    
    # =====================================
    # å‡ºåŠ›æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # =====================================

    @staticmethod
    def extract_output_text(resp: Any) -> str:
        """
        SDKå·®åˆ†ã‚’å¸åã—ã¤ã¤ã€æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å–ã‚Šå‡ºã™
        """
        if hasattr(resp, "output_text") and resp.output_text:
            return resp.output_text

        # fallback: output ã® message ã‚’èµ°æŸ»
        texts: List[str] = []
        output = getattr(resp, "output", None) or []
        for item in output:
            if getattr(item, "type", None) == "message":
                for c in getattr(item, "content", None) or []:
                    if getattr(c, "type", None) in ("output_text", "text"):
                        t = getattr(c, "text", None)
                        if t:
                            texts.append(t)

        if texts:
            return "\n".join(texts)

        return str(resp)
    
    # =====================================
    # åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰
    # =====================================
    
    def generate_response(self, input_items: List[Dict[str, Any]], max_tokens: Optional[int] = None):
        """
        Response APIã‚’ä½¿ç”¨ã—ã¦LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆï¼ˆåŒæœŸç‰ˆï¼‰
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            Response object
        """
        start_time = time.time()

        # Response APIã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹ç¯‰
        request_params: Dict[str, Any] = {
            "model": self.model,
            "input": input_items,
            "tools": [{"type": "web_search"}], # NOTE: web_search ãƒ„ãƒ¼ãƒ«ã‚’å¸¸ã«æ¸¡ã™ï¼ˆå‘¼ã³å‡ºã™ã‹ã¯ãƒ¢ãƒ‡ãƒ«ãŒåˆ¤æ–­ï¼‰
            "store": True,
        }

        if max_tokens is not None:
            request_params["max_output_tokens"] = max_tokens

        # Response APIã‚’å‘¼ã³å‡ºã—
        resp = self.client.responses.create(**request_params)
        self._update_metrics(time.time() - start_time, "sync")
        
        return resp
    
    
    def generate_text(self, input_items: List[Dict[str, Any]], max_tokens: Optional[int] = None) -> str:
        """
        Response ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            output_text
        """
        resp = self.generate_response(input_items, max_tokens=max_tokens)
        output_text = self.extract_output_text(resp)

        return output_text

    
    def generate_response_with_WebSearch(self, input_items: List[Dict[str, Any]]) -> str:
        """
        WebSearchæ©Ÿèƒ½ä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ â†’ `generate_response`ã«Webæ¤œç´¢ã¯çµ±åˆã—ãŸãŸã‚å®Ÿè³ªä¸è¦
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            
        Returns:
            WebSearchçµæœã‚’å«ã‚€LLMã‹ã‚‰ã®å¿œç­”
        """
        resp = self.client.responses.create(
            model=self.model,
            input=input_items,
            tools=[{"type": "web_search"}],
            store=True,
        )
        return resp.output_text
    
    # =====================================
    # éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰
    # =====================================
    
    async def generate_response_async(self, input_items: List[Dict[str, Any]], max_tokens: Optional[int] = None):
        """
        Response APIã‚’ä½¿ç”¨ã—ãŸéåŒæœŸå¿œç­”ç”Ÿæˆ
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            Response object
        """
        start_time = time.time()
        
        try:
            # ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
            async with self.semaphore:
                # Response APIã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹ç¯‰
                request_params: Dict[str, Any] = {
                    "model": self.model,
                    "input": input_items,
                    "tools": [{"type": "web_search"}],
                    "store": True,
                }
                
                if max_tokens is not None:
                    request_params["max_output_tokens"] = max_tokens
                
                # Response APIã‚’å‘¼ã³å‡ºã—
                response = await self.async_client.responses.create(**request_params)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                self._update_metrics(time.time() - start_time, "async")
                
                return response
                
        except Exception as e:
            logger.error(f"âŒ OpenAI APIéåŒæœŸå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŒæœŸç‰ˆã‚’éåŒæœŸã§å®Ÿè¡Œ
            return await asyncio.to_thread(
                self.generate_response,
                input_items,
                max_tokens
            )
    
    async def generate_text(self, input_items: List[Dict[str, Any]], max_tokens: Optional[int] = None) -> str:
        """
        Response ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™(éåŒæœŸ)
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            output_text
        """
        resp = await self.generate_response_async(input_items, max_tokens=max_tokens)
        output_text = self.extract_output_text(resp)

        return output_text
    
    async def generate_response_streaming(
            self,
            input_items: List[Dict[str, Any]],
            callback: Optional[callable] = None,
            max_tokens: Optional[int] = None
        ) -> AsyncIterator[str]:
            """
            Responses API ã® streaming ã¯ event.type ã‚’è¦‹ã¦ delta ã‚’æ‹¾ã†
            """
            async with self.semaphore:
                request_params: Dict[str, Any] = {
                    "model": self.model,
                    "input": input_items,
                    "tools": [{"type": "web_search"}],
                    "stream": True,
                    "store": True,
                }
                if max_tokens is not None:
                    request_params["max_output_tokens"] = max_tokens

                stream = await self.async_client.responses.create(**request_params)

                async for event in stream:
                    etype = getattr(event, "type", None)

                    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å¢—åˆ†
                    if etype == "response.output_text.delta":
                        delta = getattr(event, "delta", "") or ""
                        if delta:
                            if callback:
                                await callback(delta)
                            yield delta

                    # ã‚¨ãƒ©ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ
                    elif etype == "error":
                        raise RuntimeError(str(event))

                    # å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆãªã©ã¯ç„¡è¦–ï¼ˆå¿…è¦ãªã‚‰ã“ã“ã§ãƒãƒ³ãƒ‰ãƒ«ï¼‰
                    # - response.completed
                    # - response.created
                    # - response.web_search.* ãªã©
    
    async def batch_generate_responses(
        self, 
        input_sets: List[List[Dict[str, Any]]],
        max_tokens: Optional[int] = None
    ) -> List[Any]:
        """
        è¤‡æ•°ã®inputã‚»ãƒƒãƒˆã‚’ä¸¦åˆ—ã§å‡¦ç†
        
        Args:
            input_sets: inputã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        tasks = [
            self.generate_response_async(input_items, max_tokens)
            for input_items in input_sets
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        responses = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ (index={i}): {result}")
                responses.append(None)
            else:
                responses.append(result)
        
        return responses
    
    async def generate_with_fallback(
        self, 
        input_items: List[Dict[str, Any]], 
        fallback_model: Optional[str] = "gpt-3.5-turbo",
        max_tokens: Optional[int] = None
    ):
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            fallback_model: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ¢ãƒ‡ãƒ«å
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            Response object
        """
        try:
            # ã¾ãšãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§è©¦è¡Œ
            return await self.generate_response_async(input_items, max_tokens)
            
        except Exception as primary_error:
            logger.warning(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨: {primary_error}")
            
            if fallback_model:
                try:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã§å†è©¦è¡Œ
                    async with self.semaphore:
                        request_params = {
                            "model": fallback_model,
                            "input": input_items,
                            "store": True
                        }
                        
                        if max_tokens is not None:
                            request_params["max_output_tokens"] = min(max_tokens, 1500)
                        
                        response = await self.async_client.responses.create(**request_params)
                        return response
                        
                except Exception as fallback_error:
                    logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise primary_error
    
    async def generate_with_web_search_async(self, input_items: List[Dict[str, Any]]) -> str:
        """
        éåŒæœŸWebSearchæ©Ÿèƒ½ä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        
        Args:
            input_items: Response APIå½¢å¼ã®input items
            
        Returns:
            WebSearchçµæœã‚’å«ã‚€LLMã‹ã‚‰ã®å¿œç­”
        """
        async with self.semaphore:
            response = await self.async_client.responses.create(
                model=self.model,
                input=input_items,
                tools=[{"type": "web_search"}],
                store=True
            )
            return response.output_text if hasattr(response, 'output_text') else str(response)
    
    # =====================================
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰
    # =====================================
    
    def _update_metrics(self, response_time: float, request_type: str):
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
        
        Args:
            response_time: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
            request_type: "sync" ã¾ãŸã¯ "async"
        """
        self.request_count += 1
        self.total_response_time += response_time
        
        if request_type == "sync":
            self.sync_requests += 1
        else:
            self.async_requests += 1
        
        # 10ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ãƒ­ã‚°
        if self.request_count % 10 == 0:
            avg_time = self.total_response_time / self.request_count
            logger.info(
                f"ğŸ“Š LLM APIãƒ¡ãƒˆãƒªã‚¯ã‚¹: "
                f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ={self.request_count}, "
                f"å¹³å‡å¿œç­”æ™‚é–“={avg_time:.2f}ç§’, "
                f"åŒæœŸ/éåŒæœŸ={self.sync_requests}/{self.async_requests}"
            )
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—
        
        Returns:
            ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®Dict
        """
        if self.request_count == 0:
            return {
                "total_requests": 0,
                "average_response_time": 0,
                "sync_requests": 0,
                "async_requests": 0,
                "active_connections": self.semaphore._value if hasattr(self.semaphore, '_value') else None
            }
        
        return {
            "total_requests": self.request_count,
            "average_response_time": self.total_response_time / self.request_count,
            "sync_requests": self.sync_requests,
            "async_requests": self.async_requests,
            "active_connections": self.semaphore._value if hasattr(self.semaphore, '_value') else None
        }


# =====================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¯ãƒ©ã‚¹
# =====================================

class AsyncLearningPlanner(learning_plannner):
    """
    å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚¯ãƒ©ã‚¹
    async_llm_api.pyã§ä½¿ç”¨ã•ã‚Œã¦ã„ãŸã‚¯ãƒ©ã‚¹å
    """
    pass


# =====================================
# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†
# =====================================

_llm_instance: Optional[learning_plannner] = None
_async_llm_instance: Optional[AsyncLearningPlanner] = None


def get_async_llm_client(pool_size: int = 5) -> AsyncLearningPlanner:
    """
    éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚’å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    
    Args:
        pool_size: ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºï¼ˆåˆå›ã®ã¿æœ‰åŠ¹ï¼‰
        
    Returns:
        AsyncLearningPlannerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _async_llm_instance
    
    if _async_llm_instance is None:
        _async_llm_instance = AsyncLearningPlanner(pool_size=pool_size)
    
    return _async_llm_instance