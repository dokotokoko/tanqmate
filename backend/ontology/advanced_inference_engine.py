"""
é«˜åº¦ãªã‚°ãƒ©ãƒ•æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®å‹•çš„ãƒ«ãƒ¼ãƒ«ç”Ÿæˆã¨é©å¿œçš„æ¨è«–ã‚·ã‚¹ãƒ†ãƒ 
"""

import logging
import json
import pickle
from typing import List, Dict, Optional, Any, Tuple, Set
from datetime import datetime, timedelta
from collections import defaultdict, deque
from dataclasses import dataclass
import numpy as np
from pathlib import Path

from .ontology_graph import (
    InquiryOntologyGraph, Node, Edge, NodeType, RelationType
)
from .graph_inference_engine import GraphInferenceEngine, InferenceRule
from conversation_agent.schema import StateSnapshot, SupportType, SpeechAct

logger = logging.getLogger(__name__)


@dataclass
class LearningPattern:
    """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    sequence: List[NodeType]
    success_rate: float
    usage_count: int
    last_used: datetime
    effectiveness_score: float
    context_conditions: Dict[str, Any]


@dataclass
class AdaptiveRule:
    """é©å¿œçš„ãƒ«ãƒ¼ãƒ«"""
    rule_id: str
    name: str
    condition_template: str
    action_template: str
    priority: float
    confidence: float
    success_count: int
    failure_count: int
    learned_from_patterns: List[str]
    created_at: datetime
    last_updated: datetime


@dataclass
class UserProfile:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    user_id: str
    learning_style: Dict[str, float]  # analytical, creative, structured, exploratory
    preferred_support_types: Dict[str, float]
    effective_act_combinations: Dict[str, float]
    difficulty_preferences: Dict[str, float]
    session_patterns: List[Dict[str, Any]]
    adaptation_history: List[Dict[str, Any]]


class AdvancedInferenceEngine(GraphInferenceEngine):
    """é«˜åº¦ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå­¦ç¿’ãƒ»é©å¿œæ©Ÿèƒ½ä»˜ãï¼‰"""
    
    def __init__(self, graph: InquiryOntologyGraph, model_dir: str = "inference_models"):
        super().__init__(graph)
        
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        # å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
        self.learned_patterns: Dict[str, LearningPattern] = {}
        self.adaptive_rules: Dict[str, AdaptiveRule] = {}
        self.user_profiles: Dict[str, UserProfile] = {}
        
        # æ¨è«–å±¥æ­´
        self.inference_history: List[Dict[str, Any]] = []
        self.feedback_history: List[Dict[str, Any]] = []
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.rule_performance: Dict[str, Dict[str, float]] = defaultdict(lambda: {
            'success_rate': 0.0,
            'usage_count': 0,
            'avg_confidence': 0.0,
            'user_satisfaction': 0.0
        })
        
        # å‹•çš„é‡ã¿
        self.dynamic_weights: Dict[str, float] = {
            'pattern_match': 0.3,
            'rule_confidence': 0.25,
            'user_preference': 0.2,
            'context_similarity': 0.15,
            'temporal_relevance': 0.1
        }
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        self._load_models()
        
        # åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã«åŠ ãˆã¦å­¦ç¿’ãƒ«ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        self._initialize_adaptive_rules()
    
    def _load_models(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
            patterns_file = self.model_dir / "learned_patterns.pkl"
            if patterns_file.exists():
                with open(patterns_file, 'rb') as f:
                    self.learned_patterns = pickle.load(f)
                logger.info(f"âœ… å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿: {len(self.learned_patterns)} patterns")
            
            # é©å¿œãƒ«ãƒ¼ãƒ«
            rules_file = self.model_dir / "adaptive_rules.pkl"
            if rules_file.exists():
                with open(rules_file, 'rb') as f:
                    self.adaptive_rules = pickle.load(f)
                logger.info(f"âœ… é©å¿œãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿: {len(self.adaptive_rules)} rules")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            profiles_file = self.model_dir / "user_profiles.pkl"
            if profiles_file.exists():
                with open(profiles_file, 'rb') as f:
                    self.user_profiles = pickle.load(f)
                logger.info(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(self.user_profiles)} users")
                
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_models(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿
            with open(self.model_dir / "learned_patterns.pkl", 'wb') as f:
                pickle.dump(self.learned_patterns, f)
            
            # é©å¿œãƒ«ãƒ¼ãƒ«
            with open(self.model_dir / "adaptive_rules.pkl", 'wb') as f:
                pickle.dump(self.adaptive_rules, f)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            with open(self.model_dir / "user_profiles.pkl", 'wb') as f:
                pickle.dump(self.user_profiles, f)
                
            logger.info("ğŸ’¾ å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _initialize_adaptive_rules(self):
        """é©å¿œãƒ«ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–"""
        
        # ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«ã‹ã‚‰é©å¿œãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
        base_adaptive_rules = [
            AdaptiveRule(
                rule_id="adaptive_clarity_boost",
                name="æ˜ç¢ºæ€§å‘ä¸Šãƒ«ãƒ¼ãƒ«",
                condition_template="node.clarity < {threshold} and node.type == {node_type}",
                action_template="support_type=UNDERSTANDING, acts=[CLARIFY, PROBE], reason='æ˜ç¢ºæ€§å‘ä¸Š'",
                priority=8.0,
                confidence=0.9,
                success_count=0,
                failure_count=0,
                learned_from_patterns=[],
                created_at=datetime.now(),
                last_updated=datetime.now()
            ),
            AdaptiveRule(
                rule_id="adaptive_depth_progression",
                name="æ·±åº¦é€²è¡Œãƒ«ãƒ¼ãƒ«",
                condition_template="node.depth > {threshold} and has_child_count < {min_children}",
                action_template="support_type=PATHFINDING, acts=[OUTLINE, ACT], reason='æ·±åº¦é€²è¡Œ'",
                priority=7.0,
                confidence=0.8,
                success_count=0,
                failure_count=0,
                learned_from_patterns=[],
                created_at=datetime.now(),
                last_updated=datetime.now()
            )
        ]
        
        for rule in base_adaptive_rules:
            if rule.rule_id not in self.adaptive_rules:
                self.adaptive_rules[rule.rule_id] = rule
    
    def infer_next_step_advanced(self, current_node: Node, user_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """é«˜åº¦ãªæ¨è«–ï¼ˆå­¦ç¿’ãƒ»é©å¿œæ©Ÿèƒ½ã¨æ§‹é€ çš„ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        
        # 0. æ§‹é€ çš„æ¬ æãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
        structural_gaps = self.graph.check_structural_gaps(current_node.student_id)
        if structural_gaps:
            # æ§‹é€ çš„æ¬ æã«åŸºã¥ãæ¨è«–çµæœã‚’è¿”ã™
            top_gap = structural_gaps[0]
            return self._create_structural_inference_result(current_node, top_gap, structural_gaps)
        
        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        user_profile = self._get_or_create_user_profile(current_node.student_id)
        
        # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆæ§‹é€ çš„æƒ…å ±ã‚’å«ã‚€ï¼‰
        context_features = self._extract_context_features(current_node, user_context)
        context_features['structural_completeness'] = len(structural_gaps) == 0
        context_features['ontology_gaps'] = len(structural_gaps)
        
        # 3. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        pattern_matches = self._find_pattern_matches(current_node, context_features)
        
        # 4. é©å¿œãƒ«ãƒ¼ãƒ«è©•ä¾¡
        adaptive_results = self._evaluate_adaptive_rules(current_node, context_features, user_profile)
        
        # 5. åŸºæœ¬ãƒ«ãƒ¼ãƒ«è©•ä¾¡
        basic_result = super().infer_next_step(current_node)
        
        # 6. çµæœã®çµ±åˆã¨é‡ã¿ä»˜ã‘
        final_result = self._integrate_inference_results(
            basic_result, adaptive_results, pattern_matches, 
            user_profile, context_features
        )
        
        # 7. æ§‹é€ çš„æƒ…å ±ã‚’çµæœã«è¿½åŠ 
        final_result['structural_analysis'] = {
            'gaps_found': len(structural_gaps),
            'ontology_completeness': context_features.get('structural_completeness', False),
            'resolution_focus': 'structural' if structural_gaps else 'content'
        }
        
        # 8. æ¨è«–å±¥æ­´ã«è¨˜éŒ²
        self._record_inference(current_node, final_result, context_features)
        
        return final_result
    
    def _create_structural_inference_result(self, current_node: Node, top_gap: Dict[str, Any], all_gaps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ§‹é€ çš„æ¬ æã«åŸºã¥ãæ¨è«–çµæœã‚’ä½œæˆ"""
        
        missing_element = top_gap['missing_element']
        
        # æ”¯æ´ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        if missing_element in ['Question', 'Hypothesis']:
            support_type = SupportType.UNDERSTANDING
            acts = [SpeechAct.CLARIFY, SpeechAct.PROBE]
        elif missing_element in ['Method', 'Data']:
            support_type = SupportType.PATHFINDING  
            acts = [SpeechAct.OUTLINE, SpeechAct.ACT]
        else:
            support_type = SupportType.REFRAMING
            acts = [SpeechAct.REFRAME, SpeechAct.REFLECT]
        
        return {
            "support_type": support_type,
            "acts": acts,
            "reason": top_gap['clarification_prompt'],
            "confidence": 0.95,  # æ§‹é€ çš„æ¬ æã¯é«˜ã„ç¢ºä¿¡åº¦
            "inference_source": f"structural_gap:{top_gap['type']}",
            "structural_analysis": {
                "primary_gap": top_gap,
                "total_gaps": len(all_gaps),
                "missing_element": missing_element,
                "priority": top_gap.get('priority', 'medium'),
                "resolution_focus": "structural_completion"
            },
            "next_ontology_step": {
                "create_node_type": missing_element,
                "establish_relation": top_gap.get('required_relation'),
                "from_node": current_node.id
            }
        }
    
    def _get_or_create_user_profile(self, user_id: str) -> UserProfile:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(
                user_id=user_id,
                learning_style={
                    'analytical': 0.5,
                    'creative': 0.5,
                    'structured': 0.5,
                    'exploratory': 0.5
                },
                preferred_support_types={st: 0.5 for st in SupportType.ALL_TYPES},
                effective_act_combinations={},
                difficulty_preferences={
                    'low': 0.3,
                    'medium': 0.5,
                    'high': 0.2
                },
                session_patterns=[],
                adaptation_history=[]
            )
        
        return self.user_profiles[user_id]
    
    def _extract_context_features(self, node: Node, user_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆä¼šè©±æ–‡è„ˆã‚’å«ã‚€ï¼‰"""
        
        features = {
            'node_type': node.type.value,
            'clarity': node.clarity,
            'depth': node.depth,
            'confidence': node.confidence,
            'alignment_goal': node.alignment_goal,
            'tag_count': len(node.tags),
            'metadata_keys': list(node.metadata.keys()) if node.metadata else [],
            'time_since_creation': (datetime.now() - node.timestamp).total_seconds() / 3600,  # hours
            'session_context': user_context or {}
        }
        
        # â˜…é‡è¦: ä¼šè©±æ–‡è„ˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
        if node.metadata and 'conversation_context' in node.metadata:
            conv_context = node.metadata['conversation_context']
            features['has_conversation_context'] = True
            features['current_topic'] = conv_context.get('current_topic')
            features['mentioned_entities'] = conv_context.get('mentioned_entities', [])
            features['context_chain'] = conv_context.get('context_chain', [])
            features['key_phrases'] = conv_context.get('key_phrases', [])
        else:
            features['has_conversation_context'] = False
            features['current_topic'] = None
            features['mentioned_entities'] = []
            features['context_chain'] = []
            features['key_phrases'] = []
        
        # ã‚°ãƒ©ãƒ•æ§‹é€ çš„ç‰¹å¾´
        neighbors = self.graph.get_node_neighbors(node.id, "both")
        features['neighbor_count'] = len(neighbors)
        features['neighbor_types'] = [n.type.value for n, _ in neighbors]
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´çš„ç‰¹å¾´
        recent_nodes = self._get_recent_nodes(node.student_id, self.graph, limit=5)
        features['recent_node_types'] = [n.type.value for n in recent_nodes]
        features['recent_avg_clarity'] = np.mean([n.clarity for n in recent_nodes]) if recent_nodes else 0.5
        features['recent_avg_depth'] = np.mean([n.depth for n in recent_nodes]) if recent_nodes else 0.5
        
        return features
    
    def _find_pattern_matches(self, node: Node, context_features: Dict[str, Any]) -> List[Tuple[LearningPattern, float]]:
        """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        
        matches = []
        recent_sequence = context_features.get('recent_node_types', [])
        current_sequence = recent_sequence + [node.type.value]
        
        for pattern in self.learned_patterns.values():
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒãƒƒãƒãƒ³ã‚°
            sequence_similarity = self._calculate_sequence_similarity(
                [nt.value for nt in pattern.sequence], 
                current_sequence
            )
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ãƒãƒƒãƒãƒ³ã‚°
            context_similarity = self._calculate_context_similarity(
                pattern.context_conditions, 
                context_features
            )
            
            # ç·åˆé¡ä¼¼åº¦
            total_similarity = (sequence_similarity * 0.6 + context_similarity * 0.4)
            
            if total_similarity > 0.3:  # é–¾å€¤
                matches.append((pattern, total_similarity))
        
        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches[:5]  # ä¸Šä½5ä»¶
    
    def _calculate_sequence_similarity(self, pattern_seq: List[str], current_seq: List[str]) -> float:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        
        if not pattern_seq or not current_seq:
            return 0.0
        
        # æœ€é•·å…±é€šéƒ¨åˆ†åˆ— (LCS) ã‚’ä½¿ç”¨
        m, n = len(pattern_seq), len(current_seq)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if pattern_seq[i-1] == current_seq[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        return lcs_length / max(m, n)
    
    def _calculate_context_similarity(self, pattern_context: Dict[str, Any], current_context: Dict[str, Any]) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        
        if not pattern_context:
            return 1.0
        
        similarities = []
        
        for key, pattern_value in pattern_context.items():
            if key in current_context:
                current_value = current_context[key]
                
                if isinstance(pattern_value, (int, float)) and isinstance(current_value, (int, float)):
                    # æ•°å€¤ã®å ´åˆï¼šæ­£è¦åŒ–ã•ã‚ŒãŸå·®
                    if pattern_value == 0 and current_value == 0:
                        sim = 1.0
                    else:
                        sim = 1.0 - abs(pattern_value - current_value) / max(abs(pattern_value), abs(current_value), 1.0)
                elif isinstance(pattern_value, str) and isinstance(current_value, str):
                    # æ–‡å­—åˆ—ã®å ´åˆï¼šå®Œå…¨ä¸€è‡´
                    sim = 1.0 if pattern_value == current_value else 0.0
                elif isinstance(pattern_value, list) and isinstance(current_value, list):
                    # ãƒªã‚¹ãƒˆã®å ´åˆï¼šã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰ä¿‚æ•°
                    set1, set2 = set(pattern_value), set(current_value)
                    if not set1 and not set2:
                        sim = 1.0
                    else:
                        sim = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0.0
                else:
                    sim = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                
                similarities.append(sim)
        
        return np.mean(similarities) if similarities else 0.5
    
    def _evaluate_adaptive_rules(self, node: Node, context_features: Dict[str, Any], user_profile: UserProfile) -> List[Dict[str, Any]]:
        """é©å¿œãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡"""
        
        applicable_rules = []
        
        for rule in self.adaptive_rules.values():
            try:
                # ãƒ«ãƒ¼ãƒ«æ¡ä»¶ã‚’è©•ä¾¡
                if self._evaluate_rule_condition(rule, node, context_features, user_profile):
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
                    action = self._generate_rule_action(rule, node, context_features, user_profile)
                    
                    # ä¿¡é ¼åº¦ã‚’è¨ˆç®—
                    confidence = self._calculate_rule_confidence(rule, context_features, user_profile)
                    
                    applicable_rules.append({
                        'rule': rule,
                        'action': action,
                        'confidence': confidence,
                        'priority': rule.priority
                    })
                    
            except Exception as e:
                logger.error(f"ãƒ«ãƒ¼ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {rule.name} - {e}")
        
        # å„ªå…ˆåº¦ã¨ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        applicable_rules.sort(key=lambda x: (x['priority'], x['confidence']), reverse=True)
        return applicable_rules
    
    def _evaluate_rule_condition(self, rule: AdaptiveRule, node: Node, context: Dict[str, Any], profile: UserProfile) -> bool:
        """ãƒ«ãƒ¼ãƒ«æ¡ä»¶ã‚’è©•ä¾¡"""
        
        # åŸºæœ¬çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè©•ä¾¡
        condition = rule.condition_template
        
        # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç½®æ›
        if 'clarity' in condition:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«ã«åŸºã¥ã„ã¦é–¾å€¤ã‚’èª¿æ•´
            clarity_threshold = 0.5 * (1.0 + profile.learning_style.get('analytical', 0.5) - 0.5)
            condition = condition.replace('{threshold}', str(clarity_threshold))
        
        if 'depth' in condition:
            depth_threshold = 0.6 * (1.0 + profile.learning_style.get('structured', 0.5) - 0.5)
            condition = condition.replace('{threshold}', str(depth_threshold))
        
        if '{node_type}' in condition:
            condition = condition.replace('{node_type}', f"'{node.type.value}'")
        
        # å®‰å…¨ãªè©•ä¾¡ç’°å¢ƒ
        safe_dict = {
            'node': node,
            'context': context,
            'profile': profile,
            'NodeType': NodeType
        }
        
        try:
            return eval(condition, {"__builtins__": {}}, safe_dict)
        except:
            return False
    
    def _generate_rule_action(self, rule: AdaptiveRule, node: Node, context: Dict[str, Any], profile: UserProfile) -> Dict[str, Any]:
        """ãƒ«ãƒ¼ãƒ«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
        action_str = rule.action_template
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãèª¿æ•´
        if 'support_type=' in action_str:
            # ã‚µãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿ã«èª¿æ•´
            preferred_support = max(profile.preferred_support_types.items(), key=lambda x: x[1])
            if preferred_support[1] > 0.7:  # å¼·ã„å¥½ã¿ãŒã‚ã‚‹å ´åˆ
                action_str = action_str.replace('UNDERSTANDING', preferred_support[0])
        
        # åŸºæœ¬çš„ãªãƒ‘ãƒ¼ã‚¹
        action = {
            'support_type': SupportType.PATHFINDING,
            'acts': [SpeechAct.OUTLINE, SpeechAct.INFORM],
            'reason': 'é©å¿œãƒ«ãƒ¼ãƒ«é©ç”¨',
            'next_node_type': NodeType.QUESTION,
            'confidence': rule.confidence,
            'applied_rule': rule.rule_id
        }
        
        # ç°¡å˜ãªãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ï¼ˆå®Ÿè£…ã‚’ç°¡ç•¥åŒ–ï¼‰
        if 'UNDERSTANDING' in action_str:
            action['support_type'] = SupportType.UNDERSTANDING
            action['acts'] = [SpeechAct.CLARIFY, SpeechAct.PROBE]
        elif 'PATHFINDING' in action_str:
            action['support_type'] = SupportType.PATHFINDING
            action['acts'] = [SpeechAct.OUTLINE, SpeechAct.ACT]
        elif 'REFRAMING' in action_str:
            action['support_type'] = SupportType.REFRAMING
            action['acts'] = [SpeechAct.REFRAME, SpeechAct.REFLECT]
        
        return action
    
    def _calculate_rule_confidence(self, rule: AdaptiveRule, context: Dict[str, Any], profile: UserProfile) -> float:
        """ãƒ«ãƒ¼ãƒ«ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        
        base_confidence = rule.confidence
        
        # æˆåŠŸç‡ã«ã‚ˆã‚‹èª¿æ•´
        total_uses = rule.success_count + rule.failure_count
        if total_uses > 0:
            success_rate = rule.success_count / total_uses
            success_factor = success_rate * 2 - 1  # -1 to 1
            base_confidence += success_factor * 0.2
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼é©åˆåº¦ã«ã‚ˆã‚‹èª¿æ•´
        user_factor = self._calculate_user_rule_fit(rule, profile)
        base_confidence += user_factor * 0.1
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©åˆåº¦ã«ã‚ˆã‚‹èª¿æ•´
        context_factor = self._calculate_context_rule_fit(rule, context)
        base_confidence += context_factor * 0.1
        
        return max(0.0, min(1.0, base_confidence))
    
    def _calculate_user_rule_fit(self, rule: AdaptiveRule, profile: UserProfile) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ãƒ«ãƒ¼ãƒ«ã®é©åˆåº¦ã‚’è¨ˆç®—"""
        
        # ãƒ«ãƒ¼ãƒ«ã®ç‰¹æ€§ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã®ä¸€è‡´åº¦
        rule_characteristics = {
            'clarity_boost': 'analytical',
            'depth_progression': 'structured',
            'creative_exploration': 'creative',
            'flexible_adaptation': 'exploratory'
        }
        
        fit_score = 0.0
        for char, style in rule_characteristics.items():
            if char in rule.rule_id.lower():
                fit_score += profile.learning_style.get(style, 0.5) - 0.5
        
        return fit_score
    
    def _calculate_context_rule_fit(self, rule: AdaptiveRule, context: Dict[str, Any]) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ«ãƒ¼ãƒ«ã®é©åˆåº¦ã‚’è¨ˆç®—"""
        
        # æ™‚é–“çš„é–¢é€£æ€§
        hours_since_creation = context.get('time_since_creation', 0)
        if hours_since_creation < 1:  # æ–°ã—ã„ãƒãƒ¼ãƒ‰
            temporal_factor = 0.2
        elif hours_since_creation < 24:  # 1æ—¥ä»¥å†…
            temporal_factor = 0.0
        else:  # å¤ã„ãƒãƒ¼ãƒ‰
            temporal_factor = -0.1
        
        # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã¨ã®é–¢é€£æ€§
        node_type = context.get('node_type', '')
        type_factor = 0.0
        if 'question' in rule.rule_id.lower() and node_type == 'Question':
            type_factor = 0.1
        elif 'hypothesis' in rule.rule_id.lower() and node_type == 'Hypothesis':
            type_factor = 0.1
        
        return temporal_factor + type_factor
    
    def _integrate_inference_results(self, basic_result: Dict[str, Any], 
                                   adaptive_results: List[Dict[str, Any]], 
                                   pattern_matches: List[Tuple[LearningPattern, float]],
                                   user_profile: UserProfile,
                                   context_features: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨è«–çµæœã‚’çµ±åˆ"""
        
        # å€™è£œãƒªã‚¹ãƒˆã‚’ä½œæˆ
        candidates = []
        
        # åŸºæœ¬ãƒ«ãƒ¼ãƒ«çµæœ
        candidates.append({
            'result': basic_result,
            'score': basic_result.get('confidence', 0.5) * self.dynamic_weights['rule_confidence'],
            'source': 'basic_rule'
        })
        
        # é©å¿œãƒ«ãƒ¼ãƒ«çµæœ
        for adaptive in adaptive_results[:3]:  # ä¸Šä½3ä»¶
            candidates.append({
                'result': adaptive['action'],
                'score': adaptive['confidence'] * self.dynamic_weights['rule_confidence'],
                'source': f"adaptive_rule:{adaptive['rule'].rule_id}"
            })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒçµæœ
        for pattern, similarity in pattern_matches[:2]:  # ä¸Šä½2ä»¶
            pattern_result = self._generate_pattern_action(pattern, context_features)
            candidates.append({
                'result': pattern_result,
                'score': similarity * pattern.effectiveness_score * self.dynamic_weights['pattern_match'],
                'source': f"pattern:{pattern.pattern_id}"
            })
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿èª¿æ•´
        for candidate in candidates:
            user_pref_score = self._calculate_user_preference_score(
                candidate['result'], user_profile
            )
            candidate['score'] += user_pref_score * self.dynamic_weights['user_preference']
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å€™è£œã‚’é¸æŠ
        best_candidate = max(candidates, key=lambda x: x['score'])
        
        # çµæœã‚’èª¿æ•´
        final_result = best_candidate['result'].copy()
        final_result['inference_source'] = best_candidate['source']
        final_result['integrated_score'] = best_candidate['score']
        final_result['all_candidates'] = [
            {
                'source': c['source'],
                'score': c['score'],
                'support_type': c['result'].get('support_type', ''),
                'acts': c['result'].get('acts', [])
            }
            for c in candidates
        ]
        
        return final_result
    
    def _generate_pattern_action(self, pattern: LearningPattern, context: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’äºˆæ¸¬
        current_type = context.get('node_type', '')
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰æ¬¡ã®ã‚¿ã‚¤ãƒ—ã‚’äºˆæ¸¬
        try:
            current_index = [nt.value for nt in pattern.sequence].index(current_type)
            if current_index < len(pattern.sequence) - 1:
                next_type = pattern.sequence[current_index + 1]
            else:
                next_type = pattern.sequence[0]  # å¾ªç’°
        except ValueError:
            next_type = pattern.sequence[0] if pattern.sequence else NodeType.QUESTION
        
        # ã‚¿ã‚¤ãƒ—ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        type_actions = {
            NodeType.QUESTION: {
                'support_type': SupportType.UNDERSTANDING,
                'acts': [SpeechAct.CLARIFY, SpeechAct.PROBE]
            },
            NodeType.HYPOTHESIS: {
                'support_type': SupportType.NARROWING,
                'acts': [SpeechAct.DECIDE, SpeechAct.OUTLINE]
            },
            NodeType.METHOD: {
                'support_type': SupportType.ACTIVATION,
                'acts': [SpeechAct.ACT, SpeechAct.INFORM]
            },
            NodeType.DATA: {
                'support_type': SupportType.REFRAMING,
                'acts': [SpeechAct.REFLECT, SpeechAct.PROBE]
            },
            NodeType.INSIGHT: {
                'support_type': SupportType.REFRAMING,
                'acts': [SpeechAct.REFRAME, SpeechAct.REFLECT]
            }
        }
        
        action = type_actions.get(next_type, type_actions[NodeType.QUESTION])
        
        return {
            'support_type': action['support_type'],
            'acts': action['acts'],
            'reason': f'ãƒ‘ã‚¿ãƒ¼ãƒ³"{pattern.pattern_id}"ã‹ã‚‰äºˆæ¸¬',
            'next_node_type': next_type,
            'confidence': pattern.effectiveness_score,
            'applied_rule': f'pattern_{pattern.pattern_id}'
        }
    
    def _calculate_user_preference_score(self, result: Dict[str, Any], profile: UserProfile) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        
        support_type = result.get('support_type', '')
        acts = result.get('acts', [])
        
        # ã‚µãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—å¥½ã¿
        support_pref = profile.preferred_support_types.get(support_type, 0.5)
        
        # ã‚¢ã‚¯ãƒˆçµ„ã¿åˆã‚ã›å¥½ã¿
        acts_key = '_'.join(sorted(acts))
        acts_pref = profile.effective_act_combinations.get(acts_key, 0.5)
        
        return (support_pref - 0.5) * 0.3 + (acts_pref - 0.5) * 0.2
    
    def _record_inference(self, node: Node, result: Dict[str, Any], context: Dict[str, Any]):
        """æ¨è«–å±¥æ­´ã‚’è¨˜éŒ²"""
        
        record = {
            'timestamp': datetime.now().isoformat(),
            'user_id': node.student_id,
            'node_id': node.id,
            'node_type': node.type.value,
            'context_features': context,
            'inference_result': result,
            'source': result.get('inference_source', 'unknown')
        }
        
        self.inference_history.append(record)
        
        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.inference_history) > 10000:
            self.inference_history = self.inference_history[-5000:]
    
    def learn_from_feedback(self, inference_id: str, user_id: str, feedback: Dict[str, Any]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’"""
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²
        feedback_record = {
            'timestamp': datetime.now().isoformat(),
            'inference_id': inference_id,
            'user_id': user_id,
            'feedback': feedback
        }
        self.feedback_history.append(feedback_record)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        self._update_user_profile_from_feedback(user_id, feedback)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ›´æ–°
        self._update_patterns_from_feedback(user_id, feedback)
        
        # ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°
        self._update_rules_from_feedback(feedback)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self._save_models()
        
        logger.info(f"ğŸ“š ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’å®Œäº†: {user_id}")
    
    def _update_user_profile_from_feedback(self, user_id: str, feedback: Dict[str, Any]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
        
        profile = self._get_or_create_user_profile(user_id)
        
        satisfaction = feedback.get('satisfaction', 0.5)
        effectiveness = feedback.get('effectiveness', 0.5)
        
        if 'support_type' in feedback:
            support_type = feedback['support_type']
            current_pref = profile.preferred_support_types.get(support_type, 0.5)
            # æŒ‡æ•°ç§»å‹•å¹³å‡ã§æ›´æ–°
            alpha = 0.1
            profile.preferred_support_types[support_type] = (
                current_pref * (1 - alpha) + satisfaction * alpha
            )
        
        if 'acts' in feedback:
            acts_key = '_'.join(sorted(feedback['acts']))
            current_eff = profile.effective_act_combinations.get(acts_key, 0.5)
            alpha = 0.1
            profile.effective_act_combinations[acts_key] = (
                current_eff * (1 - alpha) + effectiveness * alpha
            )
        
        # é©å¿œå±¥æ­´ã«è¿½åŠ 
        profile.adaptation_history.append({
            'timestamp': datetime.now().isoformat(),
            'feedback_summary': {
                'satisfaction': satisfaction,
                'effectiveness': effectiveness
            }
        })
        
        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(profile.adaptation_history) > 100:
            profile.adaptation_history = profile.adaptation_history[-50:]
    
    def _update_patterns_from_feedback(self, user_id: str, feedback: Dict[str, Any]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ›´æ–°"""
        
        effectiveness = feedback.get('effectiveness', 0.5)
        
        # æœ€è¿‘ã®æ¨è«–ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä½¿ã‚ã‚ŒãŸå ´åˆ
        recent_inferences = [
            r for r in self.inference_history[-10:]
            if r['user_id'] == user_id and 'pattern:' in r.get('source', '')
        ]
        
        for inference in recent_inferences:
            pattern_id = inference['source'].split(':')[1]
            if pattern_id in self.learned_patterns:
                pattern = self.learned_patterns[pattern_id]
                
                # åŠ¹æœã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
                alpha = 0.2
                pattern.effectiveness_score = (
                    pattern.effectiveness_score * (1 - alpha) + effectiveness * alpha
                )
                
                # ä½¿ç”¨ã‚«ã‚¦ãƒ³ãƒˆå¢—åŠ 
                pattern.usage_count += 1
                pattern.last_used = datetime.now()
    
    def _update_rules_from_feedback(self, feedback: Dict[str, Any]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°"""
        
        effectiveness = feedback.get('effectiveness', 0.5)
        
        # æœ€è¿‘ã®æ¨è«–ã§é©å¿œãƒ«ãƒ¼ãƒ«ãŒä½¿ã‚ã‚ŒãŸå ´åˆ
        recent_adaptive_inferences = [
            r for r in self.inference_history[-5:]
            if 'adaptive_rule:' in r.get('source', '')
        ]
        
        for inference in recent_adaptive_inferences:
            rule_id = inference['source'].split(':')[1]
            if rule_id in self.adaptive_rules:
                rule = self.adaptive_rules[rule_id]
                
                # æˆåŠŸ/å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
                if effectiveness > 0.6:
                    rule.success_count += 1
                else:
                    rule.failure_count += 1
                
                # ä¿¡é ¼åº¦èª¿æ•´
                total_uses = rule.success_count + rule.failure_count
                if total_uses > 0:
                    success_rate = rule.success_count / total_uses
                    rule.confidence = min(0.95, max(0.1, success_rate))
                
                rule.last_updated = datetime.now()
    
    def discover_new_patterns(self, user_id: str, min_support: int = 3) -> List[LearningPattern]:
        """æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹"""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŠ½å‡º
        user_nodes = [n for n in self.graph.nodes.values() if n.student_id == user_id]
        user_nodes.sort(key=lambda n: n.timestamp)
        
        # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
        type_sequence = [n.type for n in user_nodes]
        
        # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚¤ãƒ‹ãƒ³ã‚°
        new_patterns = []
        
        for length in range(3, 6):  # é•·ã•3-5ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            for i in range(len(type_sequence) - length + 1):
                pattern_seq = type_sequence[i:i+length]
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                occurrences = self._count_pattern_occurrences(pattern_seq, type_sequence)
                
                if occurrences >= min_support:
                    # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦è¿½åŠ 
                    pattern_id = f"discovered_{user_id}_{length}_{i}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
                    
                    # åŠ¹æœã‚¹ã‚³ã‚¢ã‚’åˆæœŸåŒ–ï¼ˆéå»ã®æˆåŠŸäº‹ä¾‹ã‹ã‚‰æ¨æ¸¬ï¼‰
                    effectiveness = self._estimate_pattern_effectiveness(pattern_seq, user_id)
                    
                    new_pattern = LearningPattern(
                        pattern_id=pattern_id,
                        sequence=pattern_seq,
                        success_rate=0.5,
                        usage_count=0,
                        last_used=datetime.now(),
                        effectiveness_score=effectiveness,
                        context_conditions=self._extract_pattern_context(pattern_seq, user_nodes[i:i+length])
                    )
                    
                    self.learned_patterns[pattern_id] = new_pattern
                    new_patterns.append(new_pattern)
        
        logger.info(f"ğŸ” æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹: {len(new_patterns)} patterns for user {user_id}")
        return new_patterns
    
    def _count_pattern_occurrences(self, pattern: List[NodeType], sequence: List[NodeType]) -> int:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = 0
        pattern_len = len(pattern)
        
        for i in range(len(sequence) - pattern_len + 1):
            if sequence[i:i+pattern_len] == pattern:
                count += 1
        
        return count
    
    def _estimate_pattern_effectiveness(self, pattern: List[NodeType], user_id: str) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŠ¹æœã‚’æ¨å®š"""
        
        # é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŠ¹æœã‹ã‚‰æ¨å®š
        similar_effectiveness = []
        
        for existing_pattern in self.learned_patterns.values():
            similarity = self._calculate_sequence_similarity(
                [nt.value for nt in pattern],
                [nt.value for nt in existing_pattern.sequence]
            )
            
            if similarity > 0.5:
                similar_effectiveness.append(existing_pattern.effectiveness_score)
        
        if similar_effectiveness:
            return np.mean(similar_effectiveness)
        else:
            return 0.6  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _extract_pattern_context(self, pattern: List[NodeType], nodes: List[Node]) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã‚’æŠ½å‡º"""
        
        context = {}
        
        if nodes:
            context['avg_clarity'] = np.mean([n.clarity for n in nodes])
            context['avg_depth'] = np.mean([n.depth for n in nodes])
            context['avg_confidence'] = np.mean([n.confidence for n in nodes])
            
            # æ™‚é–“çš„ç‰¹å¾´
            time_spans = []
            for i in range(1, len(nodes)):
                span = (nodes[i].timestamp - nodes[i-1].timestamp).total_seconds() / 3600
                time_spans.append(span)
            
            if time_spans:
                context['avg_time_span_hours'] = np.mean(time_spans)
        
        return context
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """å­¦ç¿’çµ±è¨ˆã‚’å–å¾—"""
        
        stats = {
            'learned_patterns_count': len(self.learned_patterns),
            'adaptive_rules_count': len(self.adaptive_rules),
            'user_profiles_count': len(self.user_profiles),
            'inference_history_count': len(self.inference_history),
            'feedback_history_count': len(self.feedback_history),
            'top_patterns': [],
            'top_rules': [],
            'user_learning_summary': {}
        }
        
        # ãƒˆãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        top_patterns = sorted(
            self.learned_patterns.values(),
            key=lambda p: p.effectiveness_score * p.usage_count,
            reverse=True
        )[:5]
        
        stats['top_patterns'] = [
            {
                'pattern_id': p.pattern_id,
                'sequence': [nt.value for nt in p.sequence],
                'effectiveness_score': p.effectiveness_score,
                'usage_count': p.usage_count
            }
            for p in top_patterns
        ]
        
        # ãƒˆãƒƒãƒ—ãƒ«ãƒ¼ãƒ«
        top_rules = sorted(
            self.adaptive_rules.values(),
            key=lambda r: r.confidence * (r.success_count + 1),
            reverse=True
        )[:5]
        
        stats['top_rules'] = [
            {
                'rule_id': r.rule_id,
                'name': r.name,
                'confidence': r.confidence,
                'success_rate': r.success_count / (r.success_count + r.failure_count) if (r.success_count + r.failure_count) > 0 else 0
            }
            for r in top_rules
        ]
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å­¦ç¿’ã‚µãƒãƒªãƒ¼
        for user_id, profile in self.user_profiles.items():
            stats['user_learning_summary'][user_id] = {
                'learning_style': profile.learning_style,
                'preferred_support_type': max(profile.preferred_support_types.items(), key=lambda x: x[1])[0],
                'adaptation_count': len(profile.adaptation_history)
            }
        
        return stats