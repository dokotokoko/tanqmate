"""
æ¢ç©¶ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã‚’ç”¨ã„ãŸå¯¾è©±ãƒ•ãƒ­ãƒ¼ã‚’çµ±æ‹¬ã™ã‚‹ã‚¯ãƒ©ã‚¹
"""

import json
import logging
import os
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime, timedelta
from pathlib import Path

from conversation_agent.orchestrator import ConversationOrchestrator
from .advanced_inference_engine import AdvancedInferenceEngine
from .ontology_adapter import OntologyAdapter
from .ontology_graph import Node, NodeType, Edge, RelationType
from .graph_inference_engine import GraphInferenceEngine
from .session_manager import SessionManager
from .context_aware_response_generator import ContextAwareResponseGenerator
from .learning_data_collector import LearningDataCollector
from .metrics_manager import MetricsManager
from .result_packager import ResultPackager
from conversation_agent.state_extractor import StateExtractor
from conversation_agent.project_planner import ProjectPlanner
from conversation_agent.schema import (
    StateSnapshot, TurnPackage, SupportType, SpeechAct, ProjectPlan, 
    NextAction, Milestone
)

logger = logging.getLogger(__name__)


class OntologyOrchestrator(ConversationOrchestrator):
    """é«˜åº¦ãªå­¦ç¿’ãƒ»é©å¿œæ©Ÿèƒ½ã‚’æŒã¤å¯¾è©±ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, 
                 llm_client=None,
                 use_mock: bool = False,
                 use_graph: bool = True,
                 use_advanced_inference: bool = True,
                 ontology_path: str = "ontology.yaml",
                 constraints_path: str = "constraints.yaml",
                 model_dir: str = "inference_models"):
        """
        åˆæœŸåŒ–
        
        Args:
            use_advanced_inference: é«˜åº¦ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            model_dir: å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        super().__init__(llm_client, use_mock)
        
        self.use_graph = use_graph
        self.use_advanced_inference = use_advanced_inference
        self.model_dir = Path(model_dir)
        self.ontology_path = ontology_path
        self.constraints_path = constraints_path
        
        # é«˜åº¦ãªæ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        if self.use_graph:
            self._initialize_advanced_systems()
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’æ©Ÿèƒ½
        self.feedback_enabled = True
        self.auto_learning_enabled = True
        
        # æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        self._initialize_component_classes()
        
        logger.info(f"ğŸŒŸ OntologyOrchestratoråˆæœŸåŒ–å®Œäº† (advanced={use_advanced_inference})")
    
    def _initialize_advanced_systems(self):
        """é«˜åº¦ãªã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        try:
            # ã‚ªãƒ³ãƒˆãƒ­ã‚¸ãƒ¼ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
            self.ontology_adapter = OntologyAdapter(
                self.ontology_path,
                self.constraints_path
            )
            
            # é«˜åº¦ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
            if self.use_advanced_inference:
                self.advanced_inference_engine = AdvancedInferenceEngine(
                    self.ontology_adapter.graph,
                    str(self.model_dir)
                )
                logger.info("âœ… é«˜åº¦ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            else:
                # æ¨™æº–æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.inference_engine = GraphInferenceEngine(self.ontology_adapter.graph)
                logger.info("âœ… æ¨™æº–æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            
            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            self._load_graph_data()
            
        except Exception as e:
            logger.error(f"âŒ é«˜åº¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.use_graph = False
            self.use_advanced_inference = False
    
    def _initialize_component_classes(self):
        """æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–"""
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.session_manager = SessionManager(
            session_timeout_minutes=24*60,  # 24æ™‚é–“
            persist_sessions=True
        )
        
        # çŠ¶æ…‹æŠ½å‡ºå™¨ï¼ˆã‚°ãƒ©ãƒ•å¯¾å¿œæ‹¡å¼µç‰ˆï¼‰
        self.state_extractor = StateExtractor(
            llm_client=self.llm_client,
            graph_enabled=self.use_graph
        )
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»å™¨ï¼ˆã‚°ãƒ©ãƒ•å¯¾å¿œæ‹¡å¼µç‰ˆï¼‰
        self.project_planner = ProjectPlanner(
            llm_client=self.llm_client,
            graph_enabled=self.use_graph
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜å¿œç­”ç”Ÿæˆå™¨
        self.response_generator = ContextAwareResponseGenerator(
            llm_client=self.llm_client,
            base_response_generator=self._generate_llm_response
        )
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†å™¨
        self.learning_data_collector = LearningDataCollector(
            data_directory="learning_data",
            persist_data=True
        )
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†
        self.metrics_manager = MetricsManager(
            enable_detailed_tracking=True
        )
        
        # çµæœãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        self.result_packager = ResultPackager(
            include_debug_info=False  # æœ¬ç•ªç’°å¢ƒã§ã¯ç„¡åŠ¹
        )
        
        # å¾“æ¥ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–°ã—ã„ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«çµ±åˆ
        if hasattr(self, 'metrics'):
            self.metrics_manager.metrics = self.metrics
        else:
            self.metrics = self.metrics_manager.metrics
        
        logger.info("âœ… å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¯ãƒ©ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    def _load_graph_data(self):
        """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        nodes_file = Path("nodes.jsonl")
        edges_file = Path("edges.jsonl")
        
        if nodes_file.exists() and edges_file.exists():
            try:
                self.ontology_adapter.graph.import_from_jsonl(
                    str(nodes_file), 
                    str(edges_file)
                )
                logger.info(f"âœ… ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.ontology_adapter.graph.nodes)} nodes")
            except Exception as e:
                logger.error(f"âŒ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_graph_data(self):
        """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        if not self.use_graph:
            return
        
        try:
            nodes_file = Path("nodes.jsonl")
            edges_file = Path("edges.jsonl")
            
            self.ontology_adapter.graph.export_to_jsonl(
                str(nodes_file),
                str(edges_file)
            )
            logger.info("ğŸ’¾ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_turn(self,
                    user_message: str,
                    conversation_history: List[Dict[str, str]],
                    project_context: Optional[Dict[str, Any]] = None,
                    user_id: Optional[int] = None,
                    conversation_id: Optional[str] = None,
                    session_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        é«˜åº¦ãªå¯¾è©±å‡¦ç†ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼šå„è²¬å‹™ã‚’å°‚é–€ã‚¯ãƒ©ã‚¹ã«å§”è­²ï¼‰
        """
        
        logger.info("ğŸš€ OntologyOrchestratorå‡¦ç†é–‹å§‹ï¼ˆçµ±åˆç‰ˆï¼‰")
        logger.info(f"   - ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰: {self.use_graph}")
        logger.info(f"   - é«˜åº¦æ¨è«–: {self.use_advanced_inference}")
        
        try:
            # 1. ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆSessionManagerï¼‰
            session_id = f"{user_id}_{conversation_id}" if user_id and conversation_id else str(user_id)
            session_info = self.session_manager.get_or_create_session(session_id, session_context)
            logger.info(f"ğŸ“‹ Step 1: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†å®Œäº† (ID: {session_id})")
            
            # 2. çŠ¶æ…‹æŠ½å‡ºï¼ˆStateExtractor - æ‹¡å¼µç‰ˆï¼‰
            logger.info("ğŸ“Š Step 2: æ‹¡å¼µçŠ¶æ…‹æŠ½å‡ºé–‹å§‹")
            state = self.state_extractor.extract_enhanced_state(
                conversation_history, project_context, user_id, conversation_id, session_info
            )
            logger.info(f"âœ… Step 2å®Œäº†: ç›®æ¨™={state.goal or 'æœªè¨­å®š'}")
            
            # 3. ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ç®¡ç†ï¼ˆOntologyAdapterï¼‰
            graph_node = None
            graph_context = None
            if self.use_graph and user_id:
                logger.info("ğŸ”„ Step 3: ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ä½œæˆãƒ»ç®¡ç†é–‹å§‹")
                graph_node = self.ontology_adapter.create_enhanced_graph_node(
                    state, user_message, str(user_id), session_info
                )
                graph_context = self.ontology_adapter.get_graph_context(str(user_id))
                logger.info(f"âœ… Step 3å®Œäº†: ãƒãƒ¼ãƒ‰ä½œæˆ (ID: {graph_node.id})")
            
            # 4. æ¨è«–å®Ÿè¡Œï¼ˆGraphInferenceEngine or AdvancedInferenceEngineï¼‰
            logger.info("ğŸ§  Step 4: æ¨è«–å®Ÿè¡Œé–‹å§‹")
            if self.use_graph and graph_node:
                if self.use_advanced_inference and hasattr(self, 'advanced_inference_engine'):
                    inference_result = self.advanced_inference_engine.infer_next_step_advanced(
                        graph_node, session_context
                    )
                else:
                    inference_result = self.inference_engine.infer_next_step(graph_node)
                
                # æ¨è«–çµæœã§ãƒãƒ¼ãƒ‰ã‚’æ›´æ–°
                self.ontology_adapter.update_node_with_inference_result(graph_node, inference_result)
            else:
                # å¾“æ¥ã®æ¨è«–
                support_type, reason, confidence = self._determine_support_type(state)
                selected_acts, act_reason = self._select_acts(state, support_type)
                inference_result = {
                    "support_type": support_type,
                    "acts": selected_acts,
                    "reason": f"{reason} / {act_reason}",
                    "confidence": confidence
                }
            
            support_type = inference_result["support_type"]
            selected_acts = inference_result["acts"]
            reason = inference_result["reason"]
            confidence = inference_result["confidence"]
            
            logger.info(f"âœ… Step 4å®Œäº†: {support_type} (ç¢ºä¿¡åº¦: {confidence:.2f})")
            
            # 5. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»ç”Ÿæˆï¼ˆProjectPlannerï¼‰
            logger.info("ğŸ“‹ Step 5: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»ç”Ÿæˆé–‹å§‹")
            if self.use_graph and graph_node:
                predictions = inference_result.get('predictions', [])
                project_plan = self.project_planner.generate_graph_based_plan(
                    graph_node, state, inference_result, predictions
                )
            else:
                project_plan = self.project_planner.generate_project_plan(
                    state, conversation_history
                )
            logger.info(f"âœ… Step 5å®Œäº†: è¨ˆç”»ç”Ÿæˆ")
            
            # 6. å¿œç­”ç”Ÿæˆï¼ˆContextAwareResponseGeneratorï¼‰
            logger.info("ğŸ“ Step 6: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜å¿œç­”ç”Ÿæˆé–‹å§‹")
            response_package = self.response_generator.generate_context_aware_response(
                state, support_type, selected_acts, user_message, 
                session_info, inference_result, graph_node
            )
            logger.info(f"âœ… Step 6å®Œäº†: å¿œç­”æ–‡å­—æ•°={len(response_package.natural_reply)}")
            
            # 7. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆLearningDataCollectorï¼‰
            if graph_node:
                logger.info("ğŸ“ˆ Step 7: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
                learning_data = self.learning_data_collector.collect_learning_data(
                    graph_node, inference_result, response_package, session_info, state
                )
                logger.info(f"âœ… Step 7å®Œäº†: ãƒ‡ãƒ¼ã‚¿åé›†")
            
            # 8. ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ï¼ˆMetricsManagerï¼‰
            logger.info("ğŸ“Š Step 8: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°é–‹å§‹")
            self.metrics_manager.update_basic_metrics(state, support_type, selected_acts, confidence)
            if graph_node:
                self.metrics_manager.update_graph_metrics(graph_node, inference_result, graph_context)
            self.metrics_manager.update_inference_metrics(inference_result)
            self.metrics_manager.update_response_metrics(response_package)
            self.metrics_manager.update_session_metrics(session_info, str(user_id))
            if 'learning_data' in locals():
                self.metrics_manager.update_learning_effectiveness(learning_data)
            logger.info(f"âœ… Step 8å®Œäº†: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°")
            
            # 9. ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±æ›´æ–°ï¼ˆSessionManagerï¼‰
            self.session_manager.add_to_learning_trajectory(session_id, {
                'support_type': support_type,
                'acts': selected_acts,
                'confidence': confidence,
                'depth': graph_node.depth if graph_node else 0.5
            })
            
            # 10. ã‚°ãƒ©ãƒ•æ›´æ–°ã¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            if self.use_graph and graph_node:
                logger.info("ğŸ”„ Step 10: ã‚°ãƒ©ãƒ•æ›´æ–°ãƒ»ä¿å­˜é–‹å§‹")
                # å¿œç­”ã«åŸºã¥ã„ã¦ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°
                response_type = self._classify_response_type(response_package.natural_reply)
                if response_type:
                    self.ontology_adapter.create_graph_edge_from_response(
                        graph_node, response_package.natural_reply, response_type
                    )
                self._save_graph_data()
                logger.info(f"âœ… Step 10å®Œäº†: ã‚°ãƒ©ãƒ•æ›´æ–°ãƒ»ä¿å­˜")
            
            # 11. å±¥æ­´æ›´æ–°ï¼ˆå¾“æ¥æ©Ÿèƒ½ï¼‰
            self._update_history(support_type, selected_acts, response_package)
            
            # 12. çµæœãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆResultPackagerï¼‰
            logger.info("ğŸ“¦ Step 12: çµæœãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°é–‹å§‹")
            result = self.result_packager.package_enhanced_result(
                response_package=response_package,
                support_type=support_type,
                selected_acts=selected_acts,
                state=state,
                project_plan=project_plan,
                reason=reason,
                confidence=confidence,
                inference_result=inference_result,
                session_info=session_info,
                graph_context=graph_context,
                metrics=self.metrics_manager.get_comprehensive_metrics(),
                learning_data=learning_data if 'learning_data' in locals() else None,
                mode="enhanced_unified"
            )
            logger.info(f"âœ… Step 12å®Œäº†: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°")
            
            logger.info("ğŸ‰ OntologyOrchestratorå‡¦ç†å®Œäº†ï¼ˆçµ±åˆç‰ˆï¼‰")
            return result
            
        except Exception as e:
            import traceback
            logger.error(f"âŒ å¯¾è©±å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"âŒ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            
            # ã‚¨ãƒ©ãƒ¼çµæœã‚‚ResultPackagerã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
            return self.result_packager.package_error_result(
                error=e,
                context={"user_id": user_id, "session_id": session_id if 'session_id' in locals() else None}
            )
    
    def _classify_response_type(self, response_text: str) -> Optional[str]:
        """å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿œç­”ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        
        if "ï¼Ÿ" in response_text or "?" in response_text:
            return "question"
        elif any(word in response_text for word in ["ã—ã¦ã¿ã¾ã—ã‚‡ã†", "è©¦ã—ã¦", "å®Ÿé¨“", "å®Ÿè¡Œ"]):
            return "method"
        elif any(word in response_text for word in ["ã¾ã¨ã‚ã‚‹ã¨", "ã¤ã¾ã‚Š", "æ•´ç†ã™ã‚‹ã¨", "æŒ¯ã‚Šè¿”ã‚‹"]):
            return "reflection"
        elif any(word in response_text for word in ["ä»®èª¬", "æ¨æ¸¬", "è€ƒãˆã‚‰ã‚Œã‚‹"]):
            return "hypothesis"
        elif any(word in response_text for word in ["æ´å¯Ÿ", "æ°—ã¥ã", "ç™ºè¦‹", "ç†è§£"]):
            return "insight"
        else:
            return None
    
    def _get_or_create_session(self, session_id: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã¾ãŸã¯ä½œæˆï¼ˆDEPRECATED: SessionManagerã‚’ä½¿ç”¨ï¼‰"""
        
        # æ–°ã—ã„SessionManagerã‚¯ãƒ©ã‚¹ã«å§”è­²
        return self.session_manager.get_or_create_session(session_id, context)
    
    def _extract_enhanced_state(self, 
                               conversation_history: List[Dict[str, str]],
                               project_context: Optional[Dict[str, Any]],
                               user_id: Optional[int],
                               conversation_id: Optional[str],
                               session_info: Dict[str, Any]) -> StateSnapshot:
        """æ‹¡å¼µçŠ¶æ…‹æŠ½å‡ºï¼ˆDEPRECATED: StateExtractorã‚’ä½¿ç”¨ï¼‰"""
        
        # æ–°ã—ã„StateExtractorã‚¯ãƒ©ã‚¹ã«å§”è­²
        return self.state_extractor.extract_enhanced_state(
            conversation_history, project_context, user_id, conversation_id, session_info
        )
    
    def _get_conversation_context(self, state) -> Dict[str, Any]:
        """å®‰å…¨ã«conversation_contextã‚’å–å¾—ã™ã‚‹"""
        try:
            return getattr(state, 'conversation_context', {}) or {}
        except AttributeError:
            return {}
    
    def _set_conversation_context(self, state, context: Dict[str, Any]) -> None:
        """å®‰å…¨ã«conversation_contextã‚’è¨­å®šã™ã‚‹"""
        try:
            state.conversation_context = context
        except AttributeError:
            logger.warning("StateSnapshotã«conversation_contextãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å‹•çš„ã«è¿½åŠ ã—ã¾ã™ã€‚")
            setattr(state, 'conversation_context', context)

    def _extract_conversation_context(self, conversation_history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ä¼šè©±å±¥æ­´ã‹ã‚‰æ–‡è„ˆæƒ…å ±ã‚’æ±ç”¨çš„ã«æŠ½å‡º"""
        import re
        
        context = {
            'topics': [],
            'current_topic': None,
            'mentioned_entities': [],
            'key_phrases': [],
            'context_chain': [],
            'user_interests': [],
            'discussion_subjects': []
        }
        
        if not conversation_history:
            return context
        
        # æœ€è¿‘ã®ä¼šè©±ã‹ã‚‰é‡è¦ãªè¦ç´ ã‚’æŠ½å‡º
        recent_messages = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
        
        # ç›´å‰ã®ä¼šè©±å†…å®¹ã‚’é‡è¦–
        previous_assistant_response = None
        
        for msg in recent_messages:
            if msg.get('role') == 'user':
                user_text = msg.get('content', '')
                
                # åè©å¥ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªæ–¹æ³•ï¼‰
                # ã€Œã€œã«ã¤ã„ã¦ã€ã€Œã€œã«é–¢ã—ã¦ã€ã€Œã€œã‚’ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ä¸»é¡Œã‚’æ¤œå‡º
                topic_patterns = [
                    r'(.+?)ã«ã¤ã„ã¦',
                    r'(.+?)ã«é–¢ã—ã¦',
                    r'(.+?)ã«èˆˆå‘³',
                    r'(.+?)ã‚’(.+?)ã—ãŸã„',
                    r'(.+?)ãŒ(.+?)ã§ã™',
                    r'(.+?)ã§(.+?)ã‚’'
                ]
                
                for pattern in topic_patterns:
                    matches = re.findall(pattern, user_text)
                    if matches:
                        if isinstance(matches[0], tuple):
                            # ã‚¿ãƒ—ãƒ«ã®å ´åˆã€æœ€åˆã®è¦ç´ ã‚’å–å¾—
                            topic = matches[0][0].strip()
                        else:
                            topic = matches[0].strip()
                        
                        # çŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if len(topic) > 1 and topic not in ['ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ', 'ä½•']:
                            if topic not in context['mentioned_entities']:
                                context['mentioned_entities'].append(topic)
                            context['current_topic'] = topic
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‹ã‚‰èˆˆå‘³ãƒ»é–¢å¿ƒã‚’æŠ½å‡º
                # çŸ­ã„å›ç­”ï¼ˆå˜èªã‚„çŸ­æ–‡ï¼‰ã‚‚æ–‡è„ˆã¨ã—ã¦ä¿æŒ
                if len(user_text) < 20 and not any(punct in user_text for punct in ['ï¼Ÿ', '?', 'ã€‚']):
                    # çŸ­ã„å›ç­”ã¯èˆˆå‘³ã®è¡¨æ˜ã¨ã—ã¦æ‰±ã†
                    context['user_interests'].append(user_text.strip())
                    if not context['current_topic']:
                        context['current_topic'] = user_text.strip()
                
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–¢é€£ã®å‹•è©ã‚’æ¤œå‡º
                action_words = ['ä½œã‚‹', 'ä½œã‚Š', 'é–‹ç™º', 'æ§‹ç¯‰', 'å®Ÿè£…', 'è¨­è¨ˆ', 'ã¤ãã‚‹', 'åˆ¶ä½œ',
                              'å­¦ã¶', 'å­¦ç¿’', 'å‹‰å¼·', 'ç ”ç©¶', 'èª¿ã¹ã‚‹', 'çŸ¥ã‚‹', 'ç†è§£',
                              'å§‹ã‚ã‚‹', 'ã‚„ã‚‹', 'è©¦ã™', 'ä½¿ã†', 'æ´»ç”¨', 'å¿œç”¨']
                
                for word in action_words:
                    if word in user_text:
                        if context['current_topic']:
                            phrase = f"{context['current_topic']}ã‚’{word}"
                        else:
                            phrase = f"{word}ã“ã¨"
                        context['key_phrases'].append(phrase)
            
            elif msg.get('role') == 'assistant':
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è³ªå•ã‹ã‚‰æ–‡è„ˆã‚’æŠ½å‡º
                assistant_text = msg.get('content', '')
                previous_assistant_response = assistant_text
                
                # ã€Œä½•ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã€ã®ã‚ˆã†ãªè³ªå•ã‹ã‚‰æ–‡è„ˆã‚’æŠ½å‡º
                question_patterns = [
                    r'ä½•ã«(.+?)ã¾ã™ã‹',
                    r'(.+?)ã®ä½•ã«',
                    r'(.+?)ã«ã¤ã„ã¦',
                    r'ã©ã‚“ãª(.+?)ã‚’'
                ]
                
                for pattern in question_patterns:
                    matches = re.findall(pattern, assistant_text)
                    if matches:
                        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒèã„ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚‚æ–‡è„ˆã¨ã—ã¦ä¿æŒ
                        for match in matches:
                            if isinstance(match, str) and len(match) > 1:
                                context['discussion_subjects'].append(match)
        
        # ãƒˆãƒ”ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        all_topics = context['mentioned_entities'] + context['user_interests']
        if all_topics:
            context['topics'] = all_topics[:5]  # æœ€å¤§5ã¤ã®ãƒˆãƒ”ãƒƒã‚¯
            context['context_chain'] = all_topics[-3:]  # æœ€è¿‘ã®3ã¤
        
        # æ–‡è„ˆã®ç¶™ç¶šæ€§ã‚’ç¢ºä¿
        if not context['current_topic'] and context['user_interests']:
            # ç¾åœ¨ã®ãƒˆãƒ”ãƒƒã‚¯ãŒä¸æ˜ãªå ´åˆã€æœ€æ–°ã®èˆˆå‘³ã‚’ä½¿ç”¨
            context['current_topic'] = context['user_interests'][-1]
        
        return context
    
    def _find_common_elements(self, element_lists: List[List[str]]) -> List[str]:
        """è¤‡æ•°ã®ãƒªã‚¹ãƒˆã‹ã‚‰å…±é€šè¦ç´ ã‚’è¦‹ã¤ã‘ã‚‹"""
        if not element_lists:
            return []
        
        # è¦ç´ ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        element_counts = {}
        for elements in element_lists:
            for element in elements:
                element_counts[element] = element_counts.get(element, 0) + 1
        
        # åŠæ•°ä»¥ä¸Šã§å‡ºç¾ã™ã‚‹è¦ç´ ã‚’å…±é€šè¦ç´ ã¨ã™ã‚‹
        threshold = len(element_lists) // 2 + 1
        return [element for element, count in element_counts.items() if count >= threshold]
    
    def _create_graph_node(self, state: StateSnapshot, user_message: str, user_id: str) -> Node:
        """çŠ¶æ…‹ã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ã‚’ä½œæˆï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        
        # æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ã‚’å–å¾—ã¾ãŸã¯æ–°è¦ä½œæˆ
        current_position = self.ontology_adapter.graph.get_current_position(user_id)
        
        if current_position:
            # å‰ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰æƒ…å ±ã‚’å¼•ãç¶™ã
            new_node = self.ontology_adapter.state_to_graph_node(state, user_id)
            new_node.depth = current_position.depth + 0.1
            new_node.alignment_goal = current_position.alignment_goal * 0.95
        else:
            # æ–°è¦ãƒãƒ¼ãƒ‰
            new_node = self.ontology_adapter.state_to_graph_node(state, user_id)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åæ˜ 
        new_node.text = user_message[:200] if user_message else new_node.text
        
        # ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
        self.ontology_adapter.graph.add_node(new_node)
        
        # å‰ã®ãƒãƒ¼ãƒ‰ã¨ã®é–¢ä¿‚ã‚’ä½œæˆ
        if current_position:
            rel_type = self._infer_relation_type(current_position, new_node)
            if rel_type:
                edge = Edge(
                    src=current_position.id,
                    rel=rel_type,
                    dst=new_node.id,
                    confidence=0.7
                )
                self.ontology_adapter.graph.add_edge(edge)
        
        return new_node
    
    def _infer_relation_type(self, src_node: Node, dst_node: Node) -> Optional[RelationType]:
        """ãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ã‚’æ¨è«–"""
        return self.ontology_adapter._determine_relation_type(src_node.type, dst_node.type)
    
    def _generate_graph_based_plan(self, node: Node, state: StateSnapshot) -> Optional[Any]:
        """ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»ã‚’ç”Ÿæˆï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        
        # ã‚°ãƒ©ãƒ•ã®ç¾åœ¨ä½ç½®ã‹ã‚‰è¨ˆç”»ã‚’ç”Ÿæˆ
        from conversation_agent.schema import ProjectPlan, NextAction, Milestone
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬
        predictions = self.inference_engine.predict_next_nodes(node, depth=5)
        
        # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ç”Ÿæˆ
        milestones = []
        for i, pred in enumerate(predictions[:3]):
            milestones.append(Milestone(
                title=f"{pred['node_type'].value}ã®é”æˆ",
                description=f"æ¢ç©¶ãƒ—ãƒ­ã‚»ã‚¹ã®ç¬¬{i+1}æ®µéš",
                target_date=f"{i+1}é€±é–“å¾Œ",
                success_criteria=[f"{pred['node_type'].value}ãŒæ˜ç¢ºã«ãªã‚‹"],
                order=i+1
            ))
        
        # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        next_actions = []
        suggestions = self.ontology_adapter.graph.suggest_next_step(node)
        
        for i, suggestion in enumerate(suggestions[:3]):
            next_actions.append(NextAction(
                action=suggestion["action"],
                urgency=5 if suggestion["priority"] == "high" else 3,
                importance=4,
                reason=suggestion["reason"],
                expected_outcome=f"{suggestion['action']}ã®å®Œäº†"
            ))
        
        # ä»£æ›¿ãƒ‘ã‚¹ã®æ¤œè¨
        alternatives = self.inference_engine.suggest_alternative_paths(
            node, 
            NodeType.INSIGHT
        )
        
        strategic_approach = "ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ¢ç©¶ãƒ—ãƒ­ã‚»ã‚¹"
        if alternatives:
            best_alt = alternatives[0]
            strategic_approach = best_alt["description"]
        
        return ProjectPlan(
            north_star="æ¢ç©¶ã®æ·±åŒ–ã¨å¾ªç’°çš„ãªå­¦ã³",
            north_star_metric="ã‚°ãƒ©ãƒ•ã®ã‚µã‚¤ã‚¯ãƒ«å®Œæˆæ•°",
            milestones=milestones,
            next_actions=next_actions,
            strategic_approach=strategic_approach,
            risk_factors=["ãƒ«ãƒ¼ãƒ—ã«é™¥ã‚‹å¯èƒ½æ€§", "æ˜ç¢ºæ€§ã®ä½ä¸‹"],
            created_at=datetime.now().isoformat(),
            confidence=0.7
        )
    
    def _update_graph_with_response(self, 
                                   current_node: Node,
                                   response_package: TurnPackage,
                                   next_node_type: NodeType):
        """å¿œç­”ã«åŸºã¥ã„ã¦ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°ï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        
        # å¿œç­”ã®å†…å®¹ã‹ã‚‰æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹ã‹åˆ¤æ–­
        response_text = response_package.natural_reply
        
        # è³ªå•ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        if "ï¼Ÿ" in response_text or "?" in response_text:
            response_type = "question"
        # ææ¡ˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        elif any(word in response_text for word in ["ã—ã¦ã¿ã¾ã—ã‚‡ã†", "è©¦ã—ã¦", "å®Ÿé¨“"]):
            response_type = "method"
        # æŒ¯ã‚Šè¿”ã‚ŠãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        elif any(word in response_text for word in ["ã¾ã¨ã‚ã‚‹ã¨", "ã¤ã¾ã‚Š", "æ•´ç†ã™ã‚‹ã¨"]):
            response_type = "reflection"
        else:
            response_type = None
        
        if response_type:
            self.ontology_adapter.update_graph_from_response(
                current_node,
                response_type,
                response_text,
                confidence=0.6
            )
    
    def get_graph_insights(self, user_id: str) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰æ´å¯Ÿã‚’å–å¾—ï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        
        if not self.use_graph:
            return {"error": "ã‚°ãƒ©ãƒ•ãƒ¢ãƒ¼ãƒ‰ãŒç„¡åŠ¹ã§ã™"}
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹
        patterns = self.inference_engine.find_patterns(user_id, pattern_length=3)
        
        # é€²æ—æƒ…å ±
        progress = self.ontology_adapter.graph.calculate_progress(user_id)
        
        # ç¾åœ¨ä½ç½®
        current_position = self.ontology_adapter.graph.get_current_position(user_id)
        
        return {
            "current_position": current_position.to_dict() if current_position else None,
            "progress": progress,
            "patterns": patterns,
            "total_nodes": len([n for n in self.ontology_adapter.graph.nodes.values() 
                              if n.student_id == user_id]),
            "graph_mode": "enabled"
        }
    
    def switch_mode(self, use_graph: bool):
        """å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆï¼ˆV1ã‹ã‚‰çµ±åˆï¼‰"""
        
        old_mode = "graph" if self.use_graph else "linear"
        new_mode = "graph" if use_graph else "linear"
        
        if old_mode == new_mode:
            logger.info(f"ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´ãªã—: {old_mode}")
            return
        
        self.use_graph = use_graph
        
        if use_graph and not hasattr(self, 'ontology_adapter'):
            # ã‚°ãƒ©ãƒ•ã‚·ã‚¹ãƒ†ãƒ ã‚’é…å»¶åˆæœŸåŒ–
            self.ontology_adapter = OntologyAdapter()
            self.inference_engine = GraphInferenceEngine(self.ontology_adapter.graph)
            self._load_graph_data()
        
        logger.info(f"âœ… ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆå®Œäº†: {old_mode} â†’ {new_mode}")
    
    def _create_enhanced_graph_node(self, 
                                   state: StateSnapshot, 
                                   user_message: str, 
                                   user_id: str,
                                   session_info: Dict[str, Any]) -> Node:
        """æ‹¡å¼µã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã¨ä¼šè©±æ–‡è„ˆã®çµ±åˆï¼‰"""
        
        # åŸºæœ¬ãƒãƒ¼ãƒ‰ä½œæˆ
        node = self._create_graph_node(state, user_message, user_id)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã§ãƒãƒ¼ãƒ‰ã‚’å¼·åŒ–
        node.metadata = node.metadata or {}
        node.metadata['session_id'] = session_info['session_id']
        node.metadata['interaction_count'] = session_info['interaction_count']
        
        # â˜…é‡è¦: ä¼šè©±æ–‡è„ˆã‚’ä¿å­˜
        conversation_context = self._get_conversation_context(state)
        if conversation_context:
            node.metadata['conversation_context'] = conversation_context
        
        # å‰ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰æ–‡è„ˆã‚’å¼•ãç¶™ã
        current_position = self.ontology_adapter.graph.get_current_position(user_id)
        if current_position and current_position.metadata.get('conversation_context'):
            # å‰ã®ãƒãƒ¼ãƒ‰ã®æ–‡è„ˆã‚’å¼•ãç¶™ã„ã§æ‹¡å¼µ
            prev_context = current_position.metadata['conversation_context']
            if 'conversation_context' in node.metadata:
                # æ–‡è„ˆã‚’ãƒãƒ¼ã‚¸
                node.metadata['conversation_context'] = {
                    **prev_context,
                    **node.metadata['conversation_context'],
                    'previous_topic': prev_context.get('current_topic'),
                    'context_chain': prev_context.get('context_chain', []) + [prev_context.get('current_topic')]
                }
            else:
                node.metadata['conversation_context'] = prev_context
        
        # å­¦ç¿’è»Œè·¡ã‹ã‚‰æ·±ã•ã‚’èª¿æ•´
        if session_info['learning_trajectory']:
            trajectory = session_info['learning_trajectory']
            depth_progression = [item.get('depth', 0.5) for item in trajectory[-5:]]
            
            if depth_progression:
                # æ·±ã•ã®é€²è¡Œå‚¾å‘ã‚’åæ˜ 
                avg_depth = sum(depth_progression) / len(depth_progression)
                depth_trend = (depth_progression[-1] - depth_progression[0]) / len(depth_progression) if len(depth_progression) > 1 else 0
                
                node.depth = min(1.0, max(0.0, avg_depth + depth_trend * 0.1))
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‹ã‚‰æ˜ç¢ºæ€§ã‚’èª¿æ•´
        if 'learning_style' in session_info.get('user_preferences', {}):
            style = session_info['user_preferences']['learning_style']
            
            # åˆ†æçš„å­¦ç¿’è€…ã¯æ˜ç¢ºæ€§ã‚’é‡è¦–
            if style.get('analytical', 0.5) > 0.7:
                node.clarity *= 1.1
            # æ¢ç´¢çš„å­¦ç¿’è€…ã¯å¤šå°‘ã®æ›–æ˜§ã•ã‚’è¨±å®¹
            elif style.get('exploratory', 0.5) > 0.7:
                node.clarity *= 0.9
        
        return node
    
    def _generate_adaptive_plan(self, 
                               node: Node, 
                               state: StateSnapshot, 
                               inference_result: Dict[str, Any]) -> Optional[ProjectPlan]:
        """é©å¿œçš„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»ç”Ÿæˆ"""
        
        if not self.use_advanced_inference or not hasattr(self, 'advanced_inference_engine'):
            return self._generate_graph_based_plan(node, state)
        
        # é«˜åº¦æ¨è«–çµæœã‹ã‚‰è¨ˆç”»ã‚’ç”Ÿæˆ
        predictions = inference_result.get('predictions', [])
        inference_source = inference_result.get('inference_source', '')
        
        # åŒ—æ¥µæ˜Ÿã®è¨­å®šï¼ˆæ¨è«–ã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        if 'pattern:' in inference_source:
            north_star = "å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæœ€é©åŒ–ã•ã‚ŒãŸæ¢ç©¶ãƒ—ãƒ­ã‚»ã‚¹"
        elif 'adaptive_rule:' in inference_source:
            north_star = "å€‹äººé©å¿œå‹ã®æ¢ç©¶å­¦ç¿’"
        else:
            north_star = "ã‚°ãƒ©ãƒ•é§†å‹•ã®ä½“ç³»çš„æ¢ç©¶"
        
        # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ç”Ÿæˆï¼ˆäºˆæ¸¬ãƒ™ãƒ¼ã‚¹ï¼‰
        milestones = []
        for i, pred in enumerate(predictions[:4]):
            milestone = Milestone(
                title=f"æ®µéš{i+1}: {pred['node_type'].value}ã®é”æˆ",
                description=f"æ¢ç©¶ãƒ—ãƒ­ã‚»ã‚¹ã®ç¬¬{i+1}æ®µéšï¼ˆä¿¡é ¼åº¦: {pred['confidence']:.2f}ï¼‰",
                target_date=f"{i+1}é€±é–“å¾Œ",
                success_criteria=[
                    f"{pred['node_type'].value}ã®æ˜ç¢ºãªå®šç¾©",
                    f"æ¬¡æ®µéšã¸ã®æº–å‚™å®Œäº†"
                ],
                order=i+1
            )
            milestones.append(milestone)
        
        # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆæ¨è«–çµæœãƒ™ãƒ¼ã‚¹ï¼‰
        next_actions = []
        
        # æ¨è«–çµæœã‹ã‚‰ã®ä¸»è¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        main_action = NextAction(
            action=f"{inference_result['support_type']}ã‚’é€šã˜ãŸ{inference_result.get('next_node_type', NodeType.QUESTION).value}ã®ç™ºå±•",
            urgency=5,
            importance=5,
            reason=inference_result['reason'],
            expected_outcome=f"æ¢ç©¶ã®{inference_result.get('next_node_type', NodeType.QUESTION).value}æ®µéšã¸ã®é€²å±•"
        )
        next_actions.append(main_action)
        
        # å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®è£œåŠ©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if hasattr(self, 'advanced_inference_engine'):
            user_profile = self.advanced_inference_engine._get_or_create_user_profile(node.student_id)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã«åŸºã¥ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            for style, score in user_profile.learning_style.items():
                if score > 0.7:
                    style_action = self._generate_style_based_action(style, node)
                    if style_action:
                        next_actions.append(style_action)
        
        # æˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        strategic_approach = self._generate_strategic_approach(inference_result, node)
        
        # ãƒªã‚¹ã‚¯è¦å› 
        risk_factors = self._identify_risk_factors(node, inference_result)
        
        return ProjectPlan(
            north_star=north_star,
            north_star_metric="æ¢ç©¶ãƒ—ãƒ­ã‚»ã‚¹ã®å®Œæˆåº¦ã¨å­¦ç¿’è€…ã®æº€è¶³åº¦",
            milestones=milestones,
            next_actions=next_actions,
            strategic_approach=strategic_approach,
            risk_factors=risk_factors,
            created_at=datetime.now().isoformat(),
            confidence=inference_result.get('confidence', 0.7)
        )
    
    def _generate_style_based_action(self, style: str, node: Node) -> Optional[NextAction]:
        """å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã«åŸºã¥ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        
        style_actions = {
            'analytical': NextAction(
                action="ãƒ‡ãƒ¼ã‚¿ã¨è«–ç†çš„æ ¹æ‹ ã®åé›†ãƒ»åˆ†æ",
                urgency=3,
                importance=4,
                reason="åˆ†æçš„æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«ã«é©åˆ",
                expected_outcome="è«–ç†çš„ã§ä½“ç³»çš„ãªç†è§£ã®æ§‹ç¯‰"
            ),
            'creative': NextAction(
                action="å‰µé€ çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ç™ºæƒ³ã¨ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°",
                urgency=3,
                importance=4,
                reason="å‰µé€ çš„æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«ã«é©åˆ",
                expected_outcome="æ–°ã—ã„è¦–ç‚¹ã¨ã‚¢ã‚¤ãƒ‡ã‚¢ã®å‰µå‡º"
            ),
            'structured': NextAction(
                action="æ®µéšçš„ãªãƒ—ãƒ­ã‚»ã‚¹è¨ˆç”»ã¨å®Ÿè¡Œæ‰‹é †ã®ç¢ºç«‹",
                urgency=4,
                importance=4,
                reason="æ§‹é€ åŒ–æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«ã«é©åˆ",
                expected_outcome="æ˜ç¢ºã§å®Ÿè¡Œå¯èƒ½ãªè¡Œå‹•è¨ˆç”»"
            ),
            'exploratory': NextAction(
                action="å¤šè§’çš„ãªè¦–ç‚¹ã§ã®æ¢ç´¢ã¨è©¦è¡ŒéŒ¯èª¤",
                urgency=2,
                importance=3,
                reason="æ¢ç´¢çš„æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«ã«é©åˆ",
                expected_outcome="å¹…åºƒã„ç†è§£ã¨æ–°ãŸãªç™ºè¦‹"
            )
        }
        
        return style_actions.get(style)
    
    def _generate_strategic_approach(self, inference_result: Dict[str, Any], node: Node) -> str:
        """æˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç”Ÿæˆ"""
        
        base_approach = "å€‹äººé©å¿œå‹ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹æ¢ç©¶å­¦ç¿’"
        
        inference_source = inference_result.get('inference_source', '')
        confidence = inference_result.get('confidence', 0.5)
        
        if confidence > 0.8:
            confidence_desc = "é«˜ä¿¡é ¼åº¦"
        elif confidence > 0.6:
            confidence_desc = "ä¸­ä¿¡é ¼åº¦"
        else:
            confidence_desc = "æ¢ç´¢çš„"
        
        if 'pattern:' in inference_source:
            return f"{base_approach} - å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ï¼ˆ{confidence_desc}ï¼‰"
        elif 'adaptive_rule:' in inference_source:
            return f"{base_approach} - é©å¿œãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆ{confidence_desc}ï¼‰"
        else:
            return f"{base_approach} - åŸºæœ¬ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆ{confidence_desc}ï¼‰"
    
    def _identify_risk_factors(self, node: Node, inference_result: Dict[str, Any]) -> List[str]:
        """ãƒªã‚¹ã‚¯è¦å› ã®ç‰¹å®š"""
        
        risks = []
        
        # æ˜ç¢ºæ€§ã«åŸºã¥ããƒªã‚¹ã‚¯
        if node.clarity < 0.4:
            risks.append("æ¦‚å¿µã®æ˜ç¢ºæ€§ä¸è¶³ã«ã‚ˆã‚‹æ··ä¹±ã®ãƒªã‚¹ã‚¯")
        
        # æ·±ã•ã«åŸºã¥ããƒªã‚¹ã‚¯
        if node.depth > 0.8:
            risks.append("éåº¦ãªæ·±æ˜ã‚Šã«ã‚ˆã‚‹æœ¬è³ªã‚’è¦‹å¤±ã†ãƒªã‚¹ã‚¯")
        elif node.depth < 0.3:
            risks.append("è¡¨é¢çš„ãªç†è§£ã«ç•™ã¾ã‚‹ãƒªã‚¹ã‚¯")
        
        # ä¿¡é ¼åº¦ã«åŸºã¥ããƒªã‚¹ã‚¯
        confidence = inference_result.get('confidence', 0.5)
        if confidence < 0.5:
            risks.append("æ¨è«–ã®ä¸ç¢ºå®Ÿæ€§ã«ã‚ˆã‚‹æ–¹å‘æ€§ã®è¿·ã„ã®ãƒªã‚¹ã‚¯")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒªã‚¹ã‚¯
        if node.metadata:
            looping_signals = node.metadata.get('looping_signals', [])
            if looping_signals:
                risks.append("åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¹°ã‚Šè¿”ã—ã«ã‚ˆã‚‹åœæ»ã®ãƒªã‚¹ã‚¯")
        
        return risks
    
    def _build_context_enhanced_message(self, user_message: str, conversation_context: Dict[str, Any]) -> str:
        """ä¼šè©±æ–‡è„ˆã‚’å«ã‚ãŸå¼·åŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰"""
        
        # æ–‡è„ˆæƒ…å ±ã‚’å«ã‚ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        enhanced_message = user_message
        
        # ç¾åœ¨ã®ãƒˆãƒ”ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆ
        if conversation_context.get('current_topic'):
            topic = conversation_context['current_topic']
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æ–‡è„ˆã‚’è¿½åŠ 
            enhanced_message = f"[æ–‡è„ˆ: {topic}ã«ã¤ã„ã¦è©±ã—ã¦ã„ã¾ã™] {user_message}"
        
        # æ–‡è„ˆãƒã‚§ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆ
        if conversation_context.get('context_chain'):
            chain = ' â†’ '.join(conversation_context['context_chain'][-3:])  # æœ€è¿‘3ã¤
            enhanced_message = f"[è©±é¡Œã®æµã‚Œ: {chain}] {enhanced_message}"
        
        return enhanced_message
    
    def _generate_context_aware_response(self, 
                                        state: StateSnapshot,
                                        support_type: str,
                                        selected_acts: List[str],
                                        user_message: str,
                                        session_info: Dict[str, Any],
                                        inference_result: Dict[str, Any]) -> TurnPackage:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼·åŒ–å¿œç­”ç”Ÿæˆï¼ˆä¼šè©±æ–‡è„ˆã‚’è€ƒæ…®ï¼‰"""
        
        # â˜…é‡è¦: ä¼šè©±æ–‡è„ˆã‚’å¿œç­”ç”Ÿæˆã«æ¸¡ã™
        conversation_context = self._get_conversation_context(state)
        if conversation_context:
            # æ–‡è„ˆæƒ…å ±ã‚’å«ã‚ã¦å¿œç­”ç”Ÿæˆ
            context_enhanced_message = self._build_context_enhanced_message(user_message, conversation_context)
            base_response = self._generate_llm_response(state, support_type, selected_acts, context_enhanced_message)
        else:
            # åŸºæœ¬å¿œç­”ç”Ÿæˆ
            base_response = self._generate_llm_response(state, support_type, selected_acts, user_message)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã§å¿œç­”ã‚’å¼·åŒ–
        enhanced_response = base_response.natural_reply
        enhanced_followups = list(base_response.followups)
        
        # â˜…ä¼šè©±æ–‡è„ˆã«åŸºã¥ãå¿œç­”ã®èª¿æ•´
        conv_context = self._get_conversation_context(state)
        if conv_context:
            current_topic = conv_context.get('current_topic')
            
            # ãƒˆãƒ”ãƒƒã‚¯ãŒæ˜ç¢ºãªå ´åˆã€ãã‚Œã‚’å‚ç…§ã™ã‚‹å¿œç­”ã«èª¿æ•´
            if current_topic and current_topic not in enhanced_response:
                # ä»£åè©ã‚’å…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«ç½®æ›
                enhanced_response = enhanced_response.replace('ãã‚Œ', current_topic)
                enhanced_response = enhanced_response.replace('ã“ã‚Œ', current_topic)
                enhanced_response = enhanced_response.replace('ã‚ã‚Œ', current_topic)
                
                # æ–‡è„ˆã«å¿œã˜ãŸè³ªå•ã®å…·ä½“åŒ–
                # ã€Œä½•ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ã€â†’ã€Œ[ãƒˆãƒ”ãƒƒã‚¯]ã®ä½•ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ã€
                if 'ä½•ã«' in enhanced_response and 'èˆˆå‘³' in enhanced_response:
                    enhanced_response = enhanced_response.replace('ä½•ã«èˆˆå‘³', f'{current_topic}ã®ä½•ã«èˆˆå‘³')
                
                # ã€Œä½•ã‚’ã€â†’ã€Œ[ãƒˆãƒ”ãƒƒã‚¯]ã§/ã‚’ä½¿ã£ã¦ä½•ã‚’ã€
                if 'ä½•ã‚’' in enhanced_response:
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ã‚ºãŒã‚ã‚‹å ´åˆ
                    if conv_context.get('key_phrases'):
                        last_phrase = conv_context['key_phrases'][-1]
                        if 'ä½œã‚‹' in last_phrase or 'é–‹ç™º' in last_phrase or 'æ§‹ç¯‰' in last_phrase:
                            enhanced_response = enhanced_response.replace('ä½•ã‚’ä½œ', f'{current_topic}ã§ä½•ã‚’ä½œ')
                            enhanced_response = enhanced_response.replace('ä½•ã‚’é–‹ç™º', f'{current_topic}ã‚’ä½¿ã£ã¦ä½•ã‚’é–‹ç™º')
                        elif 'å­¦ã¶' in last_phrase or 'å­¦ç¿’' in last_phrase:
                            enhanced_response = enhanced_response.replace('ä½•ã‚’å­¦', f'{current_topic}ã®ä½•ã‚’å­¦')
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€Œã€œã«ã¤ã„ã¦ä½•ã‚’ã€
                        enhanced_response = enhanced_response.replace('ä½•ã‚’', f'{current_topic}ã«ã¤ã„ã¦ä½•ã‚’')
                
                # ã€Œã©ã†ã€ã€Œã©ã®ã‚ˆã†ã«ã€ã®å…·ä½“åŒ–
                if 'ã©ã†' in enhanced_response or 'ã©ã®ã‚ˆã†ã«' in enhanced_response:
                    enhanced_response = enhanced_response.replace('ã©ã†ã§ã™ã‹', f'{current_topic}ã«ã¤ã„ã¦ã¯ã©ã†ã§ã™ã‹')
                    enhanced_response = enhanced_response.replace('ã©ã®ã‚ˆã†ã«', f'{current_topic}ã‚’ã©ã®ã‚ˆã†ã«')
        
        # å­¦ç¿’è»Œè·¡ã«åŸºã¥ãèª¿æ•´
        if session_info['learning_trajectory']:
            trajectory = session_info['learning_trajectory']
            recent_support_types = [item.get('support_type') for item in trajectory[-3:]]
            
            # åŒã˜æ”¯æ´ã‚¿ã‚¤ãƒ—ãŒç¶šã„ã¦ã„ã‚‹å ´åˆã¯å¤‰åŒ–ã‚’ææ¡ˆ
            if len(set(recent_support_types)) == 1 and len(recent_support_types) >= 2:
                enhanced_followups.append("åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã—ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ")
        
        # æ¨è«–ã‚½ãƒ¼ã‚¹ã«åŸºã¥ãèª¬æ˜è¿½åŠ 
        if self.use_advanced_inference and inference_result:
            source = inference_result.get('inference_source', '')
            confidence = inference_result.get('confidence', 0.5)
            
            if 'pattern:' in source and confidence > 0.7:
                enhanced_followups.append("ã“ã‚Œã¾ã§ã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™")
            elif 'adaptive_rule:' in source and confidence > 0.7:
                enhanced_followups.append("ã‚ãªãŸã«æœ€é©åŒ–ã•ã‚ŒãŸæ”¯æ´ã‚’æä¾›ã—ã¦ã„ã¾ã™")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã«åŸºã¥ãèª¿æ•´
        user_prefs = session_info.get('user_preferences', {})
        if 'communication_style' in user_prefs:
            style = user_prefs['communication_style']
            if style == 'concise':
                enhanced_response = self._make_response_concise(enhanced_response)
            elif style == 'detailed':
                enhanced_response = self._make_response_detailed(enhanced_response, inference_result)
        
        return TurnPackage(
            natural_reply=enhanced_response,
            followups=enhanced_followups[:3],  # æœ€å¤§3å€‹
            metadata={
                **base_response.metadata,
                'session_enhanced': True,
                'inference_source': inference_result.get('inference_source', 'unknown'),
                'confidence': inference_result.get('confidence', 0.5)
            }
        )
    
    def _make_response_concise(self, response: str) -> str:
        """å¿œç­”ã‚’ç°¡æ½”ã«ã™ã‚‹"""
        sentences = response.split('ã€‚')
        if len(sentences) > 2:
            return 'ã€‚'.join(sentences[:2]) + 'ã€‚'
        return response
    
    def _make_response_detailed(self, response: str, inference_result: Dict[str, Any]) -> str:
        """å¿œç­”ã‚’è©³ç´°ã«ã™ã‚‹"""
        additional_info = []
        
        if 'applied_rule' in inference_result:
            additional_info.append(f"ï¼ˆæ¨è«–æ ¹æ‹ : {inference_result['applied_rule']}ï¼‰")
        
        if 'predictions' in inference_result:
            pred_count = len(inference_result['predictions'])
            additional_info.append(f"æ¬¡ã®{pred_count}ã‚¹ãƒ†ãƒƒãƒ—ã‚’äºˆæ¸¬ã—ã¦ææ¡ˆã—ã¦ã„ã¾ã™ã€‚")
        
        if additional_info:
            return response + ' ' + ' '.join(additional_info)
        
        return response
    
    def _update_session(self, session_id: str, update_data: Dict[str, Any]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ›´æ–°"""
        
        if session_id in self.active_sessions:
            session = self.active_sessions[session_id]
            session['interaction_count'] += 1
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå±¥æ­´ã«è¿½åŠ 
            context_entry = {
                'timestamp': datetime.now().isoformat(),
                'support_type': update_data.get('support_type'),
                'acts': update_data.get('acts', []),
                'confidence': update_data.get('confidence', 0.5)
            }
            session['context_history'].append(context_entry)
            
            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
            if len(session['context_history']) > 50:
                session['context_history'] = session['context_history'][-25:]
            
            session.update(update_data)
    
    def _collect_learning_data(self, 
                              node: Node, 
                              inference_result: Dict[str, Any], 
                              response_package: TurnPackage,
                              session_info: Dict[str, Any]) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        
        return {
            'node_features': {
                'type': node.type.value,
                'clarity': node.clarity,
                'depth': node.depth,
                'confidence': node.confidence
            },
            'inference_features': {
                'support_type': inference_result.get('support_type'),
                'acts': inference_result.get('acts', []),
                'confidence': inference_result.get('confidence', 0.5),
                'source': inference_result.get('inference_source', 'unknown')
            },
            'response_features': {
                'length': len(response_package.natural_reply),
                'followup_count': len(response_package.followups)
            },
            'session_features': {
                'interaction_count': session_info['interaction_count'],
                'session_duration': self._calculate_session_duration(session_info)
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def _calculate_session_duration(self, session_info: Dict[str, Any]) -> float:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“ã‚’è¨ˆç®—ï¼ˆæ™‚é–“ï¼‰"""
        created_at = datetime.fromisoformat(session_info['created_at'])
        now = datetime.now()
        return (now - created_at).total_seconds() / 3600
    
    def _update_graph_enhanced(self, 
                              current_node: Node,
                              response_package: TurnPackage,
                              inference_result: Dict[str, Any],
                              session_info: Dict[str, Any]):
        """ã‚°ãƒ©ãƒ•æ›´æ–°ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        
        # åŸºæœ¬çš„ãªã‚°ãƒ©ãƒ•æ›´æ–°
        self._update_graph_with_response(
            current_node, 
            response_package,
            inference_result.get("next_node_type", NodeType.QUESTION)
        )
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒãƒ¼ãƒ‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        current_node.metadata = current_node.metadata or {}
        current_node.metadata['last_session_update'] = datetime.now().isoformat()
        current_node.metadata['interaction_count'] = session_info['interaction_count']
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
        if 'learning_data' in session_info:
            current_node.metadata['learning_data'] = session_info['learning_data']
        
        # æ¨è«–å±¥æ­´ã‚’ãƒãƒ¼ãƒ‰ã«é–¢é€£ä»˜ã‘
        if self.use_advanced_inference and hasattr(self, 'advanced_inference_engine'):
            current_node.metadata['inference_history'] = inference_result.get('inference_source', 'unknown')
    
    def _update_enhanced_metrics(self, 
                                state: StateSnapshot, 
                                support_type: str, 
                                selected_acts: List[str], 
                                confidence: float,
                                inference_result: Dict[str, Any]):
        """æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        self._update_metrics(state, support_type, selected_acts)
        
        # æ¨è«–å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.metrics.inference_quality = confidence
        
        # å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if self.use_advanced_inference and hasattr(self, 'advanced_inference_engine'):
            learning_stats = self.advanced_inference_engine.get_learning_statistics()
            self.metrics.learned_patterns_count = learning_stats['learned_patterns_count']
            self.metrics.adaptive_rules_count = learning_stats['adaptive_rules_count']
    
    def _package_enhanced_result(self, 
                                response_package: TurnPackage,
                                support_type: str,
                                selected_acts: List[str],
                                state: StateSnapshot,
                                project_plan: Optional[ProjectPlan],
                                reason: str,
                                confidence: float,
                                inference_result: Dict[str, Any],
                                session_info: Dict[str, Any]) -> Dict[str, Any]:
        """æ‹¡å¼µçµæœãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°"""
        
        # åŸºæœ¬çµæœ
        result = {
            "response": response_package.natural_reply,
            "natural_reply": response_package.natural_reply,  # äº’æ›æ€§ã®ãŸã‚ä¸¡æ–¹ã®ã‚­ãƒ¼ã‚’æä¾›
            "followups": response_package.followups,
            "support_type": support_type,
            "selected_acts": selected_acts,
            "state_snapshot": state.dict(exclude={'user_id', 'conversation_id', 'turn_index'}),
            "project_plan": project_plan.dict() if project_plan else None,
            "decision_metadata": {
                "support_reason": reason,
                "support_confidence": confidence,
                "timestamp": datetime.now().isoformat(),
                "mode": "graph_enhanced" if self.use_graph else "linear",
                "advanced_inference": self.use_advanced_inference
            },
            "metrics": self.metrics.dict()
        }
        
        # ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
        if self.use_graph and hasattr(self, 'ontology_adapter'):
            graph_context = self.ontology_adapter.get_graph_context(state.user_id or "unknown")
            result["graph_context"] = graph_context
        
        # é«˜åº¦æ¨è«–æƒ…å ±
        if self.use_advanced_inference and inference_result:
            result["advanced_inference"] = {
                "source": inference_result.get('inference_source', 'unknown'),
                "confidence": inference_result.get('confidence', 0.5),
                "all_candidates": inference_result.get('all_candidates', []),
                "predictions": inference_result.get('predictions', [])
            }
            
            # å­¦ç¿’çµ±è¨ˆ
            if hasattr(self, 'advanced_inference_engine'):
                result["learning_statistics"] = self.advanced_inference_engine.get_learning_statistics()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
        result["session_info"] = {
            "session_id": session_info['session_id'],
            "interaction_count": session_info['interaction_count'],
            "session_duration_hours": self._calculate_session_duration(session_info)
        }
        
        return result
    
    def provide_feedback(self, 
                        inference_id: str, 
                        user_id: str, 
                        feedback: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æä¾›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        
        if not self.feedback_enabled:
            return {"success": False, "message": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™"}
        
        if not self.use_advanced_inference or not hasattr(self, 'advanced_inference_engine'):
            return {"success": False, "message": "é«˜åº¦æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãŒç„¡åŠ¹ã§ã™"}
        
        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚’å®Ÿè¡Œ
            self.advanced_inference_engine.learn_from_feedback(inference_id, user_id, feedback)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ›´æ–°
            session_id = f"{user_id}_{inference_id}"
            if session_id in self.active_sessions:
                session = self.active_sessions[session_id]
                session['user_preferences'] = session.get('user_preferences', {})
                
                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å¥½ã¿ã‚’å­¦ç¿’
                if feedback.get('satisfaction', 0) > 0.7:
                    if 'support_type' in feedback:
                        session['user_preferences']['preferred_support_type'] = feedback['support_type']
                
                if 'communication_style' in feedback:
                    session['user_preferences']['communication_style'] = feedback['communication_style']
            
            return {
                "success": True, 
                "message": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã—ã¾ã—ãŸ",
                "learning_enabled": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "message": f"å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def auto_discover_patterns(self, user_id: str) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•ç™ºè¦‹"""
        
        if not self.auto_learning_enabled or not hasattr(self, 'advanced_inference_engine'):
            return {"success": False, "message": "è‡ªå‹•å­¦ç¿’æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™"}
        
        try:
            new_patterns = self.advanced_inference_engine.discover_new_patterns(user_id)
            
            return {
                "success": True,
                "new_patterns_count": len(new_patterns),
                "patterns": [
                    {
                        "pattern_id": p.pattern_id,
                        "sequence": [nt.value for nt in p.sequence],
                        "effectiveness_score": p.effectiveness_score
                    }
                    for p in new_patterns
                ]
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "message": f"ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def get_enhanced_insights(self, user_id: str) -> Dict[str, Any]:
        """æ‹¡å¼µæ´å¯Ÿå–å¾—"""
        
        base_insights = self.get_graph_insights(user_id) if self.use_graph else {}
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ´å¯Ÿã‚’è¿½åŠ 
        session_insights = {}
        user_sessions = [s for s in self.active_sessions.values() if s['session_id'].startswith(str(user_id))]
        
        if user_sessions:
            latest_session = max(user_sessions, key=lambda s: s['last_activity'])
            session_insights = {
                "active_sessions": len(user_sessions),
                "latest_interaction_count": latest_session['interaction_count'],
                "preferred_support_types": latest_session.get('user_preferences', {}).get('preferred_support_type'),
                "learning_trajectory_length": len(latest_session.get('learning_trajectory', []))
            }
        
        # å­¦ç¿’çµ±è¨ˆã‚’è¿½åŠ 
        learning_insights = {}
        if self.use_advanced_inference and hasattr(self, 'advanced_inference_engine'):
            learning_stats = self.advanced_inference_engine.get_learning_statistics()
            user_summary = learning_stats.get('user_learning_summary', {}).get(user_id, {})
            learning_insights = {
                "learning_style": user_summary.get('learning_style', {}),
                "adaptation_count": user_summary.get('adaptation_count', 0),
                "patterns_discovered": len([p for p in self.advanced_inference_engine.learned_patterns.values() 
                                          if user_id in p.pattern_id])
            }
        
        return {
            **base_insights,
            "session_insights": session_insights,
            "learning_insights": learning_insights,
            "system_version": "enhanced_unified"
        }