"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çŸ­ç¸®ã¨è¨ˆç®—åŠ¹ç‡å‘ä¸Šã®ãŸã‚ã®æœ€é©åŒ–æ©Ÿèƒ½
"""

import logging
import json
import pickle
import hashlib
import time
from typing import List, Dict, Optional, Any, Tuple, Callable
from datetime import datetime, timedelta
from collections import defaultdict, deque, LRU
from dataclasses import dataclass, field
from pathlib import Path
import threading
import weakref
from functools import wraps, lru_cache
import gzip
import asyncio

from .ontology_graph import Node, Edge, NodeType, RelationType

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    expiry_time: Optional[datetime] = None
    size_bytes: int = 0
    compression_enabled: bool = False
    
    def is_expired(self) -> bool:
        """ã‚¨ãƒ³ãƒˆãƒªãŒæœŸé™åˆ‡ã‚Œã‹ãƒã‚§ãƒƒã‚¯"""
        if self.expiry_time:
            return datetime.now() > self.expiry_time
        return False
    
    def access(self):
        """ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®çµ±è¨ˆæ›´æ–°"""
        self.last_accessed = datetime.now()
        self.access_count += 1


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    cache_hits: int = 0
    cache_misses: int = 0
    total_requests: int = 0
    avg_response_time: float = 0.0
    inference_time_total: float = 0.0
    graph_traversal_time_total: float = 0.0
    rule_evaluation_time_total: float = 0.0
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“å±¥æ­´
    response_times: deque = field(default_factory=lambda: deque(maxlen=1000))
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    cache_memory_usage: int = 0
    graph_memory_usage: int = 0
    
    def add_response_time(self, response_time: float):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’è¿½åŠ """
        self.response_times.append(response_time)
        self.avg_response_time = sum(self.response_times) / len(self.response_times)
    
    def get_cache_hit_rate(self) -> float:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’å–å¾—"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0


class MultiLevelCache:
    """å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 l1_size: int = 1000,      # L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆé«˜é€Ÿï¼‰
                 l2_size: int = 10000,     # L2: åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
                 l3_enabled: bool = True,   # L3: ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                 cache_dir: str = "cache"):
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥: é«˜é€Ÿãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l1_cache: Dict[str, CacheEntry] = {}
        self.l1_max_size = l1_size
        self.l1_access_order = deque()
        
        # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥: åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l2_cache: Dict[str, CacheEntry] = {}
        self.l2_max_size = l2_size
        self.l2_access_order = deque()
        
        # L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥: ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l3_enabled = l3_enabled
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
        self.lock = threading.RLock()
        self.metrics = PerformanceMetrics()
        
        # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.cleanup_thread = None
        self.cleanup_active = True
        self._start_cleanup_thread()
    
    def _start_cleanup_thread(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        if self.cleanup_thread is None or not self.cleanup_thread.is_alive():
            self.cleanup_thread = threading.Thread(target=self._cleanup_loop, daemon=True)
            self.cleanup_thread.start()
            logger.info("ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
    
    def _cleanup_loop(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ«ãƒ¼ãƒ—"""
        while self.cleanup_active:
            try:
                time.sleep(300)  # 5åˆ†ã”ã¨
                self._cleanup_expired()
                self._optimize_cache_distribution()
            except Exception as e:
                logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—"""
        with self.lock:
            start_time = time.time()
            
            # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if key in self.l1_cache:
                entry = self.l1_cache[key]
                if not entry.is_expired():
                    entry.access()
                    self._update_access_order(key, 1)
                    self.metrics.cache_hits += 1
                    logger.debug(f"L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {key}")
                    return entry.value
                else:
                    del self.l1_cache[key]
            
            # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if key in self.l2_cache:
                entry = self.l2_cache[key]
                if not entry.is_expired():
                    entry.access()
                    value = self._decompress_value(entry.value) if entry.compression_enabled else entry.value
                    
                    # L1ã«æ˜‡æ ¼
                    self._promote_to_l1(key, value, entry)
                    self.metrics.cache_hits += 1
                    logger.debug(f"L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼ˆL1æ˜‡æ ¼): {key}")
                    return value
                else:
                    del self.l2_cache[key]
            
            # L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            if self.l3_enabled:
                l3_value = self._get_from_l3(key)
                if l3_value is not None:
                    # L2ã«èª­ã¿è¾¼ã¿
                    self._set_l2(key, l3_value, ttl=3600)
                    self.metrics.cache_hits += 1
                    logger.debug(f"L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼ˆL2æ˜‡æ ¼): {key}")
                    return l3_value
            
            self.metrics.cache_misses += 1
            self.metrics.total_requests += 1
            
            elapsed = time.time() - start_time
            if elapsed > 0.01:  # 10msä»¥ä¸Šã®å ´åˆãƒ­ã‚°å‡ºåŠ›
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼ˆ{elapsed:.3f}s): {key}")
            
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’è¨­å®š"""
        with self.lock:
            try:
                expiry_time = datetime.now() + timedelta(seconds=ttl) if ttl else None
                size_bytes = self._estimate_size(value)
                
                # ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¯åœ§ç¸®ã‚’æ¤œè¨
                if size_bytes > 10240:  # 10KBä»¥ä¸Š
                    return self._set_l2(key, value, ttl, compress=True)
                else:
                    return self._set_l1(key, value, ttl)
                    
            except Exception as e:
                logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã‚¨ãƒ©ãƒ¼ ({key}): {e}")
                return False
    
    def _set_l1(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¨­å®š"""
        expiry_time = datetime.now() + timedelta(seconds=ttl) if ttl else None
        size_bytes = self._estimate_size(value)
        
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            expiry_time=expiry_time,
            size_bytes=size_bytes
        )
        
        # å®¹é‡ãƒã‚§ãƒƒã‚¯
        if len(self.l1_cache) >= self.l1_max_size:
            self._evict_l1()
        
        self.l1_cache[key] = entry
        self._update_access_order(key, 1)
        return True
    
    def _set_l2(self, key: str, value: Any, ttl: Optional[int] = None, compress: bool = False) -> bool:
        """L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¨­å®š"""
        expiry_time = datetime.now() + timedelta(seconds=ttl) if ttl else None
        
        # åœ§ç¸®
        if compress:
            compressed_value = self._compress_value(value)
            size_bytes = self._estimate_size(compressed_value)
        else:
            compressed_value = value
            size_bytes = self._estimate_size(value)
        
        entry = CacheEntry(
            key=key,
            value=compressed_value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            expiry_time=expiry_time,
            size_bytes=size_bytes,
            compression_enabled=compress
        )
        
        # å®¹é‡ãƒã‚§ãƒƒã‚¯
        if len(self.l2_cache) >= self.l2_max_size:
            self._evict_l2()
        
        self.l2_cache[key] = entry
        self._update_access_order(key, 2)
        return True
    
    def _promote_to_l1(self, key: str, value: Any, l2_entry: CacheEntry):
        """L2ã‹ã‚‰L1ã«æ˜‡æ ¼"""
        if len(self.l1_cache) >= self.l1_max_size:
            self._evict_l1()
        
        l1_entry = CacheEntry(
            key=key,
            value=value,
            created_at=l2_entry.created_at,
            last_accessed=datetime.now(),
            access_count=l2_entry.access_count + 1,
            expiry_time=l2_entry.expiry_time,
            size_bytes=self._estimate_size(value)
        )
        
        self.l1_cache[key] = l1_entry
        self._update_access_order(key, 1)
    
    def _evict_l1(self):
        """L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰Šé™¤ï¼ˆLRUï¼‰"""
        if self.l1_access_order:
            lru_key = self.l1_access_order.popleft()
            if lru_key in self.l1_cache:
                entry = self.l1_cache[lru_key]
                
                # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦ãŒé«˜ã„å ´åˆã¯L2ã«é™æ ¼
                if entry.access_count > 3:
                    self._set_l2(lru_key, entry.value, ttl=1800)  # 30åˆ†
                
                del self.l1_cache[lru_key]
    
    def _evict_l2(self):
        """L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰Šé™¤ï¼ˆLRUï¼‰"""
        if self.l2_access_order:
            lru_key = self.l2_access_order.popleft()
            if lru_key in self.l2_cache:
                entry = self.l2_cache[lru_key]
                
                # é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã¯L3ã«ä¿å­˜
                if entry.access_count > 5 and self.l3_enabled:
                    self._set_l3(lru_key, entry.value)
                
                del self.l2_cache[lru_key]
    
    def _update_access_order(self, key: str, level: int):
        """ã‚¢ã‚¯ã‚»ã‚¹é †åºã‚’æ›´æ–°"""
        if level == 1:
            if key in self.l1_access_order:
                self.l1_access_order.remove(key)
            self.l1_access_order.append(key)
        elif level == 2:
            if key in self.l2_access_order:
                self.l2_access_order.remove(key)
            self.l2_access_order.append(key)
    
    def _get_from_l3(self, key: str) -> Optional[Any]:
        """L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        try:
            cache_file = self.cache_dir / f"{key}.cache"
            if cache_file.exists():
                with gzip.open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    
                # æœŸé™ãƒã‚§ãƒƒã‚¯
                if 'expiry' in data and datetime.now() > data['expiry']:
                    cache_file.unlink()
                    return None
                
                return data['value']
        except Exception as e:
            logger.error(f"âŒ L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({key}): {e}")
        
        return None
    
    def _set_l3(self, key: str, value: Any, ttl: int = 86400):
        """L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¨­å®š"""
        try:
            cache_file = self.cache_dir / f"{key}.cache"
            expiry = datetime.now() + timedelta(seconds=ttl)
            
            data = {
                'value': value,
                'created_at': datetime.now(),
                'expiry': expiry
            }
            
            with gzip.open(cache_file, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
        except Exception as e:
            logger.error(f"âŒ L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({key}): {e}")
    
    def _compress_value(self, value: Any) -> bytes:
        """å€¤ã‚’åœ§ç¸®"""
        pickled = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)
        return gzip.compress(pickled)
    
    def _decompress_value(self, compressed_value: bytes) -> Any:
        """å€¤ã‚’å±•é–‹"""
        decompressed = gzip.decompress(compressed_value)
        return pickle.loads(decompressed)
    
    def _estimate_size(self, value: Any) -> int:
        """å€¤ã®ã‚µã‚¤ã‚ºã‚’æ¨å®š"""
        try:
            return len(pickle.dumps(value))
        except:
            return 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1KB
    
    def _cleanup_expired(self):
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with self.lock:
            # L1ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            expired_l1 = [k for k, v in self.l1_cache.items() if v.is_expired()]
            for key in expired_l1:
                del self.l1_cache[key]
                if key in self.l1_access_order:
                    self.l1_access_order.remove(key)
            
            # L2ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            expired_l2 = [k for k, v in self.l2_cache.items() if v.is_expired()]
            for key in expired_l2:
                del self.l2_cache[key]
                if key in self.l2_access_order:
                    self.l2_access_order.remove(key)
            
            if expired_l1 or expired_l2:
                logger.info(f"ğŸ§¹ æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: L1={len(expired_l1)}, L2={len(expired_l2)}")
    
    def _optimize_cache_distribution(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†æ•£ã‚’æœ€é©åŒ–"""
        with self.lock:
            # L2ã‹ã‚‰L1ã¸ã®æ˜‡æ ¼å€™è£œã‚’æ¤œè¨
            promotion_candidates = []
            for key, entry in self.l2_cache.items():
                if entry.access_count > 5 and not entry.compression_enabled:
                    promotion_candidates.append((key, entry))
            
            # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            promotion_candidates.sort(key=lambda x: x[1].access_count, reverse=True)
            
            # L1ã«ç©ºããŒã‚ã‚Œã°æ˜‡æ ¼
            available_l1_slots = self.l1_max_size - len(self.l1_cache)
            for i in range(min(available_l1_slots, len(promotion_candidates))):
                key, entry = promotion_candidates[i]
                value = entry.value
                self._promote_to_l1(key, value, entry)
                del self.l2_cache[key]
                if key in self.l2_access_order:
                    self.l2_access_order.remove(key)
    
    def clear(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        with self.lock:
            self.l1_cache.clear()
            self.l2_cache.clear()
            self.l1_access_order.clear()
            self.l2_access_order.clear()
            
            if self.l3_enabled:
                for cache_file in self.cache_dir.glob("*.cache"):
                    try:
                        cache_file.unlink()
                    except:
                        pass
    
    def get_statistics(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        with self.lock:
            return {
                'l1_size': len(self.l1_cache),
                'l2_size': len(self.l2_cache),
                'l1_max_size': self.l1_max_size,
                'l2_max_size': self.l2_max_size,
                'hit_rate': self.metrics.get_cache_hit_rate(),
                'total_hits': self.metrics.cache_hits,
                'total_misses': self.metrics.cache_misses,
                'memory_usage_estimate': sum(e.size_bytes for e in self.l1_cache.values()) + 
                                        sum(e.size_bytes for e in self.l2_cache.values())
            }


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_dir: str = "performance_cache"):
        self.cache = MultiLevelCache(cache_dir=cache_dir)
        self.metrics = PerformanceMetrics()
        
        # è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.inference_cache = {}
        self.graph_traversal_cache = {}
        self.pattern_cache = {}
        
        # éåŒæœŸå‡¦ç†ãƒ—ãƒ¼ãƒ«
        self.executor = None
        
        # æœ€é©åŒ–è¨­å®š
        self.optimization_settings = {
            'enable_caching': True,
            'enable_async': True,
            'enable_lazy_loading': True,
            'enable_parallel_inference': True,
            'cache_ttl_default': 3600,  # 1æ™‚é–“
            'cache_ttl_inference': 1800,  # 30åˆ†
            'cache_ttl_graph': 7200,     # 2æ™‚é–“
        }
    
    def cache_key(self, *args, **kwargs) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def cached_inference(self, ttl: int = None):
        """æ¨è«–çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not self.optimization_settings['enable_caching']:
                    return func(*args, **kwargs)
                
                cache_key = f"inference_{self.cache_key(*args, **kwargs)}"
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # è¨ˆç®—å®Ÿè¡Œ
                start_time = time.time()
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                self.metrics.inference_time_total += execution_time
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache_ttl = ttl or self.optimization_settings['cache_ttl_inference']
                self.cache.set(cache_key, result, ttl=cache_ttl)
                
                return result
            return wrapper
        return decorator
    
    def cached_graph_operation(self, ttl: int = None):
        """ã‚°ãƒ©ãƒ•æ“ä½œã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not self.optimization_settings['enable_caching']:
                    return func(*args, **kwargs)
                
                cache_key = f"graph_{self.cache_key(*args, **kwargs)}"
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # è¨ˆç®—å®Ÿè¡Œ
                start_time = time.time()
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                self.metrics.graph_traversal_time_total += execution_time
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache_ttl = ttl or self.optimization_settings['cache_ttl_graph']
                self.cache.set(cache_key, result, ttl=cache_ttl)
                
                return result
            return wrapper
        return decorator
    
    def cached_pattern_matching(self, ttl: int = None):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                if not self.optimization_settings['enable_caching']:
                    return func(*args, **kwargs)
                
                cache_key = f"pattern_{self.cache_key(*args, **kwargs)}"
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # è¨ˆç®—å®Ÿè¡Œ
                start_time = time.time()
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache_ttl = ttl or self.optimization_settings['cache_ttl_default']
                self.cache.set(cache_key, result, ttl=cache_ttl)
                
                return result
            return wrapper
        return decorator
    
    def measure_performance(self, operation_name: str = "operation"):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    execution_time = time.time() - start_time
                    self.metrics.add_response_time(execution_time)
                    self.metrics.total_requests += 1
                    
                    if execution_time > 1.0:  # 1ç§’ä»¥ä¸Šã®å ´åˆã¯è­¦å‘Š
                        logger.warning(f"âš ï¸ ä½é€Ÿæ“ä½œæ¤œå‡º ({operation_name}): {execution_time:.3f}s")
                    elif execution_time > 0.1:  # 100msä»¥ä¸Šã®å ´åˆã¯æƒ…å ±
                        logger.info(f"ğŸ“Š æ“ä½œå®Œäº† ({operation_name}): {execution_time:.3f}s")
            return wrapper
        return decorator
    
    def batch_process(self, items: List[Any], batch_size: int = 10, parallel: bool = True):
        """ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–"""
        if not parallel or not self.optimization_settings['enable_parallel_inference']:
            return items
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²
        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]
        
        if self.optimization_settings['enable_async']:
            return self._async_batch_process(batches)
        else:
            return self._sync_batch_process(batches)
    
    def _async_batch_process(self, batches: List[List[Any]]):
        """éåŒæœŸãƒãƒƒãƒå‡¦ç†"""
        async def process_batch(batch):
            return batch  # å®Ÿéš›ã®å‡¦ç†ã¯ã“ã“ã«å®Ÿè£…
        
        async def run_all_batches():
            tasks = [process_batch(batch) for batch in batches]
            return await asyncio.gather(*tasks)
        
        try:
            loop = asyncio.get_event_loop()
            return loop.run_until_complete(run_all_batches())
        except RuntimeError:
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯åŒæœŸå‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._sync_batch_process(batches)
    
    def _sync_batch_process(self, batches: List[List[Any]]):
        """åŒæœŸãƒãƒƒãƒå‡¦ç†"""
        results = []
        for batch in batches:
            results.extend(batch)
        return results
    
    def optimize_graph_structure(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚°ãƒ©ãƒ•æ§‹é€ ã®æœ€é©åŒ–"""
        optimized = graph_data.copy()
        
        # ãƒãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–
        if 'nodes' in optimized:
            nodes = optimized['nodes']
            
            # é »ç¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’å‰ã«é…ç½®
            if isinstance(nodes, list):
                nodes.sort(key=lambda n: n.get('access_count', 0), reverse=True)
            
            # ãƒãƒ¼ãƒ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®
            for node in nodes:
                if 'metadata' in node and isinstance(node['metadata'], dict):
                    # ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                    node['metadata'] = {k: v for k, v in node['metadata'].items() 
                                      if k in ['learning_data', 'session_id', 'interaction_count']}
        
        # ã‚¨ãƒƒã‚¸ã®æœ€é©åŒ–
        if 'edges' in optimized:
            edges = optimized['edges']
            
            # ä¿¡é ¼åº¦ã®ä½ã„ã‚¨ãƒƒã‚¸ã‚’å‰Šé™¤
            if isinstance(edges, list):
                optimized['edges'] = [e for e in edges if e.get('confidence', 0) > 0.1]
        
        return optimized
    
    def lazy_load_graph_data(self, user_id: str, limit: int = 100) -> Dict[str, Any]:
        """é…å»¶ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        cache_key = f"lazy_graph_{user_id}_{limit}"
        
        cached_data = self.cache.get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # å¿…è¦æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿èª­ã¿è¾¼ã¿
        graph_data = {
            'user_id': user_id,
            'nodes': [],  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯æœ€æ–°ã®ãƒãƒ¼ãƒ‰ã®ã¿
            'edges': [],  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é–¢é€£ã‚¨ãƒƒã‚¸ã®ã¿
            'metadata': {
                'loaded_at': datetime.now().isoformat(),
                'limit': limit,
                'lazy_loaded': True
            }
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache.set(cache_key, graph_data, ttl=self.optimization_settings['cache_ttl_graph'])
        
        return graph_data
    
    def preload_frequent_patterns(self, user_ids: List[str]):
        """é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        if not self.optimization_settings['enable_caching']:
            return
        
        for user_id in user_ids:
            # ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            patterns = [
                f"user_profile_{user_id}",
                f"recent_interactions_{user_id}",
                f"learning_patterns_{user_id}"
            ]
            
            for pattern in patterns:
                cache_key = f"preload_{pattern}"
                if self.cache.get(cache_key) is None:
                    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
                    placeholder_data = {
                        'pattern': pattern,
                        'preloaded_at': datetime.now().isoformat()
                    }
                    self.cache.set(cache_key, placeholder_data, ttl=1800)  # 30åˆ†
    
    def optimize_inference_pipeline(self, pipeline_steps: List[Callable]) -> List[Callable]:
        """æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€é©åŒ–"""
        optimized_steps = []
        
        for i, step in enumerate(pipeline_steps):
            # ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚’è¿½åŠ 
            cached_step = self.cached_inference()(step)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚’è¿½åŠ 
            measured_step = self.measure_performance(f"pipeline_step_{i}")(cached_step)
            
            optimized_steps.append(measured_step)
        
        return optimized_steps
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
        cache_stats = self.cache.get_statistics()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çµ±è¨ˆ
        response_times = list(self.metrics.response_times)
        if response_times:
            percentiles = {
                'p50': float(np.percentile(response_times, 50)),
                'p90': float(np.percentile(response_times, 90)),
                'p95': float(np.percentile(response_times, 95)),
                'p99': float(np.percentile(response_times, 99))
            }
        else:
            percentiles = {'p50': 0, 'p90': 0, 'p95': 0, 'p99': 0}
        
        return {
            'cache_statistics': cache_stats,
            'response_time_metrics': {
                'avg_response_time': self.metrics.avg_response_time,
                'total_requests': self.metrics.total_requests,
                'percentiles': percentiles
            },
            'operation_metrics': {
                'inference_time_total': self.metrics.inference_time_total,
                'graph_traversal_time_total': self.metrics.graph_traversal_time_total,
                'rule_evaluation_time_total': self.metrics.rule_evaluation_time_total
            },
            'optimization_settings': self.optimization_settings,
            'recommendations': self._generate_optimization_recommendations()
        }
    
    def _generate_optimization_recommendations(self) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒä½ã„å ´åˆ
        hit_rate = self.metrics.get_cache_hit_rate()
        if hit_rate < 0.5:
            recommendations.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒä½ã„ã§ã™ã€‚TTLè¨­å®šã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        
        # å¹³å‡å¿œç­”æ™‚é–“ãŒé•·ã„å ´åˆ
        if self.metrics.avg_response_time > 2.0:
            recommendations.append("å¹³å‡å¿œç­”æ™‚é–“ãŒé•·ã„ã§ã™ã€‚ä¸¦åˆ—å‡¦ç†ã‚„äº‹å‰è¨ˆç®—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„å ´åˆ
        cache_stats = self.cache.get_statistics()
        if cache_stats.get('memory_usage_estimate', 0) > 100 * 1024 * 1024:  # 100MB
            recommendations.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ã§ã™ã€‚åœ§ç¸®ã‚„å®¹é‡åˆ¶é™ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        
        if not recommendations:
            recommendations.append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯è‰¯å¥½ã§ã™ã€‚")
        
        return recommendations
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        logger.info("ğŸ§¹ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
    
    def shutdown(self):
        """æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        self.cache.cleanup_active = False
        if self.cache.cleanup_thread:
            self.cache.cleanup_thread.join(timeout=5)
        
        if self.executor:
            self.executor.shutdown(wait=True)
        
        logger.info("â¹ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")


# NumPyäº’æ›ã®ç°¡æ˜“å®Ÿè£…ï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§numpyãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰
class np:
    @staticmethod
    def percentile(data, percentile):
        """ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = (percentile / 100.0) * (len(sorted_data) - 1)
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower = sorted_data[int(index)]
            upper = sorted_data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))
    
    @staticmethod
    def mean(data):
        """å¹³å‡è¨ˆç®—"""
        return sum(data) / len(data) if data else 0


# LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç°¡æ˜“å®Ÿè£…ï¼ˆPythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãªã„å ´åˆï¼‰
class LRU:
    def __init__(self, maxsize=128):
        self.maxsize = maxsize
        self.cache = {}
        self.access_order = deque()
    
    def get(self, key, default=None):
        if key in self.cache:
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return default
    
    def put(self, key, value):
        if key in self.cache:
            self.access_order.remove(key)
        elif len(self.cache) >= self.maxsize:
            oldest = self.access_order.popleft()
            del self.cache[oldest]
        
        self.cache[key] = value
        self.access_order.append(key)